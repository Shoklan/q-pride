[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Orgulo.us",
    "section": "",
    "text": "Can We Find A Better Crosshair In Valorant: Part Two?\n\n\n\n\n\nMoving Past Fundamentals To Solutions\n\n\n\n\n\n\nMar 18, 2023\n\n\n\n\n\n\n  \n\n\n\n\nReview of Specificity and Sensitivity, Etc\n\n\n\n\n\nDeeper Dive Into Measuring Metrics\n\n\n\n\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShapiro-Wilk’s Test for Normalicy in Python and R\n\n\n\n\n\nWhat Even Is Normal Anyway.\n\n\n\n\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFisher’s Exact Test in Python and R\n\n\n\n\n\nWhen you need stronger guarantees.\n\n\n\n\n\n\nFeb 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-Nearest Neighbors in Python and R\n\n\n\n\n\nThis is not KMeans!\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression in Python and R\n\n\n\n\n\nWhat Now If It’s Not Linear?\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial Test in R And Python\n\n\n\n\n\nDo Our Results Make Sense For This Distribution?\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression Trees in R And Python\n\n\n\n\n\nA Niche Little Model\n\n\n\n\n\n\nJan 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChi Squared - What Is It And How in Python and R\n\n\n\n\n\nThis is your life now.\n\n\n\n\n\n\nJan 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Linear Regression in R and Python\n\n\n\n\n\nSynthesizing Sources.\n\n\n\n\n\n\nJan 28, 2023\n\n\nCollin Mitchell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting FastPitch From NVidia Working\n\n\n\n\n\nA little extra work.\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Television Data With Spark - Part 2\n\n\n\n\n\nFrom Questions to Visualizations\n\n\n\n\n\n\nJan 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenges for 2023\n\n\n\n\n\nWhat To Expect This Year.\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: SwordShip\n\n\n\n\n\nWatership goes FZROOOOOOOOOM\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Television Data With Spark - Part 1\n\n\n\n\n\nHow to Use Spark in Python.\n\n\n\n\n\n\nDec 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview for the Year: 2022 Edition\n\n\n\n\n\nDid I accomplish what I wanted?\n\n\n\n\n\n\nDec 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nFirst Time Seeing the Pareto Plot\n\n\n\n\n\nPeople Use This?\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan We Find A Better Crosshair In Valorant?\n\n\n\n\n\nExploring Pytorch Fundamentals.\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with Mermaid.js\n\n\n\n\n\nExplore All The Tools\n\n\n\n\n\n\nDec 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-Anonymity With Python\n\n\n\n\n\nSimple Practice\n\n\n\n\n\n\nDec 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating Loan Payments - Recursively\n\n\n\n\n\nI don’t need Excel\n\n\n\n\n\n\nDec 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWhich Cycle Frontier Jobs Are Worth Doing? - Part Five.\n\n\n\n\n\nBit of Cleanup and Auto-updating\n\n\n\n\n\n\nDec 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPandas Is No Longer My Default\n\n\n\n\n\nOr Is It?\n\n\n\n\n\n\nDec 2, 2022\n\n\nCollin Mitchell\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Journey\n\n\n\n\n\nPlease More Games Like This\n\n\n\n\n\n\nDec 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatatables Were Migrated to Python\n\n\n\n\n\nAnother Thing I Miss.\n\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\nAdding Quarto Was A Good Decision\n\n\n\n\n\nFinally, Proper PDF Support.\n\n\n\n\n\n\nNov 28, 2022\n\n\nCollin Mitchell\n\n\n\n\n\n\n  \n\n\n\n\nMigrating From Fastpages to Quarto\n\n\n\n\n\nThis Was Not Simple.\n\n\n\n\n\n\nNov 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoes The Faker Package Pass Luhn?\n\n\n\n\n\nHow good is the fake generation?\n\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nNothing To See Here\n\n\n\n\n\nJust some Catposting on the Internet\n\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Introduction to Memcached\n\n\n\n\n\nAnd Maybe a Real World Use\n\n\n\n\n\n\nNov 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Planner Review - Discounting Cash Flows\n\n\n\n\n\nNot Sure I Trust This\n\n\n\n\n\n\nNov 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Cycle Frontier Jobs Are Worth Doing? - Part Four.\n\n\n\n\n\nTime To Deal With the Missing Values\n\n\n\n\n\n\nNov 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSimple Function for Highlight Box Effect\n\n\n\n\n\nBuilding on A Pretty Plot.\n\n\n\n\n\n\nNov 2, 2022\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Powerwash Simulator\n\n\n\n\n\nI wouldn’t do this in real life; Why am I doing this?\n\n\n\n\n\n\nNov 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollecting External Data for Python.\n\n\n\n\n\nOr, Delete the Intermediate When Collecting Data.\n\n\n\n\n\n\nOct 26, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSomething Cute Or Something Dangerous\n\n\n\n\n\nFerrets Vs Dragons Image Classification\n\n\n\n\n\n\nOct 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Cycle Frontier Jobs Are Worth Doing? - Part Three.\n\n\n\n\n\nLet’s do the calculations!\n\n\n\n\n\n\nOct 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Cycle Frontier Jobs Are Worth Doing? - Part Two.\n\n\n\n\n\nSecond We’ll Clean the Tasks.\n\n\n\n\n\n\nOct 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownloading and Exploring Loot in Cycle Frontier\n\n\n\n\n\nLet’s Check out the Loot!\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Test of Ipywidget Interact Function\n\n\n\n\n\nDoes this Work For the Website? No.\n\n\n\n\n\n\nSep 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Cycle Frontier Jobs Are Worth Doing? - Part One.\n\n\n\n\n\nFirst We’ll Clean the Jobs.\n\n\n\n\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Data Exploration of The Cycle - Frontier Weapons\n\n\n\n\n\nWhat Is The Most Solid Single Category of Guns?\n\n\n\n\n\n\nSep 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Lastest Lectures For Fastai are Online.\n\n\n\n\n\nLet’s Make Something Simple!\n\n\n\n\n\n\nSep 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Pawnbarian\n\n\n\n\n\nSolid Puzzle Game\n\n\n\n\n\n\nSep 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Setup V Rising on Linux\n\n\n\n\n\nYour Milage May Vary.\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Do You Create a Normal Map?\n\n\n\n\n\nFeaturing Blender and Gimp!\n\n\n\n\n\n\nFeb 2, 2022\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Eraser\n\n\n\n\n\nI’m in Pain But Nothing Looks Broken\n\n\n\n\n\n\nJan 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview of 2021 - Challenges, Thoughts\n\n\n\n\n\nDid I accomplish what I wanted and what needs to change?\n\n\n\n\n\n\nDec 8, 2021\n\n\n\n\n\n\n  \n\n\n\n\nChallenges for The Year: 2022 Edition\n\n\n\n\n\nPublic Record For Accountability\n\n\n\n\n\n\nDec 8, 2021\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Ethereal Estate\n\n\n\n\n\nMake a Mess With a Fried.\n\n\n\n\n\n\nSep 7, 2021\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: The Forest\n\n\n\n\n\nHow to Eat Your Neighbors.\n\n\n\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit Card Security\n\n\n\n\n\nWhat is Luhn’s Algorithm?\n\n\n\n\n\n\nFeb 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is a Memorandum of Understanding?\n\n\n\n\n\nCybersecurity+ Series.\n\n\n\n\n\n\nFeb 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\nQuick Introduction to Streamlit\n\n\n\n\n\nYou should use this framework too.\n\n\n\n\n\n\nFeb 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinimal Spark Cluster\n\n\n\n\n\nSetting Up Without Hadoop Cluster\n\n\n\n\n\n\nJan 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenges for 2021\n\n\n\n\n\nPublic Record For Accountability\n\n\n\n\n\n\nDec 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBook Review: The Pulp Mindset; A Newpub Survival Mindset\n\n\n\n\n\nUseful But Repetitive\n\n\n\n\n\n\nOct 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Introduction to Python Debugging\n\n\n\n\n\nSome Examples I learned From Debugging My Own Stuff\n\n\n\n\n\n\nOct 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Post Void\n\n\n\n\n\nNostalgia with a Side of Adrenaline.\n\n\n\n\n\n\nSep 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Empyrion - Galactic Survival\n\n\n\n\n\nComplexity is the Name of the Game.\n\n\n\n\n\n\nSep 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFastbooks Incidentally Supports R\n\n\n\n\n\nR and ggplot!\n\n\n\n\n\n\nSep 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\nA Post Exploring Altair\n\n\n\n\n\nA tutorial of fastpages for Jupyter notebooks.\n\n\n\n\n\n\nSep 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBook Review: Voices of the Void\n\n\n\n\n\nIs Andrew Dalatent Insane?\n\n\n\n\n\n\nJul 9, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Notes.html",
    "href": "Notes.html",
    "title": "Orgulo.us",
    "section": "",
    "text": "Spark TV Ideas: - Why are there durations above 200 minutes? - What can we do with the actors since there is so much and it is unique? - Appropriate bins for the Duration?"
  },
  {
    "objectID": "posts/nbks/2020-09-09-fastbooks-incidentally-supports-r.html",
    "href": "posts/nbks/2020-09-09-fastbooks-incidentally-supports-r.html",
    "title": "Fastbooks Incidentally Supports R",
    "section": "",
    "text": "Pleasant Surprise\nWhile trying to test the boundaries of what fastpages actually supports, I figured I’d try out installing and setting up an R Notebook as well. Luckily enough, it does indeed support compiling and building R kernels as well.\nThe first step will be to install an R kernel for the notebook which can be done using:\nconda install -c r r-essentials\nThis can be ran either from inside a notebook by prepending a ! in a cell such as !conda install -c r r-essentials or simply run it at the console if you’re in linux and in the project directory.\nThis is mostly an exibition post about how this can be done so we’re just going to show off some R stuff.\n\n# I miss this selecting over Python's Pandas:\nmtcars[order(mtcars$gear, mtcars$mpg), ]\n\n\n\nmpgcyldisphpdratwtqsecvsamgearcarb\n\n    Cadillac Fleetwood10.4 8    472.0205  2.93 5.25017.980    0    3    4    \n    Lincoln Continental10.4 8    460.0215  3.00 5.42417.820    0    3    4    \n    Camaro Z2813.3 8    350.0245  3.73 3.84015.410    0    3    4    \n    Duster 36014.3 8    360.0245  3.21 3.57015.840    0    3    4    \n    Chrysler Imperial14.7 8    440.0230  3.23 5.34517.420    0    3    4    \n    Merc 450SLC15.2 8    275.8180  3.07 3.78018.000    0    3    3    \n    AMC Javelin15.2 8    304.0150  3.15 3.43517.300    0    3    2    \n    Dodge Challenger15.5 8    318.0150  2.76 3.52016.870    0    3    2    \n    Merc 450SE16.4 8    275.8180  3.07 4.07017.400    0    3    3    \n    Merc 450SL17.3 8    275.8180  3.07 3.73017.600    0    3    3    \n    Valiant18.1 6    225.0105  2.76 3.46020.221    0    3    1    \n    Hornet Sportabout18.7 8    360.0175  3.15 3.44017.020    0    3    2    \n    Pontiac Firebird19.2 8    400.0175  3.08 3.84517.050    0    3    2    \n    Hornet 4 Drive21.4 6    258.0110  3.08 3.21519.441    0    3    1    \n    Toyota Corona21.5 4    120.1 97  3.70 2.46520.011    0    3    1    \n    Merc 280C17.8 6    167.6123  3.92 3.44018.901    0    4    4    \n    Merc 28019.2 6    167.6123  3.92 3.44018.301    0    4    4    \n    Mazda RX421.0 6    160.0110  3.90 2.62016.460    1    4    4    \n    Mazda RX4 Wag21.0 6    160.0110  3.90 2.87517.020    1    4    4    \n    Volvo 142E21.4 4    121.0109  4.11 2.78018.601    1    4    2    \n    Datsun 71022.8 4    108.0 93  3.85 2.32018.611    1    4    1    \n    Merc 23022.8 4    140.8 95  3.92 3.15022.901    0    4    2    \n    Merc 240D24.4 4    146.7 62  3.69 3.19020.001    0    4    2    \n    Fiat X1-927.3 4     79.0 66  4.08 1.93518.901    1    4    1    \n    Honda Civic30.4 4     75.7 52  4.93 1.61518.521    1    4    2    \n    Fiat 12832.4 4     78.7 66  4.08 2.20019.471    1    4    1    \n    Toyota Corolla33.9 4     71.1 65  4.22 1.83519.901    1    4    1    \n    Maserati Bora15.0 8    301.0335  3.54 3.57014.600    1    5    8    \n    Ford Pantera L15.8 8    351.0264  4.22 3.17014.500    1    5    4    \n    Ferrari Dino19.7 6    145.0175  3.62 2.77015.500    1    5    6    \n    Porsche 914-226.0 4    120.3 91  4.43 2.14016.700    1    5    2    \n    Lotus Europa30.4 4     95.1113  3.77 1.51316.901    1    5    2    \n\n\n\n\n\nmtcars[order(mtcars$gear, mtcars$mpg), ] %>%\n    ggplot(aes(disp, hp, colour = cyl)) + \n    geom_point()\n\n\n\n\n\n\n\n\ncrimes <- data.frame(state = tolower(rownames(USArrests)), USArrests)\n\n# Equivalent to crimes %>% tidyr::pivot_longer(Murder:Rape)\n vars <- lapply(names(crimes)[-1], function(j) {\ndata.frame(state = crimes$state, variable = j, value = crimes[[j]])\n})\ncrimes_long <- do.call(\"rbind\", vars)\n\nstates_map <- map_data(\"state\")\nggplot(crimes_long, aes(map_id = state)) +\n    geom_map(aes(fill = value), map = states_map) +\n    expand_limits(x = states_map$long, y = states_map$lat) +\n    facet_wrap( ~ variable)\n\n\n\n\n\n\n\nI did also try to use ggvis as well but it just wont display properly so that’s unfortunately out."
  },
  {
    "objectID": "posts/nbks/2022-09-08-reviewing-fastai-2022-coursework-thebeginning.html",
    "href": "posts/nbks/2022-09-08-reviewing-fastai-2022-coursework-thebeginning.html",
    "title": "The Lastest Lectures For Fastai are Online.",
    "section": "",
    "text": "If you’re not familiar with the list of frameworks and libraries that exist for Deep Learning - or even if you are - then you might not be aware of Fastai. This is both a Deep Learning Framework built on top of Pytorch as well as an online curriculum to quickly become productive using Deep Learning for your own interests. Jeremy is a very solid teacher and I recommend taking the course if you’re interested in learning how this all works. And, I don’t mean that simply because I like the lectures but also because I’ve taken them before and I’ve also built my own simple projects out of the lectures done by him - and Rachel since she’s also assisting with building the course. One really big plus for this course is that you’ll start by actually building a model and using it: in this version it will be Birds vs Forests instead of Cats vs Dogs like used to be. As Jeremy points out in the first lecture, most people don’t learn in an academic way. By that we mean they start with theory and then learn how to interact with the system. Instead, we learn a few basic ideas and then toss ourselves in to apply and learn as we go.\n\n\nLately, I’ve been playing a game called The Cycle: Frontier which is a kind of Extraction Shooter game. What this means is that players are dropped into a semi-persistent server to collect items, kill creatures and even kill other players. Really, it is up to the player to decide how they interact with and play the game. But being a shooter means that there are guns - and since there are guns there are categories of guns. Since fantasy is inspired by the real world in some sense, the question I would like to ask is Knowing Models are Derived from the Real World Weapons, can we build a classifier based on real life weapons that can correctly predict Fantasy Weapons?\nThis post, like the lectures, is not going to be about how this all works but instead is going to be a simple application and retrospective. That being said, let’s start! We’ll start with the important imports for getting this working.\n\nimport os\nfrom pathlib import Path\nfrom time import sleep\n\n\nfrom duckduckgo_search import ddg_images # Will write more about this later.\nfrom fastdownload import download_url    # Will need to explore this more.\n\nfrom fastcore.all import *\nfrom fastai.vision.all import *     # This is for the CNN learner.\nfrom fastai.vision.widgets import * # This is required for the cleaner later.\n\nThe first three imports you should hopefully be familiar with. Everything else you should simply grant for now.\n\n# This is a function from the notebook:\ndef search_images(term, max_images=200):\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nurls = search_images('assult rifle photos', max_images=1)\nurls[0]\n\n'http://allhdwallpapers.com/wp-content/uploads/2016/07/Assault-Rifle-5.jpg'\n\n\nSo, we’re going to download an example image using the helper functions so far and make sure its working:\n\ndest = Path('..', '__data', 'example-ar-gun.png')\ndownload_url(urls[0], dest, show_progress=False)\n\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\nNow let’s check for a DMR and make sure that is sane:\n\ndmrUrls = search_images('dmr photos', max_images=1)\ndest = Path('..', '__data', 'example-dmr-gun.png')\ndownload_url(dmrUrls[0], dest, show_progress=False)\n\nim = Image.open(dest).to_thumb(256,256)\nim\n\n\n\n\nSo, now we need data from the Internet:\n\nsearches = 'assault rifle','dmr'\npath = Path('..', '__data', 'ar_or_dmr')\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server and blocking responses\n    resize_images(path/o, max_size=400, dest=path/o)\n\n/usr/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\nThe below just checks to make sure that the downloaded images are in fact images. And, then we’re iterating though the results to remove what are failed images:\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n4\n\n\nWe can even check the source code using ?? and see for ourselves that is what it is doing:\n\n??verify_images\n\nSignature: verify_images(fns)\nDocstring: Find images in `fns` that can't be opened\nSource:   \ndef verify_image(fn):\n    \"Confirm that `fn` can be opened\"\n    try:\n        im = Image.open(fn)\n        im.draft(im.mode, (32,32))\n        im.load()\n        return True\n    except: return False\nFile:      ~/.local/lib/python3.10/site-packages/fastai/vision/utils.py\nType:      function\n\n\nDataBlocks are are really good idea which is a lazy wrapper that defines how and what to do with the downloaded images in preparation for use in a model. There is more information at the docs page here where you can check out other ways to use this. For now, we’re following Jeremy’s example and just mostly running the code as is.\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=12)\n\n\n\n\nLooking over some of htese images, we can already tell that we’re getting images that are not what we’re after. The second image in the top row is some kind radio which is not what we want. Regardless, we’re again going to simply train the model even with that image in there since we’re following Jeremy’s advice and just going to train the model and we’ll deal with the strange Radio later.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(7)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.135293\n      1.895362\n      0.333333\n      00:01\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.783494\n      1.115906\n      0.250000\n      00:01\n    \n    \n      1\n      0.578771\n      0.935797\n      0.222222\n      00:01\n    \n    \n      2\n      0.455172\n      0.774320\n      0.263889\n      00:01\n    \n    \n      3\n      0.365537\n      0.762151\n      0.263889\n      00:01\n    \n    \n      4\n      0.311685\n      0.737330\n      0.277778\n      00:01\n    \n    \n      5\n      0.263628\n      0.734078\n      0.277778\n      00:01\n    \n    \n      6\n      0.224883\n      0.718644\n      0.291667\n      00:01\n    \n  \n\n\n\nSo, this is what training will look like. The highlights are that training was very fast but - compared to many other models trained, including Jeremy’s - this one is doing pretty terrible for a Deep Learning model. The validation loss is worse than random chance which means there are very serious problems with either out Deep Learning Architecture or the data. And, the architecture is definitely not the problem so it’s the data.\n\nis_ar,_,probs = learn.predict(PILImage.create(Path('__data', 'example-dmr-gun.png')))\nprint(f\"This is a(n): {is_ar}.\")\nprint(f\"Probability it's an Assult Rifle: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a(n): assault rifle.\nProbability it's an Assult Rifle: 0.9858\n\n\n\nim = Image.open(Path('__data', 'example-dmr-gun.png')).to_thumb(512,512)\nim\n\n\n\n\nBig Oof. That’s not an Assault Rifle.\nTime to do some data cleaning. Thankfully, the Fastai library comes with a very useful function which takes the images from the data, checks what their loss is and then presents them to us so that we can remove or re-label them. Make sure you don’t make the same mistake as I did and correct import the widgets: from fastai.vision.widgets import *\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t forget once you’ve actually ran and collected what changes you want made, you’ll need actually run the below code to actually make those corrections:\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nWhile trying to train this, I found that I was having issues unless I re-built the Data Loader so that’s what we’re doing here:\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=12)\n\n\n\n\nGot some weird stuff again. Looks like some bikes as well as some other equipment that I don’t know. That would imply that using dmr is a bad search term for what we’re after. We’ll do one more training attempt to see how much harm those are doing and then we’ll consider correcting the search terms.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(7)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.281920\n      1.533394\n      0.413333\n      00:02\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.869596\n      0.949424\n      0.306667\n      00:02\n    \n    \n      1\n      0.667905\n      0.852354\n      0.266667\n      00:02\n    \n    \n      2\n      0.529514\n      0.767100\n      0.213333\n      00:02\n    \n    \n      3\n      0.418889\n      0.783195\n      0.213333\n      00:01\n    \n    \n      4\n      0.338373\n      0.754332\n      0.226667\n      00:02\n    \n    \n      5\n      0.277043\n      0.730826\n      0.226667\n      00:02\n    \n    \n      6\n      0.237752\n      0.718148\n      0.226667\n      00:02\n    \n  \n\n\n\nLooks like our validation loss is still struggling so it’s time to update our search term from dmr to designated marksmen rifle. Speedrun time!\n\nsearches = 'assault rifle','designated marksmen rifle'\npath = Path('..', '__data', 'ar_or_dmr_v2')\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    resize_images(path/o, max_size=600, dest=path/o)\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\n\n(#7) [None,None,None,None,None,None,None]\n\n\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=12)\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(7)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.279173\n      0.758920\n      0.247312\n      00:02\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.725494\n      0.552330\n      0.215054\n      00:02\n    \n    \n      1\n      0.572957\n      0.452633\n      0.161290\n      00:02\n    \n    \n      2\n      0.429993\n      0.460270\n      0.172043\n      00:02\n    \n    \n      3\n      0.329015\n      0.517723\n      0.215054\n      00:02\n    \n    \n      4\n      0.260919\n      0.542659\n      0.215054\n      00:02\n    \n    \n      5\n      0.212036\n      0.543762\n      0.204301\n      00:02\n    \n    \n      6\n      0.176182\n      0.541302\n      0.215054\n      00:02\n    \n  \n\n\n\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n\n\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=12)\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(7)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.303078\n      1.055721\n      0.304348\n      00:02\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.759311\n      0.487650\n      0.250000\n      00:02\n    \n    \n      1\n      0.568219\n      0.461627\n      0.184783\n      00:02\n    \n    \n      2\n      0.444043\n      0.539364\n      0.163043\n      00:02\n    \n    \n      3\n      0.340317\n      0.588352\n      0.141304\n      00:02\n    \n    \n      4\n      0.269084\n      0.642941\n      0.152174\n      00:02\n    \n    \n      5\n      0.217371\n      0.619099\n      0.152174\n      00:02\n    \n    \n      6\n      0.183307\n      0.610297\n      0.163043\n      00:02\n    \n  \n\n\n\n\nis_ar,_,probs = learn.predict(PILImage.create(Path('..', '__data', 'example-dmr-gun.png')))\nprint(f\"This is a(n): {is_ar}.\")\nprint(f\"Probability it's an Assult Rifle: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a(n): designated marksmen rifle.\nProbability it's an Assult Rifle: 0.0029\n\n\nThat is much better than what we were getting. Now for the real test: Can the model tell the difference between an Assult Rifle and a DMR from the game’s wiki.\n\nfor image in [PILImage.create(Path('..', '__data', 'cycle-dmr-gun.png')), PILImage.create(Path('..', '__data', 'cycle-ar-gun.png'))]:\n    plt.figure()\n    plt.imshow(image)\n\n\n\n\n\n\n\n\nis_ar,_,probs = learn.predict(PILImage.create(Path('..', '__data', 'cycle-dmr-gun.png')))\nprint(f\"This is a(n): {is_ar}.\")\nprint(f\"Probability it's an Assult Rifle: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a(n): assault rifle.\nProbability it's an Assult Rifle: 0.9968\n\n\n\nis_ar,_,probs = learn.predict(PILImage.create(Path('..', '__data', 'cycle-ar-gun.png')))\nprint(f\"This is a(n): {is_ar}.\")\nprint(f\"Probability it's an Assult Rifle: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a(n): assault rifle.\nProbability it's an Assult Rifle: 0.9805\n\n\nLooking like either I don’t have enough data or I don’t know enough about the domain because it is struggling to figure it out.\n\n\n\nDMR also stands for Digital Mobile Radio which is why we were getting all those weird results earlier in the post."
  },
  {
    "objectID": "posts/nbks/2022-09-28-cycle-jobs-part-one.html",
    "href": "posts/nbks/2022-09-28-cycle-jobs-part-one.html",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part One.",
    "section": "",
    "text": "This post series was inspired by the Korolev Job And Two Smoking Barrels where upon accepting the quest I realized how terrible the rewards were in comparison. For those that don’t play the game a little background will be necessary. The Cycle: Frontier is an Evacuation Shooter game and there are three main Corporations - or Organizations, if you prefer - who act as quest and reward givers in the game. There are two kinds of these: Campaign and Jobs. Whereas the campaign quests will push you along to different areas of the world, instead the Jobs function as a way to collect scrips and excuse to send players to the planet.\nThere are three kinds of quests: 1. Collect Stuff. 2. Deposit Stuff. 3. Kill Stuff: including players.\nFor Deposit Jobs, you carry the requested items to a Dead Drop and then deposit the items in question. For this one, it requires you deposit a gun you purchase from the shop. Now, you could find this weapon or loot it from other people but those are not garunteed at all. The Kmark - which is cash, basically - reward is $19,000 and the gun it wants you to deposit is $22,000 so there is no reason to take this job unless you already have this gun. Anyways, lets get to the fun part."
  },
  {
    "objectID": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#scraping-and-cleaning-cycle-data",
    "href": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#scraping-and-cleaning-cycle-data",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part One.",
    "section": "Scraping and Cleaning Cycle Data",
    "text": "Scraping and Cleaning Cycle Data\nSo, we’ll start with the normal imports for doing data in Python.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport requests as r\n\nThankfully, there is an official wiki for the game which is maintained by both the Developers and the Community together. We’re going to pull the data from there as that should be the most up to date data. Like most online data scraping, there is some try-fail loops to getting what you’re after from the webpage. Since there are three organizations, there are three tables with jobs we’d like to pull from the website. After some trial and error I found that using match=\"Name\" was perfect for pulling the tables out of the webpage.\n\nurl = \"https://thecyclefrontier.wiki/wiki/Jobs\"\n\nsite = pd.read_html(url, match=\"Name\",\n    converters = {\n        \"Name\": str,\n        \"Description\": str, \n        \"Unlocked\": int, \n        \"Tasks\": str,\n        \"Rewards\": str})\n\nYou may notice the addition of converters argument above which is a really useful feature I didn’t know previously; basically, if you know the column names coming in then you can tell Pandas what data type you want so you don’t have to convert later. So, what does the data look like?\n\nsite[0].head(8)\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Unlock Level\n      Difficulty\n      Tasks\n      Rewards\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      4.0\n      Easy\n      Collect: 2 Hydraulic Piston 10 Hardened Metals\n      3800  K-Marks  1  Korolev Scrip  15  Korolev R...\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      7.0\n      Medium\n      Collect: 4 Derelict Explosives\n      11000  K-Marks  8  Korolev Scrip  52  Korolev ...\n    \n    \n      5\n      11000\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6\n      8\n      Korolev Scrip\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7\n      52\n      Korolev Reputation\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nThat is not really what we were hoping would come in. Checking the actual site, this is caused by a table which exists inside one of the row cells. But - like in the previous post - this can still be used after some work. So, lets get to work!"
  },
  {
    "objectID": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#fixing-the-job-rewards",
    "href": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#fixing-the-job-rewards",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part One.",
    "section": "Fixing the Job Rewards",
    "text": "Fixing the Job Rewards\nWe dont need most of the columns for what we’re going to be accomplishing so we’re going to pull them out. I’m going to do a copy as pandas sometimes doesn’t play so nicely with updates. What I’ve found is that since Pandas uses pointers underneath, sometimes when doing updates to slices I don’t always get what I expect. So, we’ll splice and copy to only get the data we care about in its own independent dataframe.\n\nrewardsSubset = site[0][[\"Name\", \"Description\", \"Difficulty\"]].copy()\nrewardsSubset\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Difficulty\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      Easy\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      Medium\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      183\n      470\n      Korolev Reputation\n      NaN\n    \n    \n      184\n      No Expiry Date\n      There you are, finally! There's been an accide...\n      Medium\n    \n    \n      185\n      6300\n      K-Marks\n      NaN\n    \n    \n      186\n      9\n      Korolev Scrip\n      NaN\n    \n    \n      187\n      62\n      Korolev Reputation\n      NaN\n    \n  \n\n188 rows × 3 columns\n\n\n\nThese column names are not useful so we’re going to correct those so they make sense with the project. We’re going to call the final column Job which will make more sense as the work gets done.\n\nrewardsSubset.columns = [\"Units\", \"Rewards\", \"Job\"]\nrewardsSubset.head()\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      Easy\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      Medium\n    \n  \n\n\n\n\nSo, looking at above we can see that we have extra data in the Job Column which is no longer appropriate. We’re going to simply fill that column with a null value: np.NaN\n\nrewardsSubset.Job = np.NaN\nrewardsSubset.head(12)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      NaN\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      NaN\n    \n    \n      5\n      11000\n      K-Marks\n      NaN\n    \n    \n      6\n      8\n      Korolev Scrip\n      NaN\n    \n    \n      7\n      52\n      Korolev Reputation\n      NaN\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n      NaN\n    \n    \n      9\n      6900\n      K-Marks\n      NaN\n    \n    \n      10\n      9\n      Korolev Scrip\n      NaN\n    \n    \n      11\n      62\n      Korolev Reputation\n      NaN\n    \n  \n\n\n\n\nSo, now for the hard part: getting the Job Title into the Job Column. Looking at the data above, we can see that the Job Title is always stored in a multiple of four. We can confirm this by simply dividing the total number of columns by 4 just to be sure.\n\n# This should be divisible by 4 since they rewards for jobs are always in this format now.\nlen(rewardsSubset) / 4\n\n47.0\n\n\nAnd, we have a perfect divide! Good! This is since each Job always hands out Kmarks, a matching Corp Scrip and Corp Reputation. So, what we need to do now is pull the Job Title from the Units column and insert it into the next three columns under Job. To do this, we’re going to build a range of values which are multiples of 4 starting at 0 and up to the total amount of jobs. We don’t want to hard code this since the count of jobs should be expected to change over time.\n\ntopIndex = len(rewardsSubset) / 4 - 3\nindex = range( 0, 44, 4)\n\nNext we’ll want a numpy array of the offsets. We don’t want to use a list because then it will add the values to a python list instead of creating a set of indexes. In effect, we’re trying to take advantage of Broadcasting in numpy. We’ll do an illustration of this quick.\n\nlistMistake = [1,2,3]\nbroadcastCorrect = np.array(listMistake)\n\n[index[1]] + listMistake, index[1] + broadcastCorrect\n\n([4, 1, 2, 3], array([5, 6, 7]))\n\n\nAbove you can see [4, 1, 2, 3] is definitely not what we’re after. So, after setting up the proper offset lets make sure we’re getting what we want. I often sanity check my initial design since experience as taught me you can still trip even after the initial testing works. So, lets do that now.\n\noffset = np.array([1, 2, 3])\n\n\n# this is how we'll iterate; proof it works.\nfor i in index[:3]:\n    aJob = rewardsSubset.iloc[i, 0]\n    print(f'{aJob} is at index {i}')\n\nNew Mining Tools is at index 0\nExplosive Excavation is at index 4\nMining Bot is at index 8\n\n\nAnd, there we go! We’re getting exactly what we wanted and expected. This is also a good initial test for the loop which we’re going to tuck into a function at the end of all this. So, now to test the logic of swapping the values from the Unit Column to the Job Column.\n\n# Do the thing:\naJob = rewardsSubset.iloc[index[0], 0]\nindexes = index[0] + offset\nrewardsSubset.iloc[ indexes, 2 ] = aJob\nrewardsSubset.head(9)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      NaN\n    \n    \n      1\n      3800\n      K-Marks\n      New Mining Tools\n    \n    \n      2\n      1\n      Korolev Scrip\n      New Mining Tools\n    \n    \n      3\n      15\n      Korolev Reputation\n      New Mining Tools\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      NaN\n    \n    \n      5\n      11000\n      K-Marks\n      NaN\n    \n    \n      6\n      8\n      Korolev Scrip\n      NaN\n    \n    \n      7\n      52\n      Korolev Reputation\n      NaN\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n      NaN\n    \n  \n\n\n\n\n\nfor i in index:\n    aJob = rewardsSubset.iloc[i, 0]\n    indexes = i + offset\n    rewardsSubset.iloc[ indexes, 2 ] = aJob\n\nrewardsSubset.head(12)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      NaN\n    \n    \n      1\n      3800\n      K-Marks\n      New Mining Tools\n    \n    \n      2\n      1\n      Korolev Scrip\n      New Mining Tools\n    \n    \n      3\n      15\n      Korolev Reputation\n      New Mining Tools\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      NaN\n    \n    \n      5\n      11000\n      K-Marks\n      Explosive Excavation\n    \n    \n      6\n      8\n      Korolev Scrip\n      Explosive Excavation\n    \n    \n      7\n      52\n      Korolev Reputation\n      Explosive Excavation\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n      NaN\n    \n    \n      9\n      6900\n      K-Marks\n      Mining Bot\n    \n    \n      10\n      9\n      Korolev Scrip\n      Mining Bot\n    \n    \n      11\n      62\n      Korolev Reputation\n      Mining Bot\n    \n  \n\n\n\n\nPerfect! Now all we have to do is cut the Units Columns where the Job Title still remains. Luckily, the np.NaN has remained so we can collect the indexes for Job where that values exists. And, then simply get rid of them.\n\n# Kill the NA's\ncutNA = rewardsSubset.Job.isna()\nrewardsSubset[ ~cutNA ].head(15)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      1\n      3800\n      K-Marks\n      New Mining Tools\n    \n    \n      2\n      1\n      Korolev Scrip\n      New Mining Tools\n    \n    \n      3\n      15\n      Korolev Reputation\n      New Mining Tools\n    \n    \n      5\n      11000\n      K-Marks\n      Explosive Excavation\n    \n    \n      6\n      8\n      Korolev Scrip\n      Explosive Excavation\n    \n    \n      7\n      52\n      Korolev Reputation\n      Explosive Excavation\n    \n    \n      9\n      6900\n      K-Marks\n      Mining Bot\n    \n    \n      10\n      9\n      Korolev Scrip\n      Mining Bot\n    \n    \n      11\n      62\n      Korolev Reputation\n      Mining Bot\n    \n    \n      13\n      7600\n      K-Marks\n      None of your Business\n    \n    \n      14\n      10\n      Korolev Scrip\n      None of your Business\n    \n    \n      15\n      90\n      Korolev Reputation\n      None of your Business\n    \n    \n      17\n      10000\n      K-Marks\n      Insufficient Processing Power\n    \n    \n      18\n      11\n      Korolev Scrip\n      Insufficient Processing Power\n    \n    \n      19\n      110\n      Korolev Reputation\n      Insufficient Processing Power"
  },
  {
    "objectID": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#function-to-build-job-rewards",
    "href": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#function-to-build-job-rewards",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part One.",
    "section": "Function to build Job Rewards",
    "text": "Function to build Job Rewards\nNow that we have all this we can push it into a function and run it against all the different Corporation tables.\n\ndef buildJobsRewards(data):\n    # Function to take job rewards data and return a cleaned version\n\n    rewardsSubset = data[[\"Name\", \"Description\", \"Difficulty\"]].copy()\n    rewardsSubset.columns = [\"Units\", \"Rewards\", \"Job\"]\n\n    index = range( 0, len(rewardsSubset) - 4, 4)\n    offset = np.array([1, 2, 3])\n\n    rewardsSubset.Job = np.NaN\n\n    for i in index:\n        aJob = rewardsSubset.iloc[i, 0]\n        indexes = i + offset\n        rewardsSubset.iloc[ indexes, 2 ] = aJob\n        \n    cutNA = rewardsSubset.Job.isna()\n    rewardsSubset = rewardsSubset[ ~cutNA ]\n\n    rewardsSubset = rewardsSubset.assign(\n        Units = rewardsSubset['Units'].astype(int)\n    )\n\n    return rewardsSubset\n\nAnd, the final test!\n\nKorolevRewards = buildJobsRewards( site[0] )\nicaRewards = buildJobsRewards( site[1] )\nosirisRewards = buildJobsRewards( site[2] )\n\n\nKorolevRewards.head(9)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      1\n      3800\n      K-Marks\n      New Mining Tools\n    \n    \n      2\n      1\n      Korolev Scrip\n      New Mining Tools\n    \n    \n      3\n      15\n      Korolev Reputation\n      New Mining Tools\n    \n    \n      5\n      11000\n      K-Marks\n      Explosive Excavation\n    \n    \n      6\n      8\n      Korolev Scrip\n      Explosive Excavation\n    \n    \n      7\n      52\n      Korolev Reputation\n      Explosive Excavation\n    \n    \n      9\n      6900\n      K-Marks\n      Mining Bot\n    \n    \n      10\n      9\n      Korolev Scrip\n      Mining Bot\n    \n    \n      11\n      62\n      Korolev Reputation\n      Mining Bot\n    \n  \n\n\n\n\n\nicaRewards.head(9)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      1\n      4400\n      K-Marks\n      Water Filtration System\n    \n    \n      2\n      1\n      ICA Scrip\n      Water Filtration System\n    \n    \n      3\n      15\n      ICA Reputation\n      Water Filtration System\n    \n    \n      5\n      7500\n      K-Marks\n      New Beds\n    \n    \n      6\n      9\n      ICA Scrip\n      New Beds\n    \n    \n      7\n      62\n      ICA Reputation\n      New Beds\n    \n    \n      9\n      13000\n      K-Marks\n      Station Defense\n    \n    \n      10\n      12\n      ICA Scrip\n      Station Defense\n    \n    \n      11\n      130\n      ICA Reputation\n      Station Defense\n    \n  \n\n\n\n\n\nosirisRewards.head(9)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      1\n      2200\n      K-Marks\n      Lab equipment\n    \n    \n      2\n      1\n      Osiris Scrip\n      Lab equipment\n    \n    \n      3\n      13\n      Osiris Reputation\n      Lab equipment\n    \n    \n      5\n      8100\n      K-Marks\n      Surveillance Center\n    \n    \n      6\n      8\n      Osiris Scrip\n      Surveillance Center\n    \n    \n      7\n      43\n      Osiris Reputation\n      Surveillance Center\n    \n    \n      9\n      8100\n      K-Marks\n      Gun Manufacturing\n    \n    \n      10\n      8\n      Osiris Scrip\n      Gun Manufacturing\n    \n    \n      11\n      52\n      Osiris Reputation\n      Gun Manufacturing"
  },
  {
    "objectID": "posts/nbks/2022-10-14-cycle-calculate-job-values.html",
    "href": "posts/nbks/2022-10-14-cycle-calculate-job-values.html",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Three.",
    "section": "",
    "text": "This is the third post in a series about collecting, cleaning and analyzing which Jobs are worth doing in The Cycle: Frontier game. If you’ve found this post before the others I would recommend the other posts first to catch up: Post One and Post Two. We’ll pick up where we left off having separate datasets for the faction rewards as well as all the loot. Let’s get our imports!\nThere are two tasks we’ll need to adjust before we continue. The first is that our Tasks work was only for one faction so we’ll need to loop through each faction’s tasks and then append them all together into a single dataset; the code is included simply for observation.\nThe second task is that we need a single dataset for all the job rewards. We’ll definitely want to tag each so we don’t lose track of which task belongs to which Faction.\nFor this analysis, we’re really only interested in the Kmarks so we’ll need to only pull those rows; we’ll do some work with the others later but for now just the money.\nJoins are a complicated topic which I’m not going to flesh out here. In this instance, what we want is the loot table with the Kmark value connected to the jobs table with respect to the name of the loot. And, we’ll tell Pandas to join those together below making sure that as long as it exists in the left table that it gets connected to something in the right table.\nWe’ll now multiply the count of the loot times their values to get the cost per resource in the task.\nNow we’ll just group by the name of the task to and take the sume of each to get the total cost per task.\nThis is the first piece we’ll need to get the results we’re after; the other part is all the jobs we collected in the previous post - and brought here.\nNow we have the rewards and the cost per task we can finally calculate the Balance of each job!"
  },
  {
    "objectID": "posts/nbks/2022-10-14-cycle-calculate-job-values.html#conclusions---and-questions",
    "href": "posts/nbks/2022-10-14-cycle-calculate-job-values.html#conclusions---and-questions",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Three.",
    "section": "Conclusions - and Questions!",
    "text": "Conclusions - and Questions!\nSo, now that we have the data let’s work on some questions!\nGiven any particular faction, which task has the highest Balance?\n\nbalanceMax = results.Balance.max()\nresults.loc[results.Balance == balanceMax]\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Faction\n      Cost\n      Balance\n    \n  \n  \n    \n      91\n      227000\n      K-Marks\n      Striking Big\n      ICA\n      0.0\n      227000.0\n    \n  \n\n\n\n\nLooks like it is the Job Striking Big. What’s this job about?: > Damn it! We ran out of Fuel for our Radiation Shields here on the Station. And there’s no replacement for Nanite Infused Crude Oil. You know what to do.\nIt’s collecting Oil Cannisters! Those are worth a lot of money so it’s not a surprise that a task which wants you to collect them would also pay out so highly.\nWhat about the job that pays the least?\n\nbalanceMin = results.Balance.min()\nresults.loc[results.Balance == balanceMin]\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Faction\n      Cost\n      Balance\n    \n  \n  \n    \n      94\n      2200\n      K-Marks\n      Lab equipment\n      Osiris\n      676.0\n      1524.0\n    \n  \n\n\n\n\nWell, it our good friends Osiris failing the station again. That looks like a low level job from a cost and reward that low. Not really much of a surprise here.\nSo, what about the mean balance - and how many tasks are there which reward more than that average value?\n\nbalanceMean = results.Balance.mean()\nf\"${round(balanceMean, 2)}\"\n\n'$20600.89'\n\n\n\nlen( results.loc[results.Balance >= balanceMean])\n\n40\n\n\n\n# How many are there per faction?\nresults.loc[\n    results.Balance >= balanceMean ]\\\n        .groupby('Faction')\\\n        .count()\\\n        .reset_index()\\\n        .rename({'Units':'Count'}, axis=1)[[\n    'Faction', 'Count'\n]]\n\n\n\n\n\n  \n    \n      \n      Faction\n      Count\n    \n  \n  \n    \n      0\n      ICA\n      14\n    \n    \n      1\n      Korolev\n      14\n    \n    \n      2\n      Osiris\n      12\n    \n  \n\n\n\n\nAnd, unsurpisingly Osiris is slightly behind but they’re mostly the same.\nOk, so what’s the cost of the Job that inspired all this work: And Two Smoking Barrels?\n\nresults.loc[results.Job.str.contains(\"Barrel\")]\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Faction\n      Cost\n      Balance\n    \n  \n  \n    \n      39\n      19000\n      K-Marks\n      And two smoking Barrels\n      Korolev\n      0.0\n      19000.0\n    \n  \n\n\n\n\nWait a minute! There should be a cost here and it is missing! Well, that’s because there are no guns in the loot table. So, there is no cost when we connect the data together. And, after some further checking there are some missing values here. That quest we checked above with the Oil? They’re not named the same betwee the tables: NiC Oil vs NiC Oil Cannister. It’s not the only one like this either.\nLooks like we’ve got more work to do."
  },
  {
    "objectID": "posts/nbks/2022-12-14-first-time-pareto-plot.html",
    "href": "posts/nbks/2022-12-14-first-time-pareto-plot.html",
    "title": "First Time Seeing the Pareto Plot",
    "section": "",
    "text": "Pareto Plot?\nI have spent a good amount of time taking online classes and reading books around Machine Learning, Data Science and Visualizations and have never heard of this plot before. I found it while looking around at Interview questions for a Data Analyst role and it would be a shame to have lost a job to not knowing this. Again though, I am not a stranger to this kind of content and have never ever seen this plot before discussed.\nLooking around at some of the cheat sheets, it does not even exist. In this cheatsheet, the closest is the Column + line timeline here. And, if you search it by name in the r-charts site it also doesn’t exist: \n\n\nSo, What is This?\nSo, the Pareto Plot is a plot which attempts to visualize which factor among a set of categories has the most impact mixing a Bar Chart and a Cumulative Sum Chart. There is no default plot for this in matplotlib nor for seaborn so it wil have to be made itself. In practice, this is just two graphs on the same plot using the same data so it’s not too hard to solve.\nThe good news is that this has already been solved thanks to this wonderful gentleman Tyler Marrs. So, we’ll use his code - and take it apart and explain why it works, I might add - in a play example. And no, we’re not going to be using his example since I honestly do not find it believable.\nWe’re going to pretend that we are a Non-Profit that invests money in a bunch of different kinds of companies. And, we’re worried about what our legal obligations are so we’ve cleaned the data so that we can only see the type of business it is along with the actual amount of investments. So, we’ll make some pretend data with the Faker package.\n\n# fake compay type and fake investments.\ncompany = [ fake.company_suffix() for _ in range(100)]\ninvestments = [ random.randint(1,10) for _ in range(100)]\n\nIf you ask me this question normally, I would simply use a group by along some kind of aggregation function like this:\n\ndata = pd.DataFrame({'company':company, 'investments':investments})\nsumData = data.groupby('company').sum().reset_index().sort_values('investments', ascending=False)\nsumData\n\n\n\n\n\n  \n    \n      \n      company\n      investments\n    \n  \n  \n    \n      3\n      Ltd\n      127\n    \n    \n      0\n      Group\n      117\n    \n    \n      5\n      and Sons\n      108\n    \n    \n      4\n      PLC\n      95\n    \n    \n      2\n      LLC\n      72\n    \n    \n      1\n      Inc\n      63\n    \n  \n\n\n\n\nFor me - and I would expect most people - this would work fine since this is what was asked for. But, someone is going to apparently ask me to give them the results as one of these Pareto Plots. Let’s look through Tyler’s code here and figure out what he’s done step by step.\n\nxlabel = x\nylabel = y\ntmp = df.sort_values(y, ascending=False)\nx = tmp[x].values\ny = tmp[y].values\nweights = y / y.sum()\ncumsum = weights.cumsum()\n\nOk, so first we’ll grab the names of the columns create a new data frame by sorting the y column descending. Then the values are extracted from both columns for later use. Next, the percentage is calculated by broadcasting the sum of values to divide by all of them. Then, the cumulative sum of all of percentages is added up.\n\nfig, ax1 = plt.subplots()\nax1.bar(x, y)\nax1.set_xlabel(xlabel)\nax1.set_ylabel(ylabel)\n\nNext, a subplot is created at the bar graph is attached on one of the axis followed by labelling the X and Y axis labels.\n\nax2 = ax1.twinx()\nax2.plot(x, cumsum, '-ro', alpha=0.5)\nax2.set_ylabel('', color='r')\nax2.tick_params('y', colors='r')\n\nI had never seen the twinx() function before but my guess would of been that it was to share the first axis for the second subplot. And, that looks to be correct: The twinx() function in pyplot module of matplotlib library is used to make and return a second axes that shares the x-axis. The -ro is what is setting the red circle at each point; this is something from matplotlib I had to look up. The Y label is blanked and the colors are set to red.\n\nvals = ax2.get_yticks()\nax2.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n\nThe values from the the Y Axis are pulled and then their format is corrected/updated. That is a really nice touch.\n\n# hide y-labels on right side\nif not show_pct_y:\n    ax2.set_yticks([])\n\nformatted_weights = [pct_format.format(x) for x in cumsum]\nfor i, txt in enumerate(formatted_weights):\n    ax2.annotate(txt, (x[i], cumsum[i]), fontweight='heavy')   \n\nThere is a default argument to suppress the Y axis ticks - which I have kept as default. Then the percentage numbers are added to each of the points before they are displayed.\n\npareto_plot(sumData, x='company', y='investments', title='Investment by Company Type')\n\n\n\n\nThere we go! The Ltd business type is where we’re investing most of our money. I have to admit that I don’t find this chart very useful at all. Looking around, it appears that it is commonly used for helping visualize malfunctions in a manufacturing context. Maybe if there were 20 of these categories then I could perhaps see a use but I’m not well convinced of this.  Anyways, that’s all there is to the Pareto Plot."
  },
  {
    "objectID": "posts/nbks/2023-01-15-Exploring-Tv-Data-With-Spark-Part-2.html",
    "href": "posts/nbks/2023-01-15-Exploring-Tv-Data-With-Spark-Part-2.html",
    "title": "Exploring Television Data With Spark - Part 2",
    "section": "",
    "text": "In the last post, we reviewed some data cleaning and how to use Apache spark. We’re going to move on to visualizations; while I like the data tables and numbers, when showing the conclusions to others then visualizations are the way to go. Numbers without context are meaningless and the reason they mean something to the individual making the analysis is that you have the data as context. But, those you’re showing the conclusinos too do not - and they didn’t spend hours cleaning and thinking about the data like you did.\nWe’ll add all the cleaning from before just for that context I was talking about; I would recommending seeing the part one of this series if your curious about that process.\n\n# Define the schema:\nshows = StructType([\n    StructField('Ignore', IntegerType(), True),\n    StructField('Name', StringType(), True),\n    StructField('Year', StringType(), True),\n    StructField('Guide', StringType(), True),\n    StructField('Duration', StringType(), True),\n    StructField('Genres', StringType(), True),\n    StructField('Ratings', FloatType(), True),\n    StructField('Actors', StringType(), True),\n    StructField('Votes', StringType(), True),\n])\n\ndata = spark.read.csv(\n    str(Path(\n        '_data',\n        'AllThrillerSeriesListClean.csv')),\n        header=True,\n    schema=shows)\n\n\n# Cleanup\ndata = data.drop('Ignore')\n\ndata = data.withColumn('isOngoing',     # Target this column\n    F.udf(                              # define a udf\n        lambda x: x.strip()[-1] == '–') # use the logic we already wrote.\n    ('Year')                            # pass the column we're applying this to.\n)\n\ndata = data.withColumn('startYear', F.udf( lambda x: x.split('–')[0])('Year'))\ndata = data.withColumn('Votes', F.udf( lambda x: int(x.replace(',', '')))('Votes'))\ndata = data.withColumn('Votes', F.cast(IntegerType(), data['Votes']))\n\ndata = data.withColumn('Genre', F.split(data['Genres'], ','))\ndata = data.withColumn('Genre', F.explode(data['Genre']))\n\ndata = data.withColumn('Actor', F.split(data['Actors'], ','))\ndata = data.withColumn('Actor', F.explode(data['Actor']))\n\n# Gotta clean up the inputs:\ndata = data.withColumn('Genre', F.udf(lambda x: x.strip())('Genre'))\ndata = data.withColumn('Actor', F.udf(lambda x: x.strip())('Actor'))\ndata = data.withColumn('Votes', F.udf( lambda x: int(x) )('Votes'))\n\ncData = data.select(\n    ['Name', 'Year', 'Guide', \n    'Duration', 'Ratings', 'Votes', \n    'isOngoing', 'startYear', 'Genre', 'Actor']\n)\n\ncData.show(10)\n\n+-----+------+------+--------+-------+-----+---------+---------+---------+------------------+\n| Name|  Year| Guide|Duration|Ratings|Votes|isOngoing|startYear|    Genre|             Actor|\n+-----+------+------+--------+-------+-----+---------+---------+---------+------------------+\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|   Action|        Diego Luna|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|   Action|       Kyle Soller|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|   Action| Stellan Skarsgård|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|   Action|Genevieve O'Reilly|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|Adventure|        Diego Luna|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|Adventure|       Kyle Soller|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|Adventure| Stellan Skarsgård|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|Adventure|Genevieve O'Reilly|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|    Drama|        Diego Luna|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|    Drama|       Kyle Soller|\n+-----+------+------+--------+-------+-----+---------+---------+---------+------------------+\nonly showing top 10 rows"
  },
  {
    "objectID": "posts/nbks/2023-01-15-Exploring-Tv-Data-With-Spark-Part-2.html#qq-plot-of-numeric-values",
    "href": "posts/nbks/2023-01-15-Exploring-Tv-Data-With-Spark-Part-2.html#qq-plot-of-numeric-values",
    "title": "Exploring Television Data With Spark - Part 2",
    "section": "QQ plot of Numeric Values",
    "text": "QQ plot of Numeric Values\nA Quantile-Quantile Plot is a plot which takes the data and compares it against commonly another distribution. While it is less well known, it is most commonly used as a visual indicator of whether a dataset is Normally Distributed. Since matplotlib and seaborn don’t have this built in we’ll need to use a different library this time: statsmodels.\n\nimport numpy as np\nimport statsmodels.api as sm\nimport pylab as py\n\nTo plot the data we’ll need to pull it out of Spark and convert it to pandas; we’ve done this enough at this point. But, we’ve never used the sql api like this before. I quite like this interface since SQL queries are simply to read.\n\n# To use this, we'll need what is called a View; this is an idea from SQL\ncData.createOrReplaceTempView(\"TMP\")\ntmp = spark.sql(\"SELECT Ratings,Duration,Votes FROM TMP WHERE Duration IS NOT NULL\")\ntmp = tmp.withColumn('nDuration', F.udf( lambda x: x.replace(' min', '').replace(',', ''))('Duration'))\n\nqqData = tmp.select(['Ratings', 'nDuration', 'Votes']).toPandas()\nqqData\n\n\n\n\n\n  \n    \n      \n      Ratings\n      nDuration\n      Votes\n    \n  \n  \n    \n      0\n      8.4\n      40\n      82474\n    \n    \n      1\n      8.4\n      40\n      82474\n    \n    \n      2\n      8.4\n      40\n      82474\n    \n    \n      3\n      8.4\n      40\n      82474\n    \n    \n      4\n      8.4\n      40\n      82474\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      26228\n      6.4\n      45\n      25\n    \n    \n      26229\n      6.4\n      45\n      25\n    \n    \n      26230\n      6.4\n      45\n      25\n    \n    \n      26231\n      6.4\n      45\n      25\n    \n    \n      26232\n      6.4\n      45\n      25\n    \n  \n\n26233 rows × 3 columns\n\n\n\n\nAre Ratings Normally Distributed?\nNow we’ll check if the Ratings are Normally Distributed.\n\n# Add the the y=x line for comparison.\nsm.qqplot(qqData.Ratings, line ='45')\nplt.show()\n#py.show()\n\n\n\n\nFrom the above, this is not a normally distributed dataset. When we have situations like this, it is not uncommon to try a log tranform if the data does not fit. The reason for this is that taking the logarithmic of the data maintains the underlying mathematical relationship. So, let’s try it!\n\nplt.rcParams[\"figure.figsize\"] = (6,6)\nsm.qqplot(np.log(qqData.Ratings), line ='45')\nplt.show()\n\n\n\n\nStill No. We’ll want to avoid any model which has linear prerequisites.\n\n\nIs Duration Normally Distributed?\nThe duration of a show is very likely to not be normally distributed. The length is not random and there are very few posssible choices. But, we’re going to check anyways.\n\nplt.rcParams[\"figure.figsize\"] = (6,6)\nsm.qqplot(qqData.nDuration.astype(int), line ='45')\nplt.show()\n\n\n\n\nAnd, expected no.\n\n\nAre Votes Normally Distributed?\nWe would expect votes to be normally distributed as this is the kind of process where as you would select samples it should approach normal.\n\nplt.rcParams[\"figure.figsize\"] = (6,6)\nsm.qqplot(qqData.Votes.astype(int), line ='45')\nplt.show()\n\n\n\n\nStill even this is not normally distributed. This is starting to get worrying because at least this would be expected to be normally distributed."
  },
  {
    "objectID": "posts/nbks/2023-01-15-Exploring-Tv-Data-With-Spark-Part-2.html#ridge-plot-of-genre-ratings",
    "href": "posts/nbks/2023-01-15-Exploring-Tv-Data-With-Spark-Part-2.html#ridge-plot-of-genre-ratings",
    "title": "Exploring Television Data With Spark - Part 2",
    "section": "Ridge Plot of Genre Ratings",
    "text": "Ridge Plot of Genre Ratings\nLooking at the distribution of numeric variables can be useful. Normally, we could do a Distribution Plot or a Histogram to visualize this. But, I thought I would do something more interesting: a Ridge Plot. So, a Ridge Plot shows the distribution of values but conditioned on a category. And, we’re going to use it to check the ratings for the top categories and how they break down. First we’ll need to get our data.\n\nfrom pyspark.sql.functions import mean as sMean, col\n\n# Use results from previous post:\nresults = cData.select(['Genre', 'Ratings', 'Votes']).groupBy('Genre').agg(\n    sMean('Ratings').alias(\"Mean\"),\n    sMean('Votes').alias('Votes')\n    ).orderBy(F.desc(\"Mean\")).toPandas()\n\nresults.head(10)\n\n\n\n\n\n  \n    \n      \n      Genre\n      Mean\n      Votes\n    \n  \n  \n    \n      0\n      Biography\n      7.795238\n      33137.952381\n    \n    \n      1\n      Music\n      7.475000\n      413.625000\n    \n    \n      2\n      History\n      7.416667\n      3330.541667\n    \n    \n      3\n      Animation\n      7.270712\n      8447.388742\n    \n    \n      4\n      Fantasy\n      7.231312\n      18573.573756\n    \n    \n      5\n      Adventure\n      7.228473\n      8999.588235\n    \n    \n      6\n      Crime\n      7.225607\n      15978.954034\n    \n    \n      7\n      Comedy\n      7.223730\n      4206.774311\n    \n    \n      8\n      Family\n      7.207407\n      635.759259\n    \n    \n      9\n      Drama\n      7.203856\n      17351.695414\n    \n  \n\n\n\n\nWe’ll make a new SQL View with Apache Spark to filter out the top Genres.\n\ncData.createOrReplaceTempView(\"Explore\")\n# Using our results, collect the categories to filter for.\nvalues = ','.join([f\"'{x}'\" for x in results.head(8).Genre.tolist()])\n\nridgeSlice = spark.sql(f\"SELECT Genre,Ratings FROM Explore WHERE Genre IN( { values } )\").toPandas()\n\n\n# This is just to stop some spam.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Theming:\nsns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\npal = sns.dark_palette(\"seagreen\", n_colors=10)\n\nplt.rcParams[\"figure.figsize\"] = (14,7)\ng = sns.FacetGrid(ridgeSlice, row=\"Genre\", hue=\"Genre\", aspect=15, height=.5, palette=pal)\n\n# Draw the densities in a few steps\ng.map(sns.kdeplot, \"Ratings\",\n      bw_adjust=.5, clip_on=False,\n      fill=True, alpha=1, linewidth=1.5)\ng.map(sns.kdeplot, \"Ratings\", clip_on=False, color=\"w\", lw=2, bw_adjust=.5)\n\n# passing color=None to refline() uses the hue mapping\ng.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n\n# Define and use a simple function to label the plot in axes coordinates\ndef label(x, color, label):\n    ax = plt.gca()\n    ax.text(0, .2, label, fontweight=\"bold\", color=color,\n            ha=\"left\", va=\"center\", transform=ax.transAxes)\n\ng.map(label, \"Genre\")\n\n# Remove axes details that don't play well with overlap\ng.set_titles(\"\")\ng.set(yticks=[], ylabel=\"\")\ng.despine(bottom=True, left=True);\n\n\n\n\nMusic quickly stands out; people either like or dislike the show and there is little in between. Fantasy and Biography are clumped a little under 8 but mostly concentrated. The others are closer to what we’d expect from a normal distribution. Comedy, Animation and History are interesting since there is more spread among the ratings"
  },
  {
    "objectID": "posts/nbks/2023-01-15-Exploring-Tv-Data-With-Spark-Part-2.html#jointplot",
    "href": "posts/nbks/2023-01-15-Exploring-Tv-Data-With-Spark-Part-2.html#jointplot",
    "title": "Exploring Television Data With Spark - Part 2",
    "section": "JointPlot",
    "text": "JointPlot\nA Joinplot is a nicer looking scatterplot. It includes some of the same information but also you can add distibution information. Personally, I like the Hexagons so we’re going to use those. And, we’re going to see how the year affects the Rating over time.\n\ntry:\n    results = cData.select(['startYear', 'Ratings']).toPandas()\n    results = results.assign(\n        Year = results.startYear.astype(int)\n    )\n    results.head(15)\nexcept Exception as e:\n    print( e )\n\ninvalid literal for int() with base 10: 's2018'\n\n\nUh oh, looks like we’ve got some more cleaning to do. Let’s look at what is affected and see how widespread this is.\n\nresults.loc[ results.startYear == 's2018']\n\n\n\n\n\n  \n    \n      \n      startYear\n      Ratings\n    \n  \n  \n    \n      24638\n      s2018\n      6.1\n    \n    \n      24639\n      s2018\n      6.1\n    \n    \n      24640\n      s2018\n      6.1\n    \n    \n      24641\n      s2018\n      6.1\n    \n    \n      24642\n      s2018\n      6.1\n    \n    \n      24643\n      s2018\n      6.1\n    \n    \n      24644\n      s2018\n      6.1\n    \n    \n      24645\n      s2018\n      6.1\n    \n    \n      24646\n      s2018\n      6.1\n    \n    \n      24647\n      s2018\n      6.1\n    \n    \n      24648\n      s2018\n      6.1\n    \n    \n      24649\n      s2018\n      6.1\n    \n  \n\n\n\n\nThis might be the same movie/show which is being exploded out. We’re going to just fix it for our analysis at this point. Again, if we were getting this data from a Data Engineer then we’d want to discuss this with them to clean this upstream.\n\nresults['startYear'] = results.startYear.str.replace('s2018', '2018')\nresults = results.assign(\n    Year = results.startYear.astype(int)\n)\nresults.head(15)\n\n\n\n\n\n  \n    \n      \n      startYear\n      Ratings\n      Year\n    \n  \n  \n    \n      0\n      2022\n      8.4\n      2022\n    \n    \n      1\n      2022\n      8.4\n      2022\n    \n    \n      2\n      2022\n      8.4\n      2022\n    \n    \n      3\n      2022\n      8.4\n      2022\n    \n    \n      4\n      2022\n      8.4\n      2022\n    \n    \n      5\n      2022\n      8.4\n      2022\n    \n    \n      6\n      2022\n      8.4\n      2022\n    \n    \n      7\n      2022\n      8.4\n      2022\n    \n    \n      8\n      2022\n      8.4\n      2022\n    \n    \n      9\n      2022\n      8.4\n      2022\n    \n    \n      10\n      2022\n      8.4\n      2022\n    \n    \n      11\n      2022\n      8.4\n      2022\n    \n    \n      12\n      2022\n      8.0\n      2022\n    \n    \n      13\n      2022\n      8.0\n      2022\n    \n    \n      14\n      2022\n      8.0\n      2022\n    \n  \n\n\n\n\nSo, let’s see the Data!\n\nsns.jointplot(data=results, x=\"Year\", y=\"Ratings\", kind=\"hex\", color='seagreen');\n\n\n\n\nHonestly, I expected there to be a stronger relationship here. There is definitely a relationship here about the latest years beginning to fall though. If this was for some kind of report maybe it would be work breaking these into a Ridge Plots conditioned on the year."
  },
  {
    "objectID": "posts/nbks/2023-01-15-Exploring-Tv-Data-With-Spark-Part-2.html#violin-plot",
    "href": "posts/nbks/2023-01-15-Exploring-Tv-Data-With-Spark-Part-2.html#violin-plot",
    "title": "Exploring Television Data With Spark - Part 2",
    "section": "Violin Plot",
    "text": "Violin Plot\nViolin Plots are similar to Box and Whisker plots but they also add a visual element to show the distribution. We’re going to check out whether a show being finished or not has an impact on the Ratings.\n\nplt.rcParams[\"figure.figsize\"] = (14,7)\nsns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0), 'legend.facecolor':'white'})\npal = sns.dark_palette(\"seagreen\", n_colors=2)\n\nsns.violinplot(data=violinData, x=\"Genre\", y=\"Ratings\", split=True, hue=\"isOngoing\", palette=pal);\n\n\n\n\nThere is something really strange going on with the Biography category. In a real analysis we’d want to dig into Biography to pull this apart more."
  },
  {
    "objectID": "posts/nbks/2022-11-10-simple-idea-using-memcached.html",
    "href": "posts/nbks/2022-11-10-simple-idea-using-memcached.html",
    "title": "Simple Introduction to Memcached",
    "section": "",
    "text": "While taking a class to discover what is the currently most popular NoSQL databases for different use cases, I was informed that there is this technology called Memcached. From the Arch Wiki: > Memcached (pronunciation: mem-cashed, mem-cash-dee) is a general-purpose distributed memory caching system. It is often used to speed up dynamic database-driven websites by caching data and objects in RAM to reduce the number of times an external data source (such as a database or API) must be read.  Source\nThis was a technology developed at Live Journal to help with - well - caching commonly used values. This is interesting but why bring any of this up? Well, this is because NoSQL Database use a Key - Value pair to look up the matching values and that happens to be how this works as well. However, there are some hard limitations with this and especially related to the size of what is allowed to be cached; Looking around the default looks to be 1MB and you can configure it up to 1GB but that’s it.\nI was thinking about how you could apply this to Data Science and it’s pretty limited. For one, the only useful stuff to share across sessions would be either the data you are using or the actual trained model itself. Since the data to be used would assuredly be larger than the configured limit that is not of much use - at least for most interesting problems. And, as there is a glut of tooling for hosting applications online you are very unlikely to need to setup a cache for the model. The tooling online does this really for you with instances and such.\nBut, I did have an interesting idea about what I could use this for. After you’ve worked on problems, you’re bound to have functions that have been written to solve common problems. Keeping these means finding that code, then copying it into your project and finally using it for what you want. You could build a python package just for yourself but that seems overkill unless it’s a general topic to share with others.\nWhat if these simply functions could simply be a network share library? For example, date formats are something that I tend to need to convert with Python data frames. And, sadly there are no nice date format functions like there is in R; I do miss the R lubridate functions which has functions to convert a date into commonly needed formats: such as ymd(date) would convert the date into a Year-Month-Day format for display. I wrote a few lambda functions in python to do this for me and I would want them accessible while I do data exploration.\nSo, how would we go about doing this? First we’d need to install memcached for your Operating System; I have already done this but the guide from this Real Python goes over how you would do it for your own system. Mine being Manjaro, it didn’t include it and I had to find it on the Arch Wiki. Make sure to start the service and then we’ll start this off.\n\nfrom pymemcache.client import base\n# init a client; make sure it is already running:\nclient = base.Client(('localhost', 11211))\n\nUsing this is very simple and there really are only two functions to care about: get() and set(). If we wanted to set a value then we tell the client what the key, value pair is.\n\nclient.set('turtles', 'Turtles')\nclient.get('turtles')\n\nb'Turtles'\n\n\nAnd, that’s really all there is to using this from Python!\nI would like to point out that the results are encoded as byte type. This is not a problem for that text but is a problem as soon as you need to operate on the values.\n\nclient.set('someNumber', 42)\n\niLike = client.get('turtles')\ncount = client.get('someNumber')\n\nprint(f'I had {count} {iLike} but when I got 2 more I had {count +2} {iLike}')\n\nTypeError: can't concat int to bytes\n\n\nWe can solve this with a cast in this case at least.\n\nclient.set('someNumber', 42)\n\niLike = client.get('turtles')\ncount = client.get('someNumber')\n\nprint(f'I had {count.decode()} {iLike.decode()} but when I got 2 more I had {int(count) +2} {iLike.decode()}')\n\nI had 42 Turtles but when I got 2 more I had 44 Turtles\n\n\nSo, can we take a lambda function and put it in memcached?\n\nf = (lambda x: print(f'{x} likes turtles'))\nclient.set('iLike', f)\nclient.get('iLike')\n\nb'<function <lambda> at 0x7f9f70829000>'\n\n\nIt accepts it! That’s the good news. The bad news is that since it was converted it no longer works as a function.\n\nf(\"He\"), client.get('iLike')(\"He\")\n\nHe likes turtles\n\n\nTypeError: 'bytes' object is not callable\n\n\nYou cannot just decode it and get what we want.\n\nclient.get('iLike').decode()(\"He\")\n\nTypeError: 'str' object is not callable\n\n\nWe can work around this by serializing the object and then deserialize it on the other side. We’ll need to use dill and pickle; you may need to install the dill package since it is not part of the standard library but it is a requirement for this to work.\n\ns = dill.dumps(f)\nclient.set('cereal', s)\ndill.loads(client.get('cereal'))(\"He\")\n\nHe likes turtles\n\n\nNow we can implement the function I want as a Network Shared Library!\n\nfrom datetime import datetime\naDate = datetime.now()\n\n# My custom function:\nymd = (lambda x: \"{y}/{m}/{d}\".format(y=x.year, m=x.month, d=x.day ))\ns = dill.dumps(ymd)\n\n# Store in 'network library'\nclient.set('ymd', s)\nundo = (lambda key: dill.loads(client.get(key)))\n\nundo('ymd')(aDate)\n\n'2022/11/10'\n\n\nThere you go! If you have a spare Rasberry Pi or something then you too can have a small library of custom functions shareable over your home network to use!"
  },
  {
    "objectID": "posts/nbks/2022-09-27-cycle-weapons-best-general.html",
    "href": "posts/nbks/2022-09-27-cycle-weapons-best-general.html",
    "title": "Some Data Exploration of The Cycle - Frontier Weapons",
    "section": "",
    "text": "Recently, I’ve been playing as much The Cycle: Frontier as I can reasonably fit into my days along with getting work and projects done. If you’re not familiar, it’s a First Person Shooter game which focuses around dropping you to a planet in a semi-persistent world with loot and other players. The Station has some Corporations which hand out jobs as a pretty thin attempt to get you down to the planet. While down there, other players - who are not on your team - can decide how they want to deal with you: talk to you, lie to you, kill you, help you. I’ve heard these games be called both Looter Shooters as well as Evac Shooters and I’m admittidly not the biggest fan of these names.\nAs you build reputation with the different Corps on the Station you can unlock the ability to purchase weapons that each specializes in. Some of them are pretty fun and others are kind of terrible. Today we’re going to do part of the process which was inspired by this article by Robert Ritz. In it, he goes over how to setup an automated Data Pipeline using Kaggle and Deepnote together. This part is going to be simply getting the data downloaded, cleaned and some observations about the guns in this game. I still need to do some more investigation about Deepnote - namely the price, utility and such before actually commiting to that part; I should be able to simply cut that part out and do the download/upload to Kaggle from one of my own servers but if it works then I’m going to use it.\n\n\nTo start with, if you’re following along, scraping data from the Cycle’s Wiki page is annoying. There are tables inserted inside the tables which caused quite a problem while trying to simply pull the data from the website. So, if you’re going to us this as the basis for your own tools then beware that you’ll be certain to need to do some custom work. We’ll start with the normal imports for a project like this.\n\nimport pandas as pd             # for the data.\nimport numpy as np              # for a NaN type\nimport matplotlib.pyplot as plt # For plotting, and some customization of plots.\nimport seaborn as sns           # For pretty plots.\n\n# Fix the size of the graphs\nsns.set(rc={\"figure.figsize\":(11, 8)})\n\nThe website we’re going to be using for the data is their official wiki page - which can be found here. We’ll be pulling from the weapons page which luckily contains a table of all the guns without having to join them. Pandas allows you to read html off a website and will attempt to pull any tables it finds on the webpage. Sadly, due to the nested tables and the way the tables are tagged this simply doesn’t work here. But, you can ask pd.read_html() to look for an attribute and then pull the data from the page using that; it will still need to end up as a table though otherwise pandas will reject it. After doing quite a bit of exploration, I found that you can pull the total table with the attribute zebra as that is the only table which uses it.\n\n\nurl = \"https://thecyclefrontier.wiki/wiki/Weapons\"\nsite = pd.read_html(url, attrs={\"class\":\"zebra\"})[0]\n\n\nsite.head()\n\n\n\n\n\n  \n    \n      \n      Image\n      Name\n      Type\n      Ammo\n      Faction\n      Buy Price\n      Sell Value\n      Rarity\n      Weight\n      Crit Multi\n      Damage\n      Pen\n      Mag Size\n      Refire Rate\n      RPM\n      Reload Time\n      Move Speed\n      Proj. Speed\n    \n  \n  \n    \n      0\n      NaN\n      Advocate\n      AR\n      Medium\n      ICA\n      76000  K-Marks\n      22781  K-Marks\n      Epic\n      35.0\n      1.7\n      11.0\n      26.0\n      24.0\n      0.105\n      571.43\n      3.2\n      0.9\n      29000\n    \n    \n      1\n      76000.0\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      22781.0\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      AR-55 Autorifle\n      AR\n      Medium\n      Station\n      1700  K-Marks\n      524  K-Marks\n      Common\n      35.0\n      1.7\n      12.0\n      10.0\n      22.0\n      0.110\n      545.45\n      2.7\n      0.9\n      28000\n    \n    \n      4\n      1700.0\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nYou can see above that this works but the formatting is still a little messed up. Looking at the tables though, this will be an easy fix since rows which we don’t need contain lots NaN. If you’ve never seen this before it just means Not A Number and is a special value used by numpy for this. So, let’s clean the data for use. You can check a Series for these values using .isna() and then we’ll pass the opposite indexes to pull those out:\n\ndata = site[~site.Type.isna()]\ndata.head()\n\n\n\n\n\n  \n    \n      \n      Image\n      Name\n      Type\n      Ammo\n      Faction\n      Buy Price\n      Sell Value\n      Rarity\n      Weight\n      Crit Multi\n      Damage\n      Pen\n      Mag Size\n      Refire Rate\n      RPM\n      Reload Time\n      Move Speed\n      Proj. Speed\n    \n  \n  \n    \n      0\n      NaN\n      Advocate\n      AR\n      Medium\n      ICA\n      76000  K-Marks\n      22781  K-Marks\n      Epic\n      35.0\n      1.7\n      11.0\n      26.0\n      24.0\n      0.105\n      571.430\n      3.20\n      0.9\n      29000\n    \n    \n      3\n      NaN\n      AR-55 Autorifle\n      AR\n      Medium\n      Station\n      1700  K-Marks\n      524  K-Marks\n      Common\n      35.0\n      1.7\n      12.0\n      10.0\n      22.0\n      0.110\n      545.450\n      2.70\n      0.9\n      28000\n    \n    \n      6\n      NaN\n      Asp Flechette Gun\n      SMG\n      Light\n      Osiris\n      54000  K-Marks\n      16131  K-Marks\n      Epic\n      30.0\n      1.5\n      9.0\n      26.0\n      20.0\n      0.095\n      631.580\n      2.50\n      1.0\n      24000\n    \n    \n      9\n      NaN\n      B9 Trenchgun\n      Shotgun\n      Shotgun\n      Station\n      1200  K-Marks\n      371  K-Marks\n      Common\n      25.0\n      1.2\n      10.0\n      10.0\n      5.0\n      0.950\n      63.158\n      2.40\n      1.0\n      26000\n    \n    \n      12\n      NaN\n      Basilisk\n      DMR\n      Heavy\n      Osiris\n      275000  K-Marks\n      82448  K-Marks\n      Exotic\n      50.0\n      1.6\n      34.0\n      28.0\n      8.0\n      0.500\n      120.000\n      3.85\n      0.8\n      45000\n    \n  \n\n\n\n\nLooking much better now. Let’s quickly check the data’s types and make sure they make sense still.\n\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 26 entries, 0 to 75\nData columns (total 18 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Image        0 non-null      float64\n 1   Name         26 non-null     object \n 2   Type         26 non-null     object \n 3   Ammo         26 non-null     object \n 4   Faction      26 non-null     object \n 5   Buy Price    26 non-null     object \n 6   Sell Value   26 non-null     object \n 7   Rarity       26 non-null     object \n 8   Weight       26 non-null     float64\n 9   Crit Multi   26 non-null     float64\n 10  Damage       26 non-null     float64\n 11  Pen          26 non-null     float64\n 12  Mag Size     26 non-null     float64\n 13  Refire Rate  26 non-null     float64\n 14  RPM          26 non-null     float64\n 15  Reload Time  26 non-null     float64\n 16  Move Speed   26 non-null     float64\n 17  Proj. Speed  26 non-null     object \ndtypes: float64(10), object(8)\nmemory usage: 3.9+ KB\n\n\nSomething is wrong with the Proj. Speed at this point since it shouldn’t be an object but instead should be a number. Checking the values we find that there are string values in here.\n\n# There is a hitscan in there; how should we deal with that?\ndata['Proj. Speed'].unique()\n\narray(['29000', '28000', '24000', '26000', '45000', '50000', 'Hitscan',\n       '30000', '60000', '40000', '70000', '4000', '35000', '34000'],\n      dtype=object)\n\n\nThe value of Hitscan is preventing the conversion to numbers. I didn’t realize any of the guns in this game were hitscan at all. Which weapons are these?\n\ndata[ data['Proj. Speed'] == 'Hitscan' ][['Name', 'Proj. Speed']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Proj. Speed\n    \n  \n  \n    \n      21\n      Gorgon\n      Hitscan\n    \n    \n      75\n      Zeus Beam\n      Hitscan\n    \n  \n\n\n\n\nOk, so we’ll need to replace this with something that wont hurt our analysis so we’re also going to change these to np.NaN.\n\n# Fix hitscan info:\nindx = data['Proj. Speed'] == 'Hitscan'\ndata.loc[indx, 'Proj. Speed'] = np.NaN\n\nThere are some other columns - Sell Value, Buy Price as examples - which have come in as object so they’re being treated as strings. We need these to be numbers if we end up using them. And, after fixing the Hitscan problem we’ll need to convert that column to numbers.\n\n\ndata = data.assign(\n    Sell = data['Sell Value'].str.replace(' K-Marks', '').astype('float'),\n    Buy = data['Buy Price'].str.replace(' K-Marks', '').astype('float'),\n    DPS = data['Refire Rate'] * data['Damage'],\n    Faction = data['Faction'].astype('category'),\n    Velocity = data['Proj. Speed'].astype('float')\n)\ndata = data.assign(\n    perWeight = data['Sell'] / data['Weight']\n)\n\n# # This removes the legendary weapons\n# data = data.query('Faction != \"Printing\"')\n\ndata = data.drop(labels = ['Sell Value', 'Buy Price', 'Image', 'Proj. Speed'],axis = 1)\n\nNow we’ve got data to work with!\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Type\n      Ammo\n      Faction\n      Rarity\n      Weight\n      Crit Multi\n      Damage\n      Pen\n      Mag Size\n      Refire Rate\n      RPM\n      Reload Time\n      Move Speed\n      Sell\n      Buy\n      DPS\n      Velocity\n      perWeight\n    \n  \n  \n    \n      0\n      Advocate\n      AR\n      Medium\n      ICA\n      Epic\n      35.0\n      1.7\n      11.0\n      26.0\n      24.0\n      0.105\n      571.430\n      3.20\n      0.9\n      22781.0\n      76000.0\n      1.155\n      29000.0\n      650.885714\n    \n    \n      3\n      AR-55 Autorifle\n      AR\n      Medium\n      Station\n      Common\n      35.0\n      1.7\n      12.0\n      10.0\n      22.0\n      0.110\n      545.450\n      2.70\n      0.9\n      524.0\n      1700.0\n      1.320\n      28000.0\n      14.971429\n    \n    \n      6\n      Asp Flechette Gun\n      SMG\n      Light\n      Osiris\n      Epic\n      30.0\n      1.5\n      9.0\n      26.0\n      20.0\n      0.095\n      631.580\n      2.50\n      1.0\n      16131.0\n      54000.0\n      0.855\n      24000.0\n      537.700000\n    \n    \n      9\n      B9 Trenchgun\n      Shotgun\n      Shotgun\n      Station\n      Common\n      25.0\n      1.2\n      10.0\n      10.0\n      5.0\n      0.950\n      63.158\n      2.40\n      1.0\n      371.0\n      1200.0\n      9.500\n      26000.0\n      14.840000\n    \n    \n      12\n      Basilisk\n      DMR\n      Heavy\n      Osiris\n      Exotic\n      50.0\n      1.6\n      34.0\n      28.0\n      8.0\n      0.500\n      120.000\n      3.85\n      0.8\n      82448.0\n      275000.0\n      17.000\n      45000.0\n      1648.960000\n    \n  \n\n\n\n\n\n\n\nA word here about some extra cleaning which I’ve elected to do. Looking at the data, there are two more problems that should be brought up here. The first is that I’m taking Snipers out of the analysis. The reason for this is that there really are only two of them and everyone understands why they’re as powerful as they are.\n\nsns.scatterplot(x = data.Damage, y = data['Pen'], hue = data.Type)\nplt.title(\"The Reason Snipers Rule\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'The Reason Snipers Rule')\n\n\n\n\n\nThe second adjustment is that I’m pulling the ICA Garuntee out of the analysis because it’s the only one of its kind.\n\ndata.query('Type == \"LMG\"')[['Name', 'Type']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Type\n    \n  \n  \n    \n      27\n      ICA Guarantee\n      LMG\n    \n  \n\n\n\n\nAnd, the same for the Komrad for the same reason.\n\ndata.query('Type == \"Launcher\"')[['Name', 'Type']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Type\n    \n  \n  \n    \n      45\n      KOMRAD\n      Launcher\n    \n  \n\n\n\n\nSo, let’s start with what we all care about the most: Damage.\n\ndata[['Type', 'Damage']].groupby('Type').mean().sort_values('Damage', ascending=False).T\n\n\n\n\n\n  \n    \n      Type\n      DMR\n      Pistol\n      AR\n      SMG\n      Shotgun\n    \n  \n  \n    \n      Damage\n      32.0\n      17.666667\n      11.0\n      9.25\n      9.25\n    \n  \n\n\n\n\nSo, DMR’s do about twice as much damage as the next category down. From this, we’d expect DMRs to be used quite a bit; what weapons are in this category?\n\ndata.query('Type == \"DMR\"')\n\n\n\n\n\n  \n    \n      \n      Name\n      Type\n      Ammo\n      Faction\n      Rarity\n      Weight\n      Crit Multi\n      Damage\n      Pen\n      Mag Size\n      Refire Rate\n      RPM\n      Reload Time\n      Move Speed\n      Sell\n      Buy\n      DPS\n      Velocity\n      perWeight\n    \n  \n  \n    \n      12\n      Basilisk\n      DMR\n      Heavy\n      Osiris\n      Exotic\n      50.0\n      1.6\n      34.0\n      28.0\n      8.0\n      0.5\n      120.0\n      3.85\n      0.8\n      82448.0\n      275000.0\n      17.0\n      45000.0\n      1648.96\n    \n    \n      36\n      KBR Longshot\n      DMR\n      Heavy\n      Korolev\n      Epic\n      50.0\n      1.5\n      35.0\n      26.0\n      12.0\n      0.6\n      100.0\n      3.55\n      0.9\n      29776.0\n      99000.0\n      21.0\n      40000.0\n      595.52\n    \n    \n      51\n      Lacerator\n      DMR\n      Heavy\n      ICA\n      Rare\n      50.0\n      1.5\n      27.0\n      23.0\n      16.0\n      0.4\n      150.0\n      2.55\n      0.9\n      12203.0\n      41000.0\n      10.8\n      35000.0\n      244.06\n    \n  \n\n\n\n\nAll these weapons get used in my experience - and from watching others play the game. Although, the lowest tier gun in here is Rare so that probably helps a lot. Note that the Rarity of a gun informs the Pen for the Gun and therefore adds more damage when being fired. So, the higher the tier of Rarity therefore the more damage the gun can do per hit - and they have higher damage counts as well.\nConsidering this, these guns have a limit placed on their Rate of Fire.\n\ndata[['Type', 'Refire Rate']].groupby('Type').mean().sort_values('Refire Rate', ascending=False).T\n\n\n\n\n\n  \n    \n      Type\n      Shotgun\n      DMR\n      Pistol\n      AR\n      SMG\n    \n  \n  \n    \n      Refire Rate\n      0.77625\n      0.5\n      0.288333\n      0.1475\n      0.08125\n    \n  \n\n\n\n\nThis is a match in the order of the columns - ignoring the Shotgun Category. Shotguns have a low damage (per pellet), and a high re-fire rate. That’s obviously because the damage per pellet masks how lethal Shotguns are: See any Shattergun Montage basically.\nSo, it looks like either this was coincidence or they’re intentionally using this to offset damage.\n\nsns.scatterplot(x = data.Damage, y = data['Refire Rate'], hue = data.Type)\nplt.title(\"Damage With Refire Rate\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Damage With Refire Rate')\n\n\n\n\n\nIf we check the relationship, the Damage also quite high against the constraint on the Refire Rate. These guns appear really strong in comparison to everything else - setting aside Snipers of course. But is it really? Let’s Normalize the Damage column and see if it really is that far out. If you’re not familiar with Normalization then this is a common process in Machine Learning where the values of a column are scaled based on the minimum and maximum values.\n\ndef normalize(column): return ( column - column.min()) / (column.max() - column.min())\nround(normalize(data['Refire Rate']).mean(), 2)\n\n# Lets save this and re-plot:\ndata['Norm Refire'] = round(normalize(data['Refire Rate']), 2)\ndata['Norm Damage'] = round(normalize(data['Damage']), 2)\n\n\n# Re-plot:\nsns.scatterplot(x = data['Norm Damage'], y = data['Norm Refire'], hue = data.Type)\nplt.title(\"Normalized Damage With Normalized Refire Rate\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Normalized Damage With Normalized Refire Rate')\n\n\n\n\n\nNormalizing doesn’t look to have changed anyting aside from the scale so that still looks really solid. And, If we check the Critical Multiplier - which is a stand in for HeadShots since I believe the Groin also counts - the reward is about average for accuracy.\n\nsns.scatterplot(x = data.Damage, y = data['Crit Multi'], hue = data.Type)\nplt.title(\"Damage With Critical Multiplier\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Damage With Critical Multiplier')"
  },
  {
    "objectID": "posts/nbks/2022-09-30-testing-if-interact-works-online.html",
    "href": "posts/nbks/2022-09-30-testing-if-interact-works-online.html",
    "title": "Quick Test of Ipywidget Interact Function",
    "section": "",
    "text": "While watching the Lectures from Fastai this time around, Jeremy used a Python Decorator called @interact which creates a function with interactable variables. This is really useful feature when you want to experiment with specific values - like Jeremy did in the lecture. However, there is a warning in the Lecture notes: > Reminder: If the sliders above aren’t working for you, that’s because the interactive features of this notebook don’t work in Kaggle’s Reader mode. They only work in Edit mode. Please click “Copy & Edit” in the top right of this window, then in the menu click Run and then Run all. Then you’ll be able to use all the interactive sliders in this notebook.\nLet’s step that back a little bit for an explanation since some of this might not be familiar. Python Decorators are function annotations which modify the behavior of their function. There are excellent articles and descriptions about them but we’ll use this one from the Python Docs as an example: @cache from the functools library.\n\nfrom functools import cache\n@cache\ndef factorial(n):\n    return n * factorial(n-1) if n else 1\n\n\nfactorial(10)      # no previously cached result, makes 11 recursive calls\n\n3628800\n\n\n\nfactorial(5)       # just looks up cached value result\n\n120\n\n\n\nfactorial(12)      # makes two new recursive calls, the other 10 are cached\n\n479001600\n\n\nThe Decorator @cache will modify the behavior of the function to record previous computations. In this way, you can save time using the modified function we’ve written. In the instance with Jeremy, the @interact call modifies the function written to allow us to modify the values in real time without ending execution nor updating the values in the code block. What we’re really after here though is Does this work with Fastpages?\nConsidering the warning given by Jeremy, I wouldn’t expect this to work but I’ve gotten R working in these Jupyter Notebooks and uploaded them so let’s try it. First, we’ll need the imports to do this and then we’ll simply use the example from the Fastai Lecture Notebook and then push it to the site.\n\nfrom ipywidgets import interact\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom functools import partial\n\nplt.rc('figure', dpi=90) # This modifies the size of the graphs\n\n\n# This is just from the notebook\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\n# Define a quadratic function:\ndef f(x): return 3*x**2 + 2*x + 1\n\n# define generic quadratic\ndef quad(a, b, c, x): return a*x**2 + b*x + c\ndef mk_quad(a,b,c): return partial(quad, a,b,c)\n\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\n\nplot_function(f, \"$3x^2 + 2x + 1$\")\n\n\n\n\n\nf2 = mk_quad(3,2,1)\nplot_function(f2)\n\n\n\n\n\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.15, 1.5)\n\n\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    plt.scatter(x,y)\n    plot_function(mk_quad(a,b,c), ylim=(-3,13))\n\n\n\n\n\nConclusion\nAfter pushing the page, it doesn’t work. I assumed as much but was hopeful it would work."
  },
  {
    "objectID": "posts/nbks/2022-12-24-Exploring-Tv-Data-With-Spark-Part-1.html",
    "href": "posts/nbks/2022-12-24-Exploring-Tv-Data-With-Spark-Part-1.html",
    "title": "Exploring Television Data With Spark - Part 1",
    "section": "",
    "text": "I found some data online which is of some interest about movies and TV shows. And, no, this is not he IMBD data set which floats around for people’s use but is instead from someone on Kaggle which scraped lots of different websites for Thriller, Crime and Action shows. The dataset is small enough to be used with Pandas but large enough to be used to practice some Apache Spark! So, let’s get to it."
  },
  {
    "objectID": "posts/nbks/2022-12-24-Exploring-Tv-Data-With-Spark-Part-1.html#explode-a-column",
    "href": "posts/nbks/2022-12-24-Exploring-Tv-Data-With-Spark-Part-1.html#explode-a-column",
    "title": "Exploring Television Data With Spark - Part 1",
    "section": "Explode a Column!",
    "text": "Explode a Column!\nThis is surprisignly common in R but not very much in Python for some reason. When I was initially trying to work on these sorts of problems, I found that explode() was not something which was supported with Python. It did exists in Apache Spark - which is where I found the actual name for this action - so I had to work on writing my own. Thankfully, I don’t have to keep track of my worse version and use the official call now.\nTo explode a column means to duplicate rows by splitting up the values in a column. If we look at the documentation for this:\n\nimport numpy as np\ndf = pd.DataFrame({'A': [[0, 1, 2], 'foo', [], [3, 4]],\n                   'B': 1,\n                   'C': [['a', 'b', 'c'], np.nan, [], ['d', 'e']]})\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n    \n  \n  \n    \n      0\n      [0, 1, 2]\n      1\n      [a, b, c]\n    \n    \n      1\n      foo\n      1\n      NaN\n    \n    \n      2\n      []\n      1\n      []\n    \n    \n      3\n      [3, 4]\n      1\n      [d, e]\n    \n  \n\n\n\n\n\ndf.explode('A')\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n    \n  \n  \n    \n      0\n      0\n      1\n      [a, b, c]\n    \n    \n      0\n      1\n      1\n      [a, b, c]\n    \n    \n      0\n      2\n      1\n      [a, b, c]\n    \n    \n      1\n      foo\n      1\n      NaN\n    \n    \n      2\n      NaN\n      1\n      []\n    \n    \n      3\n      3\n      1\n      [d, e]\n    \n    \n      3\n      4\n      1\n      [d, e]\n    \n  \n\n\n\n\nFor our data, we’ll want to do this for the Genre and the Actor columns. You will notice that to do this we’ll need the column to contain a list instead of the comma separated values we have now. A quick assign and apply will fix this for us.\n\n# TIME TO EXPLODE\n# I am so happy this is a thing.\ndata = data.assign(\n    Genre = data.Genres.apply(lambda x: x.split(',')),\n    Actor = data.Actors.apply(lambda x: x.split(','))\n)\ndata = data.explode('Genre')\ndata = data.explode('Actor')\n\ndata = data.assign(\n    Genre = data.Genre.apply(lambda x: x.strip()),\n    Actor = data.Actor.apply(lambda x: x.strip())\n)\n\n\ndata.head(15)[['Name', 'Year', 'Genre', 'Actor']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Year\n      Genre\n      Actor\n    \n  \n  \n    \n      0\n      Andor\n      2022–\n      Action\n      Diego Luna\n    \n    \n      0\n      Andor\n      2022–\n      Action\n      Kyle Soller\n    \n    \n      0\n      Andor\n      2022–\n      Action\n      Stellan Skarsgård\n    \n    \n      0\n      Andor\n      2022–\n      Action\n      Genevieve O'Reilly\n    \n    \n      0\n      Andor\n      2022–\n      Adventure\n      Diego Luna\n    \n    \n      0\n      Andor\n      2022–\n      Adventure\n      Kyle Soller\n    \n    \n      0\n      Andor\n      2022–\n      Adventure\n      Stellan Skarsgård\n    \n    \n      0\n      Andor\n      2022–\n      Adventure\n      Genevieve O'Reilly\n    \n    \n      0\n      Andor\n      2022–\n      Drama\n      Diego Luna\n    \n    \n      0\n      Andor\n      2022–\n      Drama\n      Kyle Soller\n    \n    \n      0\n      Andor\n      2022–\n      Drama\n      Stellan Skarsgård\n    \n    \n      0\n      Andor\n      2022–\n      Drama\n      Genevieve O'Reilly\n    \n    \n      1\n      The Peripheral\n      2022–\n      Drama\n      Chloë Grace Moretz\n    \n    \n      1\n      The Peripheral\n      2022–\n      Drama\n      Gary Carr\n    \n    \n      1\n      The Peripheral\n      2022–\n      Drama\n      Jack Reynor\n    \n  \n\n\n\n\nNice! Now we can compare actors and genres!"
  },
  {
    "objectID": "posts/nbks/2022-12-24-Exploring-Tv-Data-With-Spark-Part-1.html#data-cleaning",
    "href": "posts/nbks/2022-12-24-Exploring-Tv-Data-With-Spark-Part-1.html#data-cleaning",
    "title": "Exploring Television Data With Spark - Part 1",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nIt is here we’ll need to step aside since there were problems which this data. I believe I found this while exploring some basic questions but in effect, there are parenthesis included in the Year column:\n\ndata.Year.loc[data.Year.str.contains('\\(')]\n\n45     II) (2020–2023\n45     II) (2020–2023\n45     II) (2020–2023\n45     II) (2020–2023\n45     II) (2020–2023\n            ...      \n474     I) (2013–2016\n474     I) (2013–2016\n474     I) (2013–2016\n474     I) (2013–2016\n474     I) (2013–2016\nName: Year, Length: 192, dtype: object\n\n\nNow, we could read each column and replace all these values but I think the better idea here will be to simply correct the source data. Keep in mind that if this is a dataset which pandas struggles with due to it’s size then going with the replace is going to be the better option but I corrected the Source file in this instance. To fix this, I ran these two lines:\nsed 's|II) (||'  data/AllThrillerSeriesList.csv > data/AllThrillerSeriesListCleaned.csv\nsed 's|I) (||'  data/AllThrillerSeriesListCleaned.csv > data/AllThrillerSeriesListClean.csv\nYou should always keep the original copy around in case you need to undo damage that you did in your analysis on accident. Regardless of our best intentions, that will sometimes happen and you’ll want to be able to start again.\nSadly, this still was not enough as there were more problems to fix. When I added more rows, I found that there was extra information in some of the Ratings columns - namely there was a attached | Gross which showed up:\n[data]$ grep -i gross AllThrillerSeriesList.csv \n85,Mindhunter,2017–2019,TV-MA , 60 min ,\" Crime, Drama, Mystery\",8.6,\" Jonathan Groff, Holt McCallany, Anna Torv, Hannah Gross\",\" 288,604\"\n669,Wallander,2005–2013,TV-14 , 90 min ,\" Crime, Drama, Mystery\",7.6,\" Krister Henriksson, Fredrik Gunnarsson, Mats Bergman, Douglas Johansson\",\" 5,844 | Gross\"\n736,The Brave,2017–2018,TV-14 , 43 min ,\" Action, Drama, Thriller\",7.5,\" Anne Heche, Mike Vogel, Tate Ellington, Demetrius Grosse\",\" 9,208\"\n931,Gongjo,2017– ,Not Rated , 124 min ,\" Action, Comedy, Drama\",6.6,\" Hyun Bin, Hae-Jin Yoo, Ju-hyuk Kim, Lee Hae-Young\",\" 3,637 | Gross\"\n1115,Crisis,2014,,60 min ,\" Action, Drama, Thriller\",7.0,\" Dermot Mulroney, Rachael Taylor, Lance Gross, James Lafferty\",\" 8,498\"\n1181,Hello Monster,2015,,,\"Action, Crime, Mystery\",7.7,\" Seo In-Guk, Jang Na-ra, Park Bo-gum, Choi Wonyoung\",\" 1,542 | Gross\"\n1978,Deadwax,2018– ,,,\"Horror, Music, Mystery\",6.7,\" Hannah Gross, Evan Gamble, Tracy Perez, Dohn Norwood\", 397\n[data]$\nI admit that I simply went into the dataset and deleted them the first time around. But, that was because I already had the file open after finding out that the Names field was also not clean. Meaning that some of the names were not quoted like they should of been:\n2220,Keple,s) (2018–2019,,,\"Crime, Drama, Mystery\",6.1,\" Marc Lavoine, Sofia Essaïdi, Isabelle Renauld, Serge Riaboukine\", 97\nIf you look that TV Show up you’ll find that it’s name is Kepler(s) which is going to be a problem.\n\nIf We Had a Data Engineer…\nSo, about now if we had a Data Engineer then we should reach out about how this all is being processed and cleaned up. These kinds of problems are going to make the analysis hard until we get these corrected upstream. In my case, since I have no say in the original data, we’re going to have to step back and investigate our replaces once again to make sure we’re not malforming the names when we substitute.\nReseting the data and checking through the data for the ) ( to look for fields which need to be correct, I found that there was another tier up of these weird patterns: III) (. So, these are our replaces with sed:\nsed 's|III) (||'  data/AllThrillerSeriesList.csv > data/AllThrillerSeriesListCleaned.csv\nsed -i 's|II) (||'  data/AllThrillerSeriesListCleaned.csv\nsed -i 's|I) (||'  data/AllThrillerSeriesListCleaned.csv\nsed 's|) (||'  data/AllThrillerSeriesListCleaned.csv > data/AllThrillerSeriesListClean.csv\nI also found at least four rows where we’ll need to add proper quotations marks around the values:\n369,The Guardians of Justice,Will Save You!) (2022– ,TV-MA , 30 min ,\" Animation, Action, Adventure\",5.0,\" Dallas Page, Sharni Vinson, Denise Richards, Zachery Ty Bryan\",\" 3,279\"\n\n2258,Doute Raisonnable,Reasonable Doubt) (2021– ,,60 min , Thriller,6.9,\" Marc-André Grondin, Julie Perreault, Kathleen Fortin, Dominik Dagenais\", 39\n\n2550,The Hunt for Salamander,Otdel Izdirvane) (2021– ,,,Thriller,9.1,\" Martin Radoev, Iva Mihalic, Hristo Petkov, Julian Vergov\", 67\n\n2760,Loabat Al Moot,Game of Death) (2013–2014,,57 min ,\" Drama, Family, Romance\",6.2,\" Cyrine AbdelNour, Nicolas Mouawad, Nada Abou Farhat, Abed Fahd\", 55\nJust to show how this was updated:\n[Nbks]$ grep -E \"349,|2258,|2550,|2760,\" data/AllThrillerSeriesListClean.csv \n349,La Reina del Sur,2011– ,TV-MA , 378 min ,\" Action, Crime, Drama\",7.9,\" Kate del Castillo, Humberto Zurita, Alejandro Calva, Isabella Sierra\",\" 2,728\"\n1349,Matrix,1993,,60 min ,\" Action, Drama, Fantasy\",7.8,\" Nick Mancuso, Phillip Jarrett, Carrie-Anne Moss, John Vernon\", 199\n2258,\"Doute Raisonnable,Reasonable Doubt\", 2021– ,,60 min , Thriller,6.9,\" Marc-André Grondin, Julie Perreault, Kathleen Fortin, Dominik Dagenais\", 39\n2349,Swift Justice,1996,,,\"Action, Crime, Thriller\",7.0,\" James McCaffrey, Emerson St. John, Tony Darrow, Alex Kilgore\", 61\n2550,\"The Hunt for Salamander,Otdel Izdirvane\", 2021– ,,,Thriller,9.1,\" Martin Radoev, Iva Mihalic, Hristo Petkov, Julian Vergov\", 67\n2760,\"Loabat Al Moot,Game of Death\", 2013–2014,,57 min ,\" Drama, Family, Romance\",6.2,\" Cyrine AbdelNour, Nicolas Mouawad, Nada Abou Farhat, Abed Fahd\", 55\n[Nbks]$\nNow we should have most of the issues ironed out to start trying to ask some fun questions."
  },
  {
    "objectID": "posts/nbks/2022-12-24-Exploring-Tv-Data-With-Spark-Part-1.html#what-kinds-of-movies",
    "href": "posts/nbks/2022-12-24-Exploring-Tv-Data-With-Spark-Part-1.html#what-kinds-of-movies",
    "title": "Exploring Television Data With Spark - Part 1",
    "section": "What kinds of Movies…?",
    "text": "What kinds of Movies…?\nNow we an start asking some questions about the data before we move to Spark. Looking at the data here are some fun questions: 1. Given a Genre, what is the average rating and the average number of votes? 2. Given an actor, what is the averagae rating and the average number of votes? 3. Given the Guide, what is the average rating and the average number of votes? 4. Given the Starting Year, what is the average rating and the average number of votes?\n\n# Given a Genre, what is the average rating and the average number of votes?\ndata.groupby('Genre')[['Ratings', 'Votes']].mean().sort_values('Ratings', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      Ratings\n      Votes\n    \n    \n      Genre\n      \n      \n    \n  \n  \n    \n      Biography\n      8.300000\n      109127.333333\n    \n    \n      Animation\n      8.221622\n      46288.783784\n    \n    \n      Crime\n      7.627799\n      69710.181467\n    \n    \n      Mystery\n      7.568966\n      63459.632184\n    \n    \n      Romance\n      7.545455\n      49638.090909\n    \n    \n      Comedy\n      7.545000\n      24758.975000\n    \n    \n      Drama\n      7.537500\n      72898.041667\n    \n    \n      Action\n      7.526994\n      51595.000000\n    \n    \n      Thriller\n      7.511034\n      56173.862069\n    \n    \n      Short\n      7.500000\n      90451.500000\n    \n  \n\n\n\n\nWow, I did not expect Biography to be the most well rated category in the data. I like that though and wonder what kind of movies and shows are in that list; A question for another time.\n\n# Given an actor, what is the averagae rating and the average number of votes?; Best\ndata.groupby('Actor')[['Ratings', 'Votes']].mean().sort_values('Ratings', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      Ratings\n      Votes\n    \n    \n      Actor\n      \n      \n    \n  \n  \n    \n      Aaron Paul\n      9.5\n      1872005.0\n    \n    \n      Anna Gunn\n      9.5\n      1872005.0\n    \n    \n      Betsy Brandt\n      9.5\n      1872005.0\n    \n    \n      Lance Reddick\n      9.3\n      339583.0\n    \n    \n      Dominic West\n      9.3\n      339583.0\n    \n    \n      Pratik Gandhi\n      9.3\n      144344.0\n    \n    \n      Anjali Barot\n      9.3\n      144344.0\n    \n    \n      Shreya Dhanwanthary\n      9.3\n      144344.0\n    \n    \n      Sonja Sohn\n      9.3\n      339583.0\n    \n    \n      Hemant Kher\n      9.3\n      144344.0\n    \n  \n\n\n\n\n\ndata.groupby('Actor')[['Ratings', 'Votes']].mean().sort_values('Ratings', ascending=False).tail(10)\n\n\n\n\n\n  \n    \n      \n      Ratings\n      Votes\n    \n    \n      Actor\n      \n      \n    \n  \n  \n    \n      Zachery Ty Bryan\n      5.0\n      3279.0\n    \n    \n      Denise Richards\n      5.0\n      3279.0\n    \n    \n      Ella Balinska\n      4.1\n      40569.0\n    \n    \n      Tamara Smart\n      4.1\n      40569.0\n    \n    \n      Adeline Rudolph\n      4.1\n      40569.0\n    \n    \n      Siena Agudong\n      4.1\n      40569.0\n    \n    \n      Sandra Reyes\n      2.4\n      3510.0\n    \n    \n      Constanza Camelo\n      2.4\n      3510.0\n    \n    \n      Diego Trujillo\n      2.4\n      3510.0\n    \n    \n      Roberto Urbina\n      2.4\n      3510.0\n    \n  \n\n\n\n\nSome of these I recognize and some I don’t. Nothing too interesting I think here.\n\n# Given the Guide, what is the average rating and the average number of votes?\ndata.groupby('Guide')[['Ratings', 'Votes']].mean().sort_values('Ratings', ascending=False)\n\n\n\n\n\n  \n    \n      \n      Ratings\n      Votes\n    \n    \n      Guide\n      \n      \n    \n  \n  \n    \n      TV-Y7-FV\n      7.900000\n      11485.000000\n    \n    \n      TV-PG\n      7.737895\n      31579.273684\n    \n    \n      TV-14\n      7.622013\n      72887.157233\n    \n    \n      TV-Y7\n      7.562500\n      15830.000000\n    \n    \n      TV-MA\n      7.539541\n      77243.647773\n    \n    \n      Not Rated\n      7.392857\n      11184.071429\n    \n    \n      PG-13\n      6.700000\n      3289.000000\n    \n    \n      TV-13\n      6.600000\n      1404.000000\n    \n  \n\n\n\n\nNow this is somewhat interesting. The FV means there is Fantasy Violence; the PG means Parental Guidance. The top ratings are more mature shows with an emphasis on violence. Considering the data we have - and who is certainly voting - this means audiences generally like violence more than they do not.\n\ndata.groupby('startYear')[['Ratings', 'Votes']].mean().sort_values('Ratings', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      Ratings\n      Votes\n    \n    \n      startYear\n      \n      \n    \n  \n  \n    \n      1959\n      9.100000\n      83920.000000\n    \n    \n      1955\n      8.500000\n      17597.000000\n    \n    \n      1989\n      8.466667\n      67447.000000\n    \n    \n      1974\n      8.350000\n      7413.500000\n    \n    \n      1993\n      8.333333\n      84556.000000\n    \n    \n      1961\n      8.300000\n      8012.000000\n    \n    \n      1971\n      8.300000\n      35793.000000\n    \n    \n      1990\n      8.266667\n      84247.666667\n    \n    \n      2006\n      8.214286\n      205522.428571\n    \n    \n      1963\n      8.150000\n      6364.500000\n    \n  \n\n\n\n\n\ndata.groupby('startYear')[['Ratings', 'Votes']].mean().sort_values('Ratings', ascending=False).tail(10)\n\n\n\n\n\n  \n    \n      \n      Ratings\n      Votes\n    \n    \n      startYear\n      \n      \n    \n  \n  \n    \n      2018\n      7.360870\n      40269.934783\n    \n    \n      2019\n      7.347826\n      23647.582609\n    \n    \n      1997\n      7.325000\n      29279.000000\n    \n    \n      2020\n      7.301563\n      30649.367188\n    \n    \n      2021\n      7.247200\n      29343.368000\n    \n    \n      2022\n      7.206550\n      16209.716157\n    \n    \n      1976\n      7.200000\n      6567.000000\n    \n    \n      1984\n      7.200000\n      14867.333333\n    \n    \n      1973\n      7.000000\n      5353.500000\n    \n    \n      1970\n      6.950000\n      3397.000000\n    \n  \n\n\n\n\nThe older shows are doing much better than the newer shows. There are some older years which do not do well; what happened in the 70s?"
  },
  {
    "objectID": "posts/nbks/2022-12-24-Exploring-Tv-Data-With-Spark-Part-1.html#lets-add-apache-spark",
    "href": "posts/nbks/2022-12-24-Exploring-Tv-Data-With-Spark-Part-1.html#lets-add-apache-spark",
    "title": "Exploring Television Data With Spark - Part 1",
    "section": "Let’s Add Apache Spark",
    "text": "Let’s Add Apache Spark\nIf you’re not aware then Apache Spark is a tool for large scale data analytics. I’m sure it is used for Data Engineering as well since it can clean data at a massive scale - but we’re not going to use it for that today.\nYou can use Spark from python quite easily and without needing a cluster. Just python3 -m pip install pyspark and you’re off. I would definitely recommend that you modify the logging level which gets pushed into the terminal since this makes working and reading much easier. Also, if you’re following along and you want to restart spark then you’ll need to run sc.stop() or what you named your spark context.\n\n# Conf for config; context spark \"cluster\"\nfrom pyspark import SparkConf, SparkContext\nconf = SparkConf().setMaster(\"local[*]\").setAppName(\"play\")\nsc = SparkContext(conf = conf)\nsc.setLogLevel(\"ERROR\")\n\n# session to talk to the \"cluster\"\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"play\").getOrCreate()\n\nWe’ve already identified the data types we want so we’ll tag them on import. To do this, we’ll need types since this is not brought in by default.\n\n\nfrom pyspark.sql.types import *\n\n# Define the schema:\nshows = StructType([\n    StructField('Ignore', IntegerType(), True),\n    StructField('Name', StringType(), True),\n    StructField('Year', StringType(), True),\n    StructField('Guide', StringType(), True),\n    StructField('Duration', StringType(), True),\n    StructField('Genres', StringType(), True),\n    StructField('Ratings', FloatType(), True),\n    StructField('Actors', StringType(), True),\n    StructField('Votes', StringType(), True),\n])\n\nAnd, now we bring in the data.\n\ndata = spark.read.csv(str(Path('data', 'AllThrillerSeriesListClean.csv')), header=True, schema=shows)\ndata.show(10)\n\n+------+--------------------+---------+------+--------+--------------------+-------+--------------------+----------+\n|Ignore|                Name|     Year| Guide|Duration|              Genres|Ratings|              Actors|     Votes|\n+------+--------------------+---------+------+--------+--------------------+-------+--------------------+----------+\n|     0|               Andor|   2022– |TV-14 | 40 min | Action, Adventur...|    8.4| Diego Luna, Kyle...|    82,474|\n|     1|      The Peripheral|   2022– |TV-MA |    null| Drama, Mystery, ...|    8.0| Chloë Grace More...|    34,768|\n|     2|    The Walking Dead|2010–2022|TV-MA | 44 min | Drama, Horror, T...|    8.1| Andrew Lincoln, ...|   988,666|\n|     3|      Criminal Minds|   2005– |TV-14 | 42 min | Crime, Drama, My...|    8.1| Kirsten Vangsnes...|   198,262|\n|     4|        Breaking Bad|2008–2013|TV-MA | 49 min | Crime, Drama, Th...|    9.5| Bryan Cranston, ...| 1,872,005|\n|     5|                Dark|2017–2020|TV-MA | 60 min | Crime, Drama, My...|    8.7| Louis Hofmann, K...|   384,702|\n|     6|            Manifest|   2018– |TV-14 | 43 min | Drama, Mystery, ...|    7.1| Melissa Roxburgh...|    66,158|\n|     7|     Stranger Things|   2016– |TV-14 | 51 min | Drama, Fantasy, ...|    8.7| Millie Bobby Bro...| 1,179,168|\n|     8|Guillermo del Tor...|   2022– |TV-MA | 60 min | Drama, Horror, M...|    7.1| Lize Johnston, K...|    33,660|\n|     9|              Echo 3|   2022– |TV-MA | 49 min | Action, Drama, T...|    6.6| Michiel Huisman,...|     1,333|\n+------+--------------------+---------+------+--------+--------------------+-------+--------------------+----------+\nonly showing top 10 rows\n\n\n\nAnd, now we need to repeat what we prototyped in the pandas starting with dropping the column.\n\ndata = data.drop('Ignore')\ndata.show(10)\n\n+--------------------+---------+------+--------+--------------------+-------+--------------------+----------+\n|                Name|     Year| Guide|Duration|              Genres|Ratings|              Actors|     Votes|\n+--------------------+---------+------+--------+--------------------+-------+--------------------+----------+\n|               Andor|   2022– |TV-14 | 40 min | Action, Adventur...|    8.4| Diego Luna, Kyle...|    82,474|\n|      The Peripheral|   2022– |TV-MA |    null| Drama, Mystery, ...|    8.0| Chloë Grace More...|    34,768|\n|    The Walking Dead|2010–2022|TV-MA | 44 min | Drama, Horror, T...|    8.1| Andrew Lincoln, ...|   988,666|\n|      Criminal Minds|   2005– |TV-14 | 42 min | Crime, Drama, My...|    8.1| Kirsten Vangsnes...|   198,262|\n|        Breaking Bad|2008–2013|TV-MA | 49 min | Crime, Drama, Th...|    9.5| Bryan Cranston, ...| 1,872,005|\n|                Dark|2017–2020|TV-MA | 60 min | Crime, Drama, My...|    8.7| Louis Hofmann, K...|   384,702|\n|            Manifest|   2018– |TV-14 | 43 min | Drama, Mystery, ...|    7.1| Melissa Roxburgh...|    66,158|\n|     Stranger Things|   2016– |TV-14 | 51 min | Drama, Fantasy, ...|    8.7| Millie Bobby Bro...| 1,179,168|\n|Guillermo del Tor...|   2022– |TV-MA | 60 min | Drama, Horror, M...|    7.1| Lize Johnston, K...|    33,660|\n|              Echo 3|   2022– |TV-MA | 49 min | Action, Drama, T...|    6.6| Michiel Huisman,...|     1,333|\n+--------------------+---------+------+--------+--------------------+-------+--------------------+----------+\nonly showing top 10 rows\n\n\n\nWe need to replicate the custom functions like we did with the isOngoing column. To do this, we’ll need to define our own functions in Spark and that requires User Defined Functions. There are different ways to do this but I think my favorite so far - which is the closest to a python lambda function - is defining it and using it inline:\n\n# collect the ability to define udfs\nimport pyspark.sql.functions as F\n\ndata = data.withColumn('isOngoing',     # Target this column\n    F.udf(                              # define a udf\n        lambda x: x.strip()[-1] == '–') # use the logic we already wrote.\n    ('Year')                            # pass the column we're applying this to.\n)\ndata.select('isOngoing').show(10)\n\n+---------+\n|isOngoing|\n+---------+\n|     true|\n|     true|\n|    false|\n|     true|\n|    false|\n|    false|\n|     true|\n|     true|\n|     true|\n|     true|\n+---------+\nonly showing top 10 rows\n\n\n\nWith that, we can easily apply what we wrote before:\n\n\ndata = data.withColumn('startYear', F.udf( lambda x: x.split('–')[0])('Year'))\ndata = data.withColumn('Votes', F.udf( lambda x: int(x.replace(',', '')))('Votes'))\ndata = data.withColumn('Votes', F.cast(IntegerType(), data['Votes']))\ndata.show(10)\n\n+--------------------+---------+------+--------+--------------------+-------+--------------------+-------+---------+---------+\n|                Name|     Year| Guide|Duration|              Genres|Ratings|              Actors|  Votes|isOngoing|startYear|\n+--------------------+---------+------+--------+--------------------+-------+--------------------+-------+---------+---------+\n|               Andor|   2022– |TV-14 | 40 min | Action, Adventur...|    8.4| Diego Luna, Kyle...|  82474|     true|     2022|\n|      The Peripheral|   2022– |TV-MA |    null| Drama, Mystery, ...|    8.0| Chloë Grace More...|  34768|     true|     2022|\n|    The Walking Dead|2010–2022|TV-MA | 44 min | Drama, Horror, T...|    8.1| Andrew Lincoln, ...| 988666|    false|     2010|\n|      Criminal Minds|   2005– |TV-14 | 42 min | Crime, Drama, My...|    8.1| Kirsten Vangsnes...| 198262|     true|     2005|\n|        Breaking Bad|2008–2013|TV-MA | 49 min | Crime, Drama, Th...|    9.5| Bryan Cranston, ...|1872005|    false|     2008|\n|                Dark|2017–2020|TV-MA | 60 min | Crime, Drama, My...|    8.7| Louis Hofmann, K...| 384702|    false|     2017|\n|            Manifest|   2018– |TV-14 | 43 min | Drama, Mystery, ...|    7.1| Melissa Roxburgh...|  66158|     true|     2018|\n|     Stranger Things|   2016– |TV-14 | 51 min | Drama, Fantasy, ...|    8.7| Millie Bobby Bro...|1179168|     true|     2016|\n|Guillermo del Tor...|   2022– |TV-MA | 60 min | Drama, Horror, M...|    7.1| Lize Johnston, K...|  33660|     true|     2022|\n|              Echo 3|   2022– |TV-MA | 49 min | Action, Drama, T...|    6.6| Michiel Huisman,...|   1333|     true|     2022|\n+--------------------+---------+------+--------+--------------------+-------+--------------------+-------+---------+---------+\nonly showing top 10 rows\n\n\n\nThe Actors and Genre work is easy in spark since it’s inbuilt. We’ll just split and explode like we did in pandas - but in spark:\n\ndata = data.withColumn('Genre', F.split(data['Genres'], ','))\ndata = data.withColumn('Genre', F.explode(data['Genre']))\n\ndata = data.withColumn('Actor', F.split(data['Actors'], ','))\ndata = data.withColumn('Actor', F.explode(data['Actor']))\n\n# Gotta clean up the inputs:\ndata = data.withColumn('Genre', F.udf(lambda x: x.strip())('Genre'))\ndata = data.withColumn('Actor', F.udf(lambda x: x.strip())('Actor'))\ndata = data.withColumn('Votes', F.udf( lambda x: int(x) )('Votes'))\n\ndata.select(\n    ['Name', 'Year', 'Guide', \n    'Duration', 'Ratings', 'Votes', \n    'isOngoing', 'startYear', 'Genre', 'Actor']\n).show(10)\n\n+-----+------+------+--------+-------+-----+---------+---------+---------+------------------+\n| Name|  Year| Guide|Duration|Ratings|Votes|isOngoing|startYear|    Genre|             Actor|\n+-----+------+------+--------+-------+-----+---------+---------+---------+------------------+\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|   Action|        Diego Luna|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|   Action|       Kyle Soller|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|   Action| Stellan Skarsgård|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|   Action|Genevieve O'Reilly|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|Adventure|        Diego Luna|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|Adventure|       Kyle Soller|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|Adventure| Stellan Skarsgård|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|Adventure|Genevieve O'Reilly|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|    Drama|        Diego Luna|\n|Andor|2022– |TV-14 | 40 min |    8.4|82474|     true|     2022|    Drama|       Kyle Soller|\n+-----+------+------+--------+-------+-----+---------+---------+---------+------------------+\nonly showing top 10 rows\n\n\n\n\n# DELETE\ndata.printSchema()\n\nroot\n |-- Name: string (nullable = true)\n |-- Year: string (nullable = true)\n |-- Guide: string (nullable = true)\n |-- Duration: string (nullable = true)\n |-- Genres: string (nullable = true)\n |-- Ratings: float (nullable = true)\n |-- Actors: string (nullable = true)\n |-- Votes: string (nullable = true)\n |-- isOngoing: string (nullable = true)\n |-- startYear: string (nullable = true)\n |-- Genre: string (nullable = true)\n |-- Actor: string (nullable = true)\n\n\n\nOne mistake that will be eventually made is using other aggregation functions like pandas or the builtin to try to do aggregations. But, Spark has its own versions of these. You can find them under pyspark.sql.functions and import them as you like:\n\nfrom pyspark.sql.functions import mean as sMean, col\ntry:\n    data.select(['Genre', 'Ratings', 'Votes']).groupBy('Genre').agg(\n        sMean('Ratings').alias(\"Mean\"),\n        sMean('Votes').alias('Votes')\n        ).orderBy(F.desc(\"Mean\")).show()\nexcept Exception as e:\n    print( e )\n\n+---------+------------------+------------------+\n|    Genre|              Mean|             Votes|\n+---------+------------------+------------------+\n|Biography| 7.795238108862014| 33137.95238095238|\n|    Music| 7.474999845027924|           413.625|\n|  History| 7.416666656732559|3330.5416666666665|\n|Animation| 7.270712390319233| 8447.388742304309|\n|  Fantasy| 7.231312224012695|18573.573755656107|\n|Adventure| 7.228473101449998| 8999.588235294117|\n|    Crime| 7.225606645016681|15978.954034326298|\n|   Comedy| 7.223730058905348| 4206.774310595065|\n|   Family| 7.207407372969168| 635.7592592592592|\n|    Drama|7.2038561880837015| 17351.69541427827|\n|     News| 7.199999809265137|              28.0|\n|  Mystery| 7.192586999365133|15005.853252647505|\n|   Action|7.1531927343807755| 11602.80236925744|\n|  Romance| 7.139181296727811|3810.4619883040937|\n| Thriller| 7.113366268083392| 6469.523950617284|\n|   Horror| 7.089073947713345|15463.994670219854|\n|   Sci-Fi| 7.085540543375789|21185.131081081083|\n|    Sport| 7.033333460489909| 281.3333333333333|\n|      War| 6.940000057220459|             756.2|\n|    Short| 6.932031240314245|      6693.5390625|\n+---------+------------------+------------------+\nonly showing top 20 rows\n\n\n\nThere are some which are definitely outliers - like Music, News and War. Biography is still a hard winner though.\nThere is no plotting functions inside of Spark so you will need to pull the data for visualizations. Spark has a handy function .toPandas() which will pull the data out and convert it into a pandas data frame for usage. Please make sure when you do this to only pull the results and not the totality of the data.\n\nresults = data.select(['Genre', 'Ratings', 'Votes']).groupBy('Genre').agg(\n    sMean('Ratings').alias(\"Mean\"),\n    sMean('Votes').alias('Votes')\n    ).orderBy(F.desc(\"Mean\")).toPandas()\n\nresults.head(10)\n\n\n\n\n\n  \n    \n      \n      Genre\n      Mean\n      Votes\n    \n  \n  \n    \n      0\n      Biography\n      7.795238\n      33137.952381\n    \n    \n      1\n      Music\n      7.475000\n      413.625000\n    \n    \n      2\n      History\n      7.416667\n      3330.541667\n    \n    \n      3\n      Animation\n      7.270712\n      8447.388742\n    \n    \n      4\n      Fantasy\n      7.231312\n      18573.573756\n    \n    \n      5\n      Adventure\n      7.228473\n      8999.588235\n    \n    \n      6\n      Crime\n      7.225607\n      15978.954034\n    \n    \n      7\n      Comedy\n      7.223730\n      4206.774311\n    \n    \n      8\n      Family\n      7.207407\n      635.759259\n    \n    \n      9\n      Drama\n      7.203856\n      17351.695414\n    \n  \n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (18,7)\n\nfig, ax = plt.subplots()\ntmp = results.sort_values('Votes', ascending=False)\nax.bar(tmp.Genre, tmp.Votes)\nax.set_xlabel('Votes By Category')\nplt.xticks(rotation=35)\n\nax2 = ax.twinx()\nax2.scatter(tmp.Genre, tmp.Mean, color='red')\nax2.legend(loc='upper right', labels=['Rating Mean'])\nplt.title(\"Vote Counts Vs Rating\")\n#plt.title(\"Relationships Between Votes and Their Ratings\")\nplt.show()"
  },
  {
    "objectID": "posts/nbks/2022-11-29-datatables-come-to-python.html",
    "href": "posts/nbks/2022-11-29-datatables-come-to-python.html",
    "title": "Datatables Were Migrated to Python",
    "section": "",
    "text": "Another Useful Tool From R\nBrowsing Medium can sometimes be quite useful; you can find some gems in there still. I came across this post which was about getting much faster read times from CSV files and the results looked really good. As I was reading it, I realized the command to read the files in was .fread() and then I realized this looked exactly like the data.table library from R. And, that’s exactly what it is: > Thanks for sharing the story on datatable Parul Pandey. The team H2O.ai is working tirelessly to add missing pandas.Frame functionalities to datatable. If there is something that you wished it would have to file issues here → https://github.com/h2oai/datatable/issues \ncf: Medium\nSo, let’s try it out!\n\nimport datatable as dt\nimport pandas as pd\nimport seaborn as sns\nfrom pathlib import Path\n\n\n\n\n\n\ndiamonds = sns.load_dataset('diamonds')\ndiamonds.head()\ndtDiamonds = dt.Frame(diamonds)\ndtDiamonds.head()\n\n\n\n  \n  \n    caratcutcolorclaritydepthtablepricexyz\n    ▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪\n  \n  \n    00.23IdealESI261.5553263.953.982.43\n    10.21PremiumESI159.8613263.893.842.31\n    20.23GoodEVS156.9653274.054.072.31\n    30.29PremiumIVS262.4583344.24.232.63\n    40.31GoodJSI263.3583354.344.352.75\n    50.24Very GoodJVVS262.8573363.943.962.48\n    60.24Very GoodIVVS162.3573363.953.982.47\n    70.26Very GoodHSI161.9553374.074.112.53\n    80.22FairEVS265.1613373.873.782.49\n    90.23Very GoodHVS159.46133844.052.39\n  \n  \n  \n    10 rows × 10 columns\n  \n\n\n\n\n\nNo Plotting By Default\nOne point which might harm someone’s willingness to switch over is that plotting is not built directly into the objects like it is with pandas. This means you’ll have to be explicit about importing and using matplotlib or seaborn. But, not only that becuase if you try to pass the datatable frame to Seaborn then it will fair:\n# You can run this but it will fail:\nsns.displot(dtDiamonds, x='x')\nWhen you run this, you will get the error: > ValueError: Could not interpret value x for parameter x  … and the code which causes this is:\n# Raise when data object is present and a vector can't matched\nif isinstance(data, pd.DataFrame) and not isinstance(val, pd.Series):\nSo, if it’s not a pandas data frame then seaborn just wont accept it. There is a matching tool which implements the Grammar of Graphics for python in the package plotnine. I tried doing this within the VM and it literally crashed my Virtual Machine. Not just my Python Kernel but the whole thing. So, we’re not going to do that. And, I wouldn’t recommend that you do it either. Which is a shame since I really like ggplot and the plotnine library from python.\n\n\nIs this Worth it?\nYou should check out the documentation to see if the analytics side of this tool is worth it. From using Datatable on the R side I’m definitely going to be trying this out. But, if I want to do any graphing then I have to convert to pandas - which has a cost to convert. Let’s measure the cost like the other bloggers did. First, we’ll write this to a CSV since we’ll have to account for the transition back.\n\ndiamonds.to_csv(Path(\"_data/diamonds.csv\")), len( diamonds )\nimport matplotlib.pyplot as plt\n\nI will have to copy the results because I just could not find a way to suppress the graphs printing while keeping the timeit outputs. You can copy and run these but keep in mind it will spam you with graphs.\n\n%%timeit -r2 -n10\ndata = pd.read_csv(Path(\"_data/diamonds.csv\"))\na = sns.displot(data, x='x', kde=True);\n\n\n%%timeit -r2 -n10\ndata = dt.fread(Path(\"_data/diamonds.csv\"))\na = sns.displot(data.to_pandas(), x='x', kde=True);\n\nResults:\npandas:    321 ms ± 2.59 ms per loop (mean ± std. dev. of 2 runs, 10 loops each)\ndatatable: 339 ms ± 8.55 ms per loop (mean ± std. dev. of 2 runs, 10 loops each)\nSo, pandas wins. This dataset though is small though so let’s try a more real world dataset. The analysis in the posts used a dataset with millions of rows so maybe we can test this using a much bigger dataset: All Lending Club loan data.\n\n# The big boi\npath = Path('_data/accepted_2007_to_2018Q4.csv')\n\n\n%%timeit -r2 -n3\ndata = pd.read_csv(path)\n_ = sns.displot(data, x='loan_amnt', kde=True);\n\n\n%%timeit -r2 -n3\ndata = dt.fread(path)\n_ = sns.displot(data.to_pandas(), x='loan_amnt', kde=True);\n\nResults:\npandas:    1min 8s ± 64.7 ms per loop (mean ± std. dev. of 2 runs, 3 loops each)\ndatatable: 55 s ± 389 ms per loop (mean ± std. dev. of 2 runs, 3 loops each)\n\n\nConclusions\nAnd, so datatable wins on the larger dataset even when you have to convert it over. So, somewhere between 53940 and 2260701 rows is where this works better. Like most tools, you’ll have to use your own judgement and your own circumstances whether you’ll find the tool useful. I’m definitely going to pick it up for no other reason than the read speed is superior and I happen to like the data.table experiences when I was using R."
  },
  {
    "objectID": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html",
    "href": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html",
    "title": "Minimal Spark Cluster",
    "section": "",
    "text": "If you’d like to setup Apache Spark to experiment with but you don’t want to use a premade ISO or setup your own then I’m going to show you how. This configuration will be a minimal one using Linux Operating Sytems; I’m going to use Ubuntu so change the install based on your package mananger. I’m going to assume that you’ve setup the hosts, their networking and have some way to configure and deploy them. There are options like Puppet or Salt but I’ll be avoiding those and leave them up to you."
  },
  {
    "objectID": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#datastore",
    "href": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#datastore",
    "title": "Minimal Spark Cluster",
    "section": "Datastore",
    "text": "Datastore\nNow we’re going to work around not having a Hadoop cluster. How this works, is that we’re going to create a shared folder on all of the hosts which references the Master as the Source of Truth. First, create a folder in your spark home to hold the data:\nmdkir $SPARK_HOME/Data\nGo ahead and create a file in here for future usage: touch turtles\nNext you’ll go ahead and install a package called sshfs which is used to remotely mount a folder from one host and another:\nsudo apt install sshfs\nRepeat this for all the hosts in your cluster. Once that is done, you’ll connect the slaves to the master using:\nsshfs <username>@<master-address>:/opt/spark-2.4.6-bin-hadoop2.7/Data /opt/spark-2.4.6-bin-hadoop2.7/Data\nNow you should be able to see the turtles file we created earlier if you list the files in the Data directory\nls Data\nIf you see the file then feel free to move on! If not, then double back and troubleshoot the connection between those two computers. Could also be permissions or something like that as well!"
  },
  {
    "objectID": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#connect-the-dots-start-the-services",
    "href": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#connect-the-dots-start-the-services",
    "title": "Minimal Spark Cluster",
    "section": "Connect the Dots, Start the Services",
    "text": "Connect the Dots, Start the Services\nNow that we’ve got it all connected together, go ahead and run the appropriate commands on the masters and servers to start them all up:\n# master:\n$SPARK_HOME/sbin/start-master.sh\n\n# slaves:\n$SPARK_HOME/sbin/start-slave.sh spark://<master-Addr>:7077"
  },
  {
    "objectID": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#success",
    "href": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#success",
    "title": "Minimal Spark Cluster",
    "section": "Success!",
    "text": "Success!\nNow try and run it on the master:\nusername@HOST:~# $SPARK_HOME/bin/pyspark \nPython 2.7.12 (default, Apr 15 2020, 17:07:12) \n[GCC 5.4.0 20160609] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n20/10/13 05:19:50 WARN Utils: Your hostname, HOST.localdomain resolves to a loopback address: 127.0.0.1; using <address> instead (on interface eth0)\n20/10/13 05:19:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n20/10/13 05:19:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.6\n      /_/\n\nUsing Python version 2.7.12 (default, Apr 15 2020 17:07:12)\nSparkSession available as 'spark'.\n>>> \nThat should give you the above.\nNow you can transfer data into that directory and read from it using the spark.read.* function that you need. Note that copying Big Data into that directory is not a good idea. If you’re looking at TeraBytes or Petabytes worth of data then you’ll definitely need a real Cluster. But, I’ve already made some interesting observations in this limited environment."
  },
  {
    "objectID": "posts/nbks/2022-10-07-cycle-jobs-part-two.html",
    "href": "posts/nbks/2022-10-07-cycle-jobs-part-two.html",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Two.",
    "section": "",
    "text": "This is Part Two of a Series towards scraping, cleaning and analyzing the Jobs for The Cycle: Frontier. If you haven’t read Part One then I’d suggest start there. We’re picking up now with cleaning the tasks to complete for the job instead. So, let’s get started! We’ll pull our normal libraries for working on projects like this.\n\nimport pandas as pd             # for the data.\nimport numpy as np              # for a NaN type\nimport matplotlib.pyplot as plt # For plotting, and some customization of plots.\nimport seaborn as sns           # For pretty plots.\n\n# Fix the size of the graphs\nsns.set(rc={\"figure.figsize\":(11, 8)})\n\nWe’re actually going to be using the same data table as before from the Official Wiki and the Jobs Page. Like before, wer’e going to use the same read_html() call targetting the name class.\n\nurl = \"https://thecyclefrontier.wiki/wiki/Jobs\"\nsite = pd.read_html(url, match=\"Name\",\n    converters = {\n        \"Name\": str,\n        \"Description\": str, \n        \"Unlocked\": int, \n        \"Tasks\": str,\n        \"Rewards\": str})\n\nAnd here is the data in the weird rows and columns like before:\n\n# Weird Problem: Data Looks funny, can still use this:\nsite[0].head(8)\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Unlock Level\n      Difficulty\n      Tasks\n      Rewards\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      4.0\n      Easy\n      Collect: 2 Hydraulic Piston 10 Hardened Metals\n      3800  K-Marks  1  Korolev Scrip  15  Korolev R...\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      7.0\n      Medium\n      Collect: 4 Derelict Explosives\n      11000  K-Marks  8  Korolev Scrip  52  Korolev ...\n    \n    \n      5\n      11000\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6\n      8\n      Korolev Scrip\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7\n      52\n      Korolev Reputation\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nWe’re going to make a copy of the specific columns we want to avoid any strange insert/update issues.\n\ntasksSubset = site[0][[\"Name\", \"Description\", \"Tasks\"]].copy()\ntasksSubset\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Tasks\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      Collect: 2 Hydraulic Piston 10 Hardened Metals\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      Collect: 4 Derelict Explosives\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      183\n      470\n      Korolev Reputation\n      NaN\n    \n    \n      184\n      No Expiry Date\n      There you are, finally! There's been an accide...\n      Collect: 10 Old Medicine\n    \n    \n      185\n      6300\n      K-Marks\n      NaN\n    \n    \n      186\n      9\n      Korolev Scrip\n      NaN\n    \n    \n      187\n      62\n      Korolev Reputation\n      NaN\n    \n  \n\n188 rows × 3 columns\n\n\n\n\n# get rid of the middle stuff we don't need.\ntasksSubset = tasksSubset[ ~tasksSubset.Tasks.isna()]\ntasksSubset.head(15)\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Tasks\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      Collect: 2 Hydraulic Piston 10 Hardened Metals\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      Collect: 4 Derelict Explosives\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n      Collect: 2 Zero Systems CPU 3 Ball Bearings\n    \n    \n      12\n      None of your Business\n      Prospector. We need Toxic Glands. Don't ask qu...\n      Collect: 2 Toxic Glands\n    \n    \n      16\n      Insufficient Processing Power\n      Prospector! The Zero Systems CPU you brought u...\n      Collect: 1 Master Unit CPU\n    \n    \n      20\n      Excavator Improvements\n      The suspension on our mining excavators need i...\n      Collect: 2 Co-TEC MultiTool 3 Ball Bearings 3 ...\n    \n    \n      24\n      A new type of Alloy\n      Our scientists are confident they can create a...\n      Collect: 4 Hardened Bone Plates 12 Compound Sh...\n    \n    \n      28\n      Automated Security\n      We will have to build new turrets to help prot...\n      Collect: 5 Zero Systems CPU 16 Hardened Metals\n    \n    \n      32\n      Energy Crisis\n      Veltecite supplies are low, but we need energy...\n      Collect: 4 Miniature Reactor\n    \n    \n      36\n      Classified I\n      Prospector! We need Derelict Explosives, Maste...\n      Collect: 10 Derelict Explosives 2 Master Unit ...\n    \n    \n      40\n      Clear Veltecite\n      The Veltecite you brought us the other day is ...\n      Collect: 2 Clear Veltecite\n    \n    \n      44\n      Time to Focus\n      One of our miners searching the Jungle for Foc...\n      Kill 6 Creatures at Jungle Collect: 4 Focus Cr...\n    \n    \n      48\n      Pure Veltecite\n      The Clear Veltecite was an improvement, we gai...\n      Collect: 2 Pure Veltecite\n    \n    \n      52\n      Titans of Industry\n      Scouts have found Titan Ore deposits on Fortun...\n      Collect: 2 Titan Ore 6 Altered Nickel\n    \n    \n      56\n      Crystal Frenzy\n      We're working on a new type of laser for our l...\n      Collect: 2 Clear Veltecite 8 Focus Crystal\n    \n  \n\n\n\n\nAs I discussed in the previous post, there are three kinds of jobs: Collect, Deposit and Kill. The Collect and Deposit jobs are fine since they involved something easily quantifiable: loot. However, the Kill quests present a very real problem since there is no simple way to address quanitfying them. For killing creatures, maybe we could take the sum of their expected drops and their rate of drop and include that as part of the rewards? Of course, the player may simply choose not to pick any of that up.\nAnother problem is killing players; how much is a player kill actually worth? And, the difficulty of killing players is connected to the skill level of each player - which we also cannot know. Therefore, I’ve elected to remove the Kill Jobs from the analysis.\n\n# anything with kill just remove until I can think of a better way to deal with this.\ntasksSubset = tasksSubset[ ~tasksSubset.Tasks.str.contains(\"Kill\")]\ntasksSubset.head(15)\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Tasks\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      Collect: 2 Hydraulic Piston 10 Hardened Metals\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      Collect: 4 Derelict Explosives\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n      Collect: 2 Zero Systems CPU 3 Ball Bearings\n    \n    \n      12\n      None of your Business\n      Prospector. We need Toxic Glands. Don't ask qu...\n      Collect: 2 Toxic Glands\n    \n    \n      16\n      Insufficient Processing Power\n      Prospector! The Zero Systems CPU you brought u...\n      Collect: 1 Master Unit CPU\n    \n    \n      20\n      Excavator Improvements\n      The suspension on our mining excavators need i...\n      Collect: 2 Co-TEC MultiTool 3 Ball Bearings 3 ...\n    \n    \n      24\n      A new type of Alloy\n      Our scientists are confident they can create a...\n      Collect: 4 Hardened Bone Plates 12 Compound Sh...\n    \n    \n      28\n      Automated Security\n      We will have to build new turrets to help prot...\n      Collect: 5 Zero Systems CPU 16 Hardened Metals\n    \n    \n      32\n      Energy Crisis\n      Veltecite supplies are low, but we need energy...\n      Collect: 4 Miniature Reactor\n    \n    \n      36\n      Classified I\n      Prospector! We need Derelict Explosives, Maste...\n      Collect: 10 Derelict Explosives 2 Master Unit ...\n    \n    \n      40\n      Clear Veltecite\n      The Veltecite you brought us the other day is ...\n      Collect: 2 Clear Veltecite\n    \n    \n      48\n      Pure Veltecite\n      The Clear Veltecite was an improvement, we gai...\n      Collect: 2 Pure Veltecite\n    \n    \n      52\n      Titans of Industry\n      Scouts have found Titan Ore deposits on Fortun...\n      Collect: 2 Titan Ore 6 Altered Nickel\n    \n    \n      56\n      Crystal Frenzy\n      We're working on a new type of laser for our l...\n      Collect: 2 Clear Veltecite 8 Focus Crystal\n    \n    \n      60\n      Geologist\n      You got time for a job, Prospector? The sample...\n      Collect: 2 Pure Veltecite 1 Pure Focus Crystal\n    \n  \n\n\n\n\nSo, each Job can request you collect multiple loots as well as more than one type of loot. Here is a good example of what I mean by this:\n\ntasksSubset.loc[28].Tasks \n\n'Collect: 5 Zero Systems CPU 16 Hardened Metals'\n\n\nAs you can see, this task requires you to collect both Zero System CPUs as well as Hardened Metals - and a good number of them. What we want is to not only extract each type of loot independently of each other but also to keep the count paired with the loot type.\nI would like to take this moment to thank the developers of Pandas. I spent a bit of time thinking about how I would solve this and they had already included a solution to this problem: extractall(). What this does is allows you to pass Regular Expressions and it will then pull out anything in the string which matches. It even puts them into their own separate rows! Again, thank you!\nFor Regular Expressions, this is something you will have to learn on your own. I used a website to test and build mine from an example row; there is plenty of documentaiton about how to use these.\n\nregex = r\"(\\d+\\s[\\w]+\\s[\\w]+)\"\ntmp = tasksSubset.Tasks.str.extractall(regex)\ntmp.head(15)\n\n\n\n\n\n  \n    \n      \n      \n      0\n    \n    \n      \n      match\n      \n    \n  \n  \n    \n      0\n      0\n      2 Hydraulic Piston\n    \n    \n      1\n      10 Hardened Metals\n    \n    \n      4\n      0\n      4 Derelict Explosives\n    \n    \n      8\n      0\n      2 Zero Systems\n    \n    \n      1\n      3 Ball Bearings\n    \n    \n      12\n      0\n      2 Toxic Glands\n    \n    \n      16\n      0\n      1 Master Unit\n    \n    \n      20\n      0\n      3 Ball Bearings\n    \n    \n      1\n      3 Hydraulic Piston\n    \n    \n      24\n      0\n      4 Hardened Bone\n    \n    \n      1\n      12 Compound Sheets\n    \n    \n      28\n      0\n      5 Zero Systems\n    \n    \n      1\n      16 Hardened Metals\n    \n    \n      32\n      0\n      4 Miniature Reactor\n    \n    \n      36\n      0\n      10 Derelict Explosives\n    \n  \n\n\n\n\nPerfect! Now we have all the different loot and we got to keep the row’s index for later when we’ll attach the job name and description. Before that though, we’ll need to do some work to separate the count and the loot type into their own columns. While I’m sure there is a better way to do this, I could not think of one so we’re going to write a function to break the loot and count apart and then return them.\nThere is a solid function for this already called .split() and we’re going to use it to split on spaces but we since some of the loot is multiple words we need to force it to only split once.\n\nexample = tmp.reset_index()[0][1]\nparts = example.split(' ', maxsplit=1)\nnumber, loot = int(parts[0]), parts[1]\nnumber, loot\n\n(10, 'Hardened Metals')\n\n\nNow we’ll create the function. What we can do here though is return either the count value or the loot value depending on an index passed: 0 for count and 1 for loot.\n\ndef breakLoot(taskString, index=0):\n    parts = taskString.split(' ', maxsplit=1)\n    if index == 0:\n        return int(parts[index])\n    elif index == 1:\n        return parts[index]\n    else:\n        # This shouldn't be called.\n        return None\n\nNow we just run two .apply() calls to get the values out:\n\ncount = tmp.reset_index()[0].apply(breakLoot).values\naLoot = tmp.reset_index()[0].apply(breakLoot, index=1).values\n\nAnd, then assign them to our brand new columns for them.\n\ntmp = tmp.assign(\n    count = count,\n    loot = aLoot\n)\n\ntmp.head(15)\n\n\n\n\n\n  \n    \n      \n      \n      0\n      count\n      loot\n    \n    \n      \n      match\n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      2 Hydraulic Piston\n      2\n      Hydraulic Piston\n    \n    \n      1\n      10 Hardened Metals\n      10\n      Hardened Metals\n    \n    \n      4\n      0\n      4 Derelict Explosives\n      4\n      Derelict Explosives\n    \n    \n      8\n      0\n      2 Zero Systems\n      2\n      Zero Systems\n    \n    \n      1\n      3 Ball Bearings\n      3\n      Ball Bearings\n    \n    \n      12\n      0\n      2 Toxic Glands\n      2\n      Toxic Glands\n    \n    \n      16\n      0\n      1 Master Unit\n      1\n      Master Unit\n    \n    \n      20\n      0\n      3 Ball Bearings\n      3\n      Ball Bearings\n    \n    \n      1\n      3 Hydraulic Piston\n      3\n      Hydraulic Piston\n    \n    \n      24\n      0\n      4 Hardened Bone\n      4\n      Hardened Bone\n    \n    \n      1\n      12 Compound Sheets\n      12\n      Compound Sheets\n    \n    \n      28\n      0\n      5 Zero Systems\n      5\n      Zero Systems\n    \n    \n      1\n      16 Hardened Metals\n      16\n      Hardened Metals\n    \n    \n      32\n      0\n      4 Miniature Reactor\n      4\n      Miniature Reactor\n    \n    \n      36\n      0\n      10 Derelict Explosives\n      10\n      Derelict Explosives\n    \n  \n\n\n\n\nAnd, there we go! We have our columns how we want them. Now we just need to work on getting the Name and Description values attached to our new data frame. One way to do this would be to do some sort of merge or join based on the index we’ve saved. Or, we can do something even easier!\nIf we look at the rows when we do an index reset:\n\ntmp.reset_index().head(8)\n\n\n\n\n\n  \n    \n      \n      level_0\n      match\n      0\n      count\n      loot\n    \n  \n  \n    \n      0\n      0\n      0\n      2 Hydraulic Piston\n      2\n      Hydraulic Piston\n    \n    \n      1\n      0\n      1\n      10 Hardened Metals\n      10\n      Hardened Metals\n    \n    \n      2\n      4\n      0\n      4 Derelict Explosives\n      4\n      Derelict Explosives\n    \n    \n      3\n      8\n      0\n      2 Zero Systems\n      2\n      Zero Systems\n    \n    \n      4\n      8\n      1\n      3 Ball Bearings\n      3\n      Ball Bearings\n    \n    \n      5\n      12\n      0\n      2 Toxic Glands\n      2\n      Toxic Glands\n    \n    \n      6\n      16\n      0\n      1 Master Unit\n      1\n      Master Unit\n    \n    \n      7\n      20\n      0\n      3 Ball Bearings\n      3\n      Ball Bearings\n    \n  \n\n\n\n\n… we can see that the column level_0 actaully contains duplicate indexes from our matches. So, the values Hydraulic Piston and Hardened Metals both are associated with Task with index 0. As long as we can use that index to get duplicate values then we can just pull all the Name and Descriptions in their matching order.\n\ntasksSubset.loc[tmp.reset_index().loc[:5, \"level_0\"], ['Name', 'Description']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n    \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n    \n    \n      12\n      None of your Business\n      Prospector. We need Toxic Glands. Don't ask qu...\n    \n  \n\n\n\n\n… which is exactly what we get! Duplicates! Time to slice it out and then assign the values.\n\nnameDescriptSlice = tasksSubset.loc[tmp.reset_index()[\"level_0\"], ['Name', 'Description']]\n\ntmp = tmp.assign(\n    name = nameDescriptSlice.Name.values,\n    description = nameDescriptSlice.Description.values\n)\n\ntmp.head(15)\n\n\n\n\n\n  \n    \n      \n      \n      0\n      count\n      loot\n      name\n      description\n    \n    \n      \n      match\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      2 Hydraulic Piston\n      2\n      Hydraulic Piston\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n    \n    \n      1\n      10 Hardened Metals\n      10\n      Hardened Metals\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n    \n    \n      4\n      0\n      4 Derelict Explosives\n      4\n      Derelict Explosives\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n    \n    \n      8\n      0\n      2 Zero Systems\n      2\n      Zero Systems\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n    \n    \n      1\n      3 Ball Bearings\n      3\n      Ball Bearings\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n    \n    \n      12\n      0\n      2 Toxic Glands\n      2\n      Toxic Glands\n      None of your Business\n      Prospector. We need Toxic Glands. Don't ask qu...\n    \n    \n      16\n      0\n      1 Master Unit\n      1\n      Master Unit\n      Insufficient Processing Power\n      Prospector! The Zero Systems CPU you brought u...\n    \n    \n      20\n      0\n      3 Ball Bearings\n      3\n      Ball Bearings\n      Excavator Improvements\n      The suspension on our mining excavators need i...\n    \n    \n      1\n      3 Hydraulic Piston\n      3\n      Hydraulic Piston\n      Excavator Improvements\n      The suspension on our mining excavators need i...\n    \n    \n      24\n      0\n      4 Hardened Bone\n      4\n      Hardened Bone\n      A new type of Alloy\n      Our scientists are confident they can create a...\n    \n    \n      1\n      12 Compound Sheets\n      12\n      Compound Sheets\n      A new type of Alloy\n      Our scientists are confident they can create a...\n    \n    \n      28\n      0\n      5 Zero Systems\n      5\n      Zero Systems\n      Automated Security\n      We will have to build new turrets to help prot...\n    \n    \n      1\n      16 Hardened Metals\n      16\n      Hardened Metals\n      Automated Security\n      We will have to build new turrets to help prot...\n    \n    \n      32\n      0\n      4 Miniature Reactor\n      4\n      Miniature Reactor\n      Energy Crisis\n      Veltecite supplies are low, but we need energy...\n    \n    \n      36\n      0\n      10 Derelict Explosives\n      10\n      Derelict Explosives\n      Classified I\n      Prospector! We need Derelict Explosives, Maste...\n    \n  \n\n\n\n\nFinally, we’ll drop all those extra columns we don’t need.\n\ntasks = tmp.reset_index().drop([\n    'level_0',\n    'match',\n    0\n], axis =1 )\n\ntasks = tasks[['name', 'count', 'loot', 'description']]\ntasks.head(15)\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      0\n      New Mining Tools\n      2\n      Hydraulic Piston\n      We are producing new Mining Tools for new Pros...\n    \n    \n      1\n      New Mining Tools\n      10\n      Hardened Metals\n      We are producing new Mining Tools for new Pros...\n    \n    \n      2\n      Explosive Excavation\n      4\n      Derelict Explosives\n      One of our mines collapsed with valuable equip...\n    \n    \n      3\n      Mining Bot\n      2\n      Zero Systems\n      Our engineers have designed an autonomous mini...\n    \n    \n      4\n      Mining Bot\n      3\n      Ball Bearings\n      Our engineers have designed an autonomous mini...\n    \n    \n      5\n      None of your Business\n      2\n      Toxic Glands\n      Prospector. We need Toxic Glands. Don't ask qu...\n    \n    \n      6\n      Insufficient Processing Power\n      1\n      Master Unit\n      Prospector! The Zero Systems CPU you brought u...\n    \n    \n      7\n      Excavator Improvements\n      3\n      Ball Bearings\n      The suspension on our mining excavators need i...\n    \n    \n      8\n      Excavator Improvements\n      3\n      Hydraulic Piston\n      The suspension on our mining excavators need i...\n    \n    \n      9\n      A new type of Alloy\n      4\n      Hardened Bone\n      Our scientists are confident they can create a...\n    \n    \n      10\n      A new type of Alloy\n      12\n      Compound Sheets\n      Our scientists are confident they can create a...\n    \n    \n      11\n      Automated Security\n      5\n      Zero Systems\n      We will have to build new turrets to help prot...\n    \n    \n      12\n      Automated Security\n      16\n      Hardened Metals\n      We will have to build new turrets to help prot...\n    \n    \n      13\n      Energy Crisis\n      4\n      Miniature Reactor\n      Veltecite supplies are low, but we need energy...\n    \n    \n      14\n      Classified I\n      10\n      Derelict Explosives\n      Prospector! We need Derelict Explosives, Maste...\n    \n  \n\n\n\n\n\nConclusions\nAnd, there we have it! Another piece to the puzzle solved. Next we’re going to work to combine all the faction rewards, the job requirements and the loot tables together to finally calculate which jobs you should definitly avoid doing."
  },
  {
    "objectID": "posts/nbks/2020-09-06-exploring-altair-visualization.html",
    "href": "posts/nbks/2020-09-06-exploring-altair-visualization.html",
    "title": "A Post Exploring Altair",
    "section": "",
    "text": "This Is The Tool We Have\nI’m trying out the fastpages in hopes that I wont have to spend the time building out my own website toolset. I’ve been slowly building something out of Wagtail which is a really just Djnago with some bells. The real allure though is going to be the Notebook conversions - specifically the Data Visualizations. The library for interactive version is Altair and we’re going to explore some data!\n\n\nLets Explore!\nSo, the tutorial for Scatter Plot uses the car data but I figured we mind as well do the classic Iris dataset. We’ll start by importing the dataset from vega_datasets - which is the Javascript library that Altair is built on top of - using iris = data.iris().\n\niris = data.iris()\niris\n\n\n\n\n\n  \n    \n      \n      sepalLength\n      sepalWidth\n      petalLength\n      petalWidth\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      145\n      6.7\n      3.0\n      5.2\n      2.3\n      virginica\n    \n    \n      146\n      6.3\n      2.5\n      5.0\n      1.9\n      virginica\n    \n    \n      147\n      6.5\n      3.0\n      5.2\n      2.0\n      virginica\n    \n    \n      148\n      6.2\n      3.4\n      5.4\n      2.3\n      virginica\n    \n    \n      149\n      5.9\n      3.0\n      5.1\n      1.8\n      virginica\n    \n  \n\n150 rows × 5 columns\n\n\n\nFirst lets see the graph - and then we’ll discuss the functions\n\nalt\\\n    .Chart(iris)\\\n    .mark_point()\\\n    .encode(\n        x='sepalLength',\n        y='sepalWidth',\n        color='species',\n        tooltip = ['species', 'petalLength', 'petalWidth']\n    )\\\n    .interactive()\n\n\n\n\n\n\nIt is interesting to note that - per the Docs - : > Create a basic Altair/Vega-Lite chart. > > Although it is possible to set all Chart properties as constructor attributes, > it is more idiomatic to use methods such as mark_point(), encode(), > transform_filter(), properties(), etc.\n.. which means that it’s found a way to do something similar to the R Programming Languages pipe operator. For reference, it would looks something like this:\nmtcars %>% \n    ggplot(aes(wt, mpg)) +\n    geom_point(aes(colour = factor(cyl)))\n\n\n\nExample R Graph\n\n\nFirst we tell altair to make a chart using the dataset we’re using:\n.Chart(iris)\n… which is then followed by the kind of graph that we’re after - in this case we’re after a scatter plot:\n.mark_point()\n… and then we tell it where everything belongs.\n.encode(\n    x='sepalLength',\n    y='sepalWidth',\n    color='species',\n    tooltip = ['species', 'petalLength', 'petalWidth']\n)\nOf interest is that you can add data from the other columns easily using the tooltip without having to add anything extra. Layering information which is relevant but lacks a meaningful graphic representation was a nice touch.\nThen of course, you allow users to interact with it via:\n.interactive()\nLets see what this post looks like on the blog!"
  },
  {
    "objectID": "posts/nbks/2023-01-19-trying-out-fastpitch-from-nvidia.html",
    "href": "posts/nbks/2023-01-19-trying-out-fastpitch-from-nvidia.html",
    "title": "Getting FastPitch From NVidia Working",
    "section": "",
    "text": "I spend a reasonable amount of time watching Twitch streamers online while working on projects. For those who might not know, Twitch is a platform for people to stream games or other creative content. Mostly, I watch people play video games that I like. And, while watching a channel you earn what are called Channel Points. These are used for all kinds of different tasks like disabling peoples guns to spamming the screen and even Text-to-Speech. These kinds of models are not quite in the domain I’d consider apart of what I focus on but they exist and would be a good first step into this area.\nWe’re going to start with using the Nvidia TTS FastPitch from Hugging Face. If you’re not aware of Hugging Face then definitely check it out; much of the state of the art models end up on this platform for general use.\n\n\nSettings this up with not as simple as pip install nemo_toolkit['all'] like stated in the documents. So, we’re going to go over all the problems and how I solved them along the way to getting this working.\n\n\n\nThe first problem is that while cython was included in the dependencies it was not installed like it should of been. I’m not sure why this is the case becuase running a python3 -m pip install cython worked fine. So, that was the first problem which was solved without issue.\n\n\n\nThis was an annoying one to solve. I tried to install llvmlite from PyPi and that sadly didn’t work. I ended up having to install this via the pacakge manager using sudo apt-get install llvm on my desktop. Also, you’ll want to configure the environmental variable for llvm once it’s installed using export LLVM_CONFIG=$(which llvm-config) as you’ll need it to install the package.\n\n\n\nFor my Desktop, I needed to install the python dev packages to get this installed. The error I got was missing the python.h while trying to compile the package. If you don’t have this already installed then python3 -m pip install python3-dev and then simply install python3 -m pip install pynini and it should work.\nStrangly, I didn’t have this problem on my laptop - which is running Arch - but instead I needed to download and compile OpenFST instead. I also ran an install of sudo pacman -S base-devel before this since the python-dev package does not exist in arch. And, then it worked fine."
  },
  {
    "objectID": "posts/nbks/2023-01-19-trying-out-fastpitch-from-nvidia.html#apply-and-use",
    "href": "posts/nbks/2023-01-19-trying-out-fastpitch-from-nvidia.html#apply-and-use",
    "title": "Getting FastPitch From NVidia Working",
    "section": "Apply and Use!",
    "text": "Apply and Use!\nOk, now that we’ve got this all install we’ll need some data. I thought it would be useful to find a Public Domain book on Project Gutenberg which sounded interesting. I settled on Two Years and Four Months in a Lunatic Asylum by Hiram Chase which sounded like it would be interesting to listen to. Collecting the data is easy and we’ve been over this multiple times in the past.\n\nimport requests\nurl = \"https://www.gutenberg.org/cache/epub/46179/pg46179.txt\"\ntitle = \"Two Years and Four Months in a Lunatic Asylum\".replace(' ', '-')\nr = requests.get(url)\ntext = r.text\n\nIf we tryto use this as is we get an error since there is simply too much data.\n\ntry:\n    parsed = spec_generator.parse(text)\n    spectrogram = spec_generator.generate_spectrogram(tokens=parsed)\n    audio = model.convert_spectrogram_to_audio(spec=spectrogram)\nexcept Exception as e:\n    print( e )\n\nWARNING! Your input is too long and could take a long time to normalize.Use split_text_into_sentences() to make the input shorter and then call normalize_list().\nmaximum recursion depth exceeded while calling a Python object\n\n\nSo, the methods refered to here are not specified anywhere in the documentation so I had to go find them. And, you can find them under nemo_text_processing.text_normalization.normalize which we’ll go ahead and do now:\n\nfrom nemo_text_processing.text_normalization.normalize import Normalizer\n\n# assert input_case in [\"lower_cased\", \"cased\"]\n# Another thing from the code which is not doucmented\nnorm = Normalizer(input_case='cased')\nsText = norm.split_text_into_sentences(text=text)\ntext = norm.normalize_list(sText)\n\nThe problem we have now is that the FastPitch model’s .parse() call requires a string and does not understand the list we get back from norm.normalize_list(). However, if we pass the normalize=False then it does accept it so we’ll do that. However, If we try to run this via a for loop then we’ll get a the dreaded CUDA Error:\n\ntry:\n\n    parsed = spec_generator.parse(text, normalize=False)\n    audio = model.convert_spectrogram_to_audio(\n        spec=spec_generator.generate_spectrogram(\n            tokens=parsed\n        )\n    )\n\nexcept Exception as e:\n    print( e )\n\nCUDA out of memory. Tried to allocate 132.88 GiB (GPU 0; 10.91 GiB total capacity; 6.64 GiB already allocated; 3.08 GiB free; 7.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
  },
  {
    "objectID": "posts/nbks/2022-12-16-valorant-color-crosshair.html",
    "href": "posts/nbks/2022-12-16-valorant-color-crosshair.html",
    "title": "Can We Find A Better Crosshair In Valorant?",
    "section": "",
    "text": "A word of warning: if you’re playing Valorant and came for the magic of better results then you’re in the wrong place. Fundamentals are where you should be spending your time and not on tricks which grant minor advantages over players at your own skill level. Granting that this post does what I expect it to - namely, that optimizing the color of a player’s crosshair on a per map basis will net some kind of advantage - then applying this wont help you if you cannot shoot. Tracking your crosshair matters - but it matters less than your accuracy and your mental."
  },
  {
    "objectID": "posts/nbks/2022-12-16-valorant-color-crosshair.html#draw-with-numbers.",
    "href": "posts/nbks/2022-12-16-valorant-color-crosshair.html#draw-with-numbers.",
    "title": "Can We Find A Better Crosshair In Valorant?",
    "section": "Draw With Numbers.",
    "text": "Draw With Numbers.\nFirstly, we’ll want to understand how to draw and image so when we start importing images we will be able to manipulate and visualize them. An image is nothing more than a height, a width an a color value for each pixel. So, if you wanted an image which is 10x10 then we would do the following:\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# generate test data\ntest_data = np.random.beta( size = [100, 100, 3], a = 1, b = 1)\n\n# display the test_data\nplt.imshow(test_data[0:20, 0:20, :]);\n\n\n\n\nWe can run this over and over again to get different results. What this is doing is taking random numbers from the Beta Distribution and then filling an array of 100 rows by 100 columns with another dimension of size 3. This is very important to remember because this is called the array’s shape in numpy parlance. And, to get images to display you will need to be very careful about what dimensions are where. In this case, the arrangement is the Height, then the Width and then the Color - or Depth if you prefer.\n\ntest_data.shape\n\n(100, 100, 3)\n\n\nRemember: The shape must look like this or else you get nothing. We’ll see once we start working with tensors that this order is not maintained."
  },
  {
    "objectID": "posts/nbks/2022-12-16-valorant-color-crosshair.html#aggregation-of-an-imagearray",
    "href": "posts/nbks/2022-12-16-valorant-color-crosshair.html#aggregation-of-an-imagearray",
    "title": "Can We Find A Better Crosshair In Valorant?",
    "section": "Aggregation Of An Image/Array",
    "text": "Aggregation Of An Image/Array\nSo, what we really want is the average color of the pixel on the screen. There are different kinds of average but we’ll use the Arithmatic Mean since that is what most people think of average. In the future, we may try out others and see if they’re better but maybe tomorrow.\nGetting the mean of an array is very easy thanks to numpy; we just run .mean() and we get what we want - or do we?\n\ntest_data.mean()\n\n0.4985771653927345\n\n\nSo, this number is nice but it doesn’t mean anything. And, that’s because we don’t want the mean of all the dimensions but really only along the height and width. We want to keep the color dimenion since that is what we’re actaully after. the mean() function allows us to specify the axis to calculate the mean of:\n\ntest_data[0:10, 0:10, :].mean( axis=(0, 1))\n\narray([0.55203737, 0.52206348, 0.4831622 ])\n\n\nExcellent! So, now we just need to fill some kind of image with these values and we can see the color!\n\ncup = np.random.beta( size = [10, 10, 3], a = 1, b = 1)\ncup[::] = test_data[0:10, 0:10, :].mean( axis=(0, 1))\nplt.imshow(cup);\n\n\n\n\nSince the data is random, seeing some form of grey here makes sense."
  },
  {
    "objectID": "posts/nbks/2022-12-16-valorant-color-crosshair.html#lets-meet-alex",
    "href": "posts/nbks/2022-12-16-valorant-color-crosshair.html#lets-meet-alex",
    "title": "Can We Find A Better Crosshair In Valorant?",
    "section": "Lets Meet Alex",
    "text": "Lets Meet Alex\nNow we’ll need real data now and we get that from the real world. Meet Alex.\n\n\n\n\n\nAlex plays Valorant on Twitch. Alex is very good at Valorant.  Sign FakeAnanas.\nWith Alex’s permission, I used some of his footage from Twitch to turn one of his streams into images for this idea. To convert the video to images I used ffmpeg which is a pretty common tool used in programs like OBS and other video tools. I created bunch of images from the frames using ffmpeg -i collected-alex.mp4 -vf fps=5 alex%d.png which extracted out hundreds of thousands of images for our use.\nSo, we have our aggregation and we have our image so let’s test this out: can we insert the average color into the image and split the image?\n\n# Get the shape of the image; find the middle of each dimension\nalexShape = np.array(alex).shape\n\n# numpy requires these be ints\nmidHeight, midWidth = int(alexShape[0]/2), int(alexShape[1]/2)\nmidHeight, midWidth\n\n(643, 1266)\n\n\n\n# Convert to a real array\nsample = np.array(alex)\nmeanColor = tuple(sample.mean(axis=(0,1)))\n\nsample[midHeight,:,:] = meanColor\nsample[:,midWidth,:] = meanColor\n\nplt.axis('off')\nplt.imshow( sample );\n\n\n\n\nIf you look carefully, the color inserts are there but hard to see. Let’s make them a bit wider and try again. Lets add make the line 16 pixels wide on both dimensions.\n\noffset=8\n\nsample[midHeight-offset:midHeight+offset,:,:] = meanColor\nsample[:,midWidth-offset:midWidth+offset,:] = meanColor\n\nplt.axis('off')\nplt.imshow( sample );"
  },
  {
    "objectID": "posts/nbks/2022-12-16-valorant-color-crosshair.html#what-is-contrast-really",
    "href": "posts/nbks/2022-12-16-valorant-color-crosshair.html#what-is-contrast-really",
    "title": "Can We Find A Better Crosshair In Valorant?",
    "section": "What is Contrast Really?",
    "text": "What is Contrast Really?\nSo, in laymen’s terms Contrast is when it is easy to distinguish between two parts with respect to colors. Two obvious examples would be the colors red and blue next to one another; you’re not going to struggle to tell where the blue nor the red is. But, I found this article easy enough for my poorly trained artistic understanding does a good job of outlining that contrast is not really about colors at all but actually about the luminosity - or the amount of light in the colors. The formula for this is pretty simple but thankfully the Python Image Library can already do this for us with ImageEnhance!\n\nfrom PIL import Image, ImageEnhance\n\n# Reset:\nsample = np.array(alex)\n\nheight, width, channels = 100, 100, 3\nimage_blank = np.zeros((height, width, channels), np.uint8)\nimage_blank[::] = tuple(sample.mean( axis=(0,1)))\nplt.imshow(image_blank);\n\n\n\n\n\n# Increase the contrast; make it really different!\nim = Image.fromarray(image_blank)\nimC = ImageEnhance.Contrast(im)\nplt.imshow(imC.enhance(10));\n\n\n\n\nThat should be much better so let’s insert that instead into our older image and see if it stands out.\n\ncontrastColor = tuple(np.array(imC.enhance(10))[0,0,:])\n\nsample[midHeight-offset:midHeight+offset,:,:] = contrastColor\nsample[:,midWidth-offset:midWidth+offset,:] = contrastColor\n\nplt.axis('off')\nplt.imshow( sample );\n\n\n\n\nLook how much better that is!"
  },
  {
    "objectID": "posts/nbks/2022-12-16-valorant-color-crosshair.html#bricks-n-clay",
    "href": "posts/nbks/2022-12-16-valorant-color-crosshair.html#bricks-n-clay",
    "title": "Can We Find A Better Crosshair In Valorant?",
    "section": "Bricks ’N Clay",
    "text": "Bricks ’N Clay\nOk, so now just need lots of images. Luckily, I already have those in the samples collected so now we just need some way to use them. This is the dirty part of all this which is not fun: getting, organizing and cleaning data. In this instance, cleaning is not really necessary as the data is about as ideal as it can get. We need to tag the images and pick a map to start with.\nMuch of the time labels come in a CSV file which contains a pairing of the data and the label: in this case the image and the map. So, we’ll have to create this from scratch. First, we’ll get all the files we’ve collect and then add them to dictionary where the image is one value and a blank map value is the other. This will allow us to insert the values manually - yes manually - for the images. We’ll glob for files which end in .png and then create the dictionary from them.\n\nfrom pathlib import Path\n\ndataDirectory = Path( path )\ntargetFiles = sorted( dataDirectory.glob('*.png') )\n\n# This returns a generator so you'll need to re-run it after you sanity check it\nimages = map( lambda x: {'image':x.name, 'map':''}, targetFiles)\nnext( images )\n\n{'image': 'alex00001.png', 'map': ''}\n\n\n\nData Problems\nIf you’ve used ffmpeg before and you noticed that something was wrong with the name of that file then good on you! This was actually a problem that I had to solve; namely that sorting by name created a problem where images were not showing up in what would of been assumed the proper order. A snippet of what happened when you sorted them before is:\n[PosixPath('###/images/alex1.png'),\n PosixPath('###/images/alex10.png'),\n PosixPath('###/images/alex100.png'),\n PosixPath('###/images/alex1000.png'),\n PosixPath('###/images/alex10000.png'),\n PosixPath('###/images/alex10001.png'),\n PosixPath('###/images/alex10002.png'),\n PosixPath('###/images/alex10003.png'),\n PosixPath('###/images/alex10004.png'),\n PosixPath('###/images/alex10005.png')]\nThe files are not being sorted in numerical order as would be expected becuase as far as the computer was concerned alex1.png and alex10.png where adjacent and not alex2.png. This makes the labeling impossible since the order is wrong. And, it was startling when I started trying to label everything when the images loaded were so random.\nTo fix this, we’ll need to expand the name of the file out and replace it with 0 filled names. First we needed to find the largest number to fill out to:\n\n# Get the files:\nfiles = dataDirectory.glob('*.png')\naSomething = [x.name for x in files]\n\n# Split off and sort by numbers:\nfilenames = sorted( aSomething, key = lambda x: int(x.split('.')[0].replace('alex', '')))\n\n# Find the largest number\nfilenames[-1]\n\n'alex53278.png'\n\n\nSo, now we know there are up to five digits we’ll need to fill. Thankfully, python already has a function to do this for us: zfill(). This will take a number as a string and then fill that string out. It can do other things as well but it is mainly for formatting text output - which we want to do here.\n\n'1'.zfill(5)\n\n'00001'\n\n\nNow we’ll test this to make sure it works:\nlist(\n    map(lambda x: str(x.parent/'alex') + x.name\\\n            .split('.')[0]\\\n            .replace('alex', '')\\\n            .zfill(5)+'.png',\n        targetFiles[:10]))\n['###/images/alex00001.png',\n '###/images/alex00010.png',\n '###/images/alex00100.png',\n '###/images/alex01000.png',\n '###/images/alex10000.png',\n '###/images/alex10001.png',\n '###/images/alex10002.png',\n '###/images/alex10003.png',\n '###/images/alex10004.png',\n '###/images/alex10005.png']\nAnd, it does! Now we just move the files to fix their names:\nimport shutil as fm\n\nfor filename in targetFiles:\n    newFileName = str(filename.parent/'alex') + filename.name.split('.')[0].replace('alex', '').zfill(5)+'.png'\n    fm.move(filename, newFileName)\n\n\nBack To Our CSV\nSo, now we can return to making the CSV file we’ll be putting out labels in:\n\ntargetFiles = sorted( dataDirectory.glob('*.png') )\n\n# This returns a generator so you'll need to re-run it after you sanity check it\nimages = list(map( lambda x: {'image':x.name, 'map':''}, targetFiles))\n\n# And, our blank csv for labels:\nimageSolutions = pd.DataFrame(images)\nimageSolutions.to_csv(dataDirectory/'alexImages.csv', index=False)\n\nAnd, now the non-fun of labeling the data by typing entries into the CSV. I’m only going to do Ice Box in this post since it was the first map I ran across while reviewing the data. So, we’ll focus on just that map. I went ahead and labeled some of the images for Icebox and left the others as blank."
  },
  {
    "objectID": "posts/nbks/2022-12-16-valorant-color-crosshair.html#inserting-pytorch-datasets",
    "href": "posts/nbks/2022-12-16-valorant-color-crosshair.html#inserting-pytorch-datasets",
    "title": "Can We Find A Better Crosshair In Valorant?",
    "section": "Inserting PyTorch Datasets",
    "text": "Inserting PyTorch Datasets\nThis is the unnecessary part of this post; I wanted to play around with this to better understand this Dataset class as a general tool. Pytorch comes with something called a Dataset which a simple way to encapsulate the data being used. You can think of it as front end API to a dataset. And, that’s exactly how I’m going to use it.\nPer the documentation, this class needs to implement three functions: __init__, __len__ and __getitem__. To keep with our Front End idea:\n\n__init__ does the processing of the data.\n__len__ tells you how many there are.\n__getitem__ gives you one of your data from the dataset.\n\nIn the example, all the configuration details are included when you create the object: where the labels can be found, where the data is being stored and what transforms should be applied when an item is called. Note that none of the transforms are ran when the object is created but instead are ran when one of the data is asked for. We’re not actually going to be putting the calculation of the means into the transforms - but we could definitely do that instead of iterating through and processing them.\nFirst, we’re going to need to define the __init__ part of our object; what do we want this to do? Again, we want it to: 1. Store where the data is. 2. Tell it where to find the labels for the data.\nI will be adding another function to my dataset: filtering based on map. Right now, I only have one map to work with and I don’t want to think about the others yet. And, as more data is added I want to be able to ask only for specific maps per dataset. So, we’ll want a filter in here too.\nclass ImageAverageDataset(Dataset):\n    def __init__(self, annotations, directory, filter=None):\n        self.data = pd.read_csv(directory/annotations)\n        self.directory = directory\n        # here is our filter; we will need to define this function still.\n        self.data = self.imageFilter(self.data, filter)\nImplementing the length is easy enough:\ndef __len__(self):\n    return len(self.data)\n… and lastly we want to be able to ask for samples from specific maps:\ndef imageFilter(self, data, filter):\n        return data.query(f'map == \"{filter}\"')\nPutting this all together with the imports and we have our Dataset class for our project.\n\nimport pandas as pd\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom torchvision.io import read_image\n\n\nclass ImageAverageDataset(Dataset):\n    def __init__(self, annotations, directory, filter=None):\n        self.data = pd.read_csv(directory/annotations)\n        self.directory = directory\n        self.data = self.imageFilter(self.data, filter)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image, label = tuple(self.data.iloc[idx])\n        tensor = read_image( str(self.directory) + '/' + image)\n        return tensor, label\n\n    def imageFilter(self, data, filter):\n        return data.query(f'map == \"{filter}\"')\n\nExcellent! Now we just point this at where our data and our annotations file are to create it:\nicebox = ImageAverageDataset('alexImages.csv', dataDirectory, 'Icebox')\nLet’s pull one of them!\n\nplt.imshow(icebox[10])\n\n# ...\n\nTypeError: Image data of dtype <U6 cannot be converted to float\nAh, we’re getting both a label and an array back. But also, is that really an array? Recall that I had mentioned much earlier that tensors don’t maintain the dimensions? Here they are!\n\ntype(icebox[10][0])\n\ntorch.Tensor\n\n\nSo, what is a Tensor anyways? Well, these are what Pytorch calls arrays and is a different implementation of the same idea. Tensors come from Physics and were the initial inspiration for all this decades ago. And, the assumptions are not the same between numpy and torch. In the code above there is a call to the function read_image() and this takes the image and converts it into a tensor and not an array. When we look at the shape this will be more obvious.\n\nicebox[10][0].shape\n\ntorch.Size([3, 1080, 1920])\n\n\nSo, the color is the first dimension now instead of the last dimension for the images we worked with before. This is a problem because when we try to display the image it wont know what to do with this. The fastest way I have found to solve this is with a call to .swapaxis() which will change the shape by reordering the dimensions.\n\nplt.axis('off')\nplt.imshow(icebox[10][0].moveaxis(0,2));\n# Dont actually need to convert to .numpy() and it still works fine.\n# I didn't know that but hurray!\n\n\n\n\n\nGeneralize this all.\nAnd, now we come to the fun part: calculations and the conclusions. All we need now is to iterate through our dataset collecting the mean color for each image. Then, take another mean of all the color values to get our average color across all images. We’ve seen most of this before so here is the code for it:\n\n# tuple(np.mean(np.array(alex)[:, :, :], axis=(0,1)))\ndef getMeanColor(img):\n    \"\"\"Convert the tensor and get the mean color\"\"\"\n    return tuple(\n        img.moveaxis(0,2).numpy().mean(axis=(0,1))\n    )\n\ncontainer = pd.DataFrame({'y':[], 'x':[], 'c':[]})\nfor i in range(len(icebox)):\n    container.loc[i, :] = getMeanColor(icebox[i][0])\n\nYou may think - being a quality pythonista - that you could iterate through each image instead of using an index like I have. Well, I tried that initially and these datasets don’t implement the necessary functions to be a generator. When trying that, I kept getting the labels from the CSV instead of the data. I’m not sure the how or why but you’re welcome to add those to the function if you’re - ahem - borrowing this for your own projects. Or, do what I did since that works too.\nNext we calculate the mean of all the images and store that in a new image to display.\n\nheight, width, channels = 100, 100, 3\nimage_blank_new = np.zeros((height, width, channels), np.uint8)\nimage_blank_new[::] = container.sum(axis=0) # tuple(np.mean(np.array(alex)[:, :, :], axis=(0,1)))\n\nplt.imshow( image_blank_new );\n\n\n\n\nAND ENHANCE!\n\nim_new = Image.fromarray(image_blank_new)\nimC_new = ImageEnhance.Contrast(im_new)\nplt.imshow(imC_new.enhance(10));\n\n\n\n\nI didn’t think that Ice Box had a pink tint to it but that’s what the results are. Now we’ll want to get this color and then convert it to a hexcode for use. Again, python comes to the resuce as matplotlib has a function to do this once we’ve corrected the color.\n\nfrom matplotlib.colors import rgb2hex\nnp.array(imC_new.enhance(10)).mean(axis=0)[0]/255\nrgb2hex(np.array(imC_new.enhance(10)).mean(axis=0)[0]/255)\n\n'#ff00ff'"
  },
  {
    "objectID": "posts/nbks/2022-12-01-do-we-replace-pandas.html",
    "href": "posts/nbks/2022-12-01-do-we-replace-pandas.html",
    "title": "Pandas Is No Longer My Default",
    "section": "",
    "text": "Quick Observation\nI am starting to understand why there are so many posts about tools and not as much analysis. The desire to post content and not spend weeks working through problems which may not work out makes posting about new tools quite alluring. That is not to say these posts are of no value but they’re certainly not results driven.\n\n\nModin Is My New Default.\nI wish I could give credit to where I stumbled across this but I admittely already lost it. Coming soon after my previuos post about being excited that Datatables have come to Python - and I will certianly do a follow up post about using datatables -, I fell over another new tool which is definitely replacing Pandas for me: Modin. So, what is this?: > The modin.pandas DataFrame is an extremely light-weight parallel DataFrame. Modin transparently distributes the data and computation so that all you need to do is continue using the pandas API as you were before installing Modin. Unlike other parallel DataFrame systems, Modin is an extremely light-weight, robust DataFrame. Because it is so light-weight, Modin provides speed-ups of up to 4x on a laptop with 4 physical cores.\nIt is a drop in replacement which is bulit out of Pandas. But, it also has two libraries underneath - if you installed them - to silently allow you to scale your analysis data size upwards. Take the previous post which was done where we did a test to see how long importing and posting the graph - but now in modin!\n\n\nCode\nimport modin.pandas as md\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport seaborn as sns\n\n\n\n# If it uses ray:\n# import ray\n# ray.init()\n\n\nYou can change the engine it uses underneath by passing different arguments on the command line - per the Docs.\n\n\nCode\nimport os\n\nos.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\nos.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n\n\n\n\nI don’t Recommend.\nThis might be jarring for me to be excited - and then not. But, I tried to get this all working - and the good news is that I did. But, only kind of. When I ran without setting up which to use, it defaulted to Ray and that redlined my CPU and locked up my system. I did end up getting a successful run but the speed of it was moot.\n\n\nCode\n%%timeit -r2 -n3\n# The big boi\npath = Path('../../_drafts/_data/accepted_2007_to_2018Q4.csv')\n\ndata = md.read_csv(path)\n_ = sns.displot(data, x='loan_amnt', kde=True);\n\n\n1min 47s ± 1.34 s per loop (mean ± std. dev. of 2 runs, 3 loops each)\nIf we compare those numbers to the tests from the previous post, it is worse:\npandas:    1min 8s ± 64.7 ms per loop (mean ± std. dev. of 2 runs, 3 loops each)\ndatatable: 55 s ± 389 ms per loop (mean ± std. dev. of 2 runs, 3 loops each)\nSo, I have worse performance with the risk of crashing my computer. Maybe there is a way to tune the CPU usage so it is not this painful but under these conditions I cannot trust using this on a large dataset."
  },
  {
    "objectID": "posts/nbks/2022-10-05-cycle-loot-values-download.html",
    "href": "posts/nbks/2022-10-05-cycle-loot-values-download.html",
    "title": "Downloading and Exploring Loot in Cycle Frontier",
    "section": "",
    "text": "First, we’ll pull our normal libraries for working on projects like this.\nOur data source is still going to be the Official Wiki and we’ll be pulling from the Loot Page - which has already started being updated for Season Two. From my previous post, the class zebra is still the best attribute to target to pull the data into a usable Data Frame.\nAgain, from previous experience, it is best to create a copy of columns that we actually need.\nLooking at our data, we already have a problem: the Printing column has columns NaN values insted of no. The blank values got converted to NaN when it was imported. While I’m sure there is a way to correct this on import, this isn’t how I’m going to do it. What I’m going to do is find the indexes which contain Yes and then update the other columns. You might be thinking - correctly - that we could simply find the indexes of the other values and then update those. Unfortunately, this is harder and I’ll show you what I mean.\nIf we check the values, we’ll see there is this nan value which doens’t look like the normal np.NaN value.\nAnd, if we try to do a logical comparison with the literal value from the data it fails.\nHere we so the problem with objects and probably the pointers underneath. We could fix this but it’s much easier to reverse what we’re detecting so I’m doing that instead.\nI’m not going to spend much time explaining this part; please see the previous posts about how this works.\nLooking at the rows, we can see the data we’re after - the Name of the Loot - is in the Name column. Luckily, like the previous post about data extraction, these are in multiples of 5 insted of 4 like before. So, we can simply borrow the same code from before and update the range.\nNow we’ll build the loop for the iteration like before.\nNow we need a column to dump the correct values into which is going to be called Loot. We’ll fill these with np.NaN so we can repeat the same trick as the previous post.\nSo, pull the name of of the loot per subset and then update it like before.\nIf we play through the old trick of dropping the Loot columns now we’ll lose valuable data. So, we’ll need a new trick to keep that data. Thankfully, this is a problem I’d already solved at a previous time: we’ll simply fill the values. As a word of caution, filling values can be dangerous to an analysis so if you’re doing this then make sure it will not have a negative impact. In this instance, I’m duplicating the data so that it’s not lost at all. Even then, we’ll need to be cautious in the future about how the data is used.\nThere are a few techniques for filling in missing values and one such technique is called Fill Forward. What this does is take the values in a column, take that value and then simply inserts it down the rows where it finds NAs. This is exactly what we’re after - but only for those specific columns.\nThen, we’ll take these values and simply insert them into the real data frame where we want them.\nAnd, now we’ll pull the same trick and delete the rows with np.NaN.\nAlmost there! The column name Image is not what that should be called so we’ll update that to Unit like we had in the prevous post.\nJust one more problem I’d like to correct before we do some fun questions at the end. Pandas has also copied the idea of Categoreies from R. And, it allows us to set the order of the values as well. We’re going to use this in a minute but it’s also a nice to have.\nChanging the type is just as easy as you’d think:\nYou can see that now there is a list of the possible Categories below the values: Categories (6, object): ['Common', 'Epic', 'Exotic', 'Legendary', 'Rare', 'Uncommon']. However, the order matters and so we cannot leave this how it is; Uncommon is clealy not larger in type then Epic. The proper way to do this would be to declare the order - which is what is next. We’ll use pd.Categorical() to convert them and set the order of the Categories.\nThere we go! See that the order has now been fixed: Categories (6, object): ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']. Here I would like to note that I did take liberty to set Legendary above Exotic since the wiki as them mixed and the actual games sorting puts Legendary above Exotic and so I did the same."
  },
  {
    "objectID": "posts/nbks/2022-10-05-cycle-loot-values-download.html#what-are-the-best-two-items-to-sell-per-rarity",
    "href": "posts/nbks/2022-10-05-cycle-loot-values-download.html#what-are-the-best-two-items-to-sell-per-rarity",
    "title": "Downloading and Exploring Loot in Cycle Frontier",
    "section": "What Are The Best Two Items To Sell, Per Rarity?",
    "text": "What Are The Best Two Items To Sell, Per Rarity?\nThis is a pretty simple ask. We just have to remember that we’re working with Tidy Data and we’ll want to filter the Name column for K-Marks before doing anything else. Next, we’ll do a groupby() for the Rarity and then pull out the Units column since that has the values we’re after. Luckily, pandas already has a function to get the largest values .largest() which also works as an aggregation function for groupby().\n\ntmp = lootData.query(\"Name == 'K-Marks'\").groupby(\"Rarity\")['Unit'].nlargest(2)\ntmp\n\nRarity        \nCommon     336       900.0\n           111       760.0\nUncommon   356      3417.0\n           181      1709.0\nRare       371     11533.0\n           366      5126.0\nEpic       121     20183.0\n           396     17300.0\nExotic     401    129746.0\n           106     77848.0\nLegendary  441    518985.0\n           431    116772.0\nName: Unit, dtype: float64\n\n\nSo, we have our values but we don’t have any way to identify them since we cannot include the Name column since then it becomes a Data Frame and .nlargest() doesn’t work. If we look closely, the old indexes have been carried over and are included in our results so we’ll need to give ourselves access to them. We’re going to do this with .reset_index() since that will push those indexes into the Data Frame.\n\ntmp.reset_index()\n\n\n\n\n\n  \n    \n      \n      Rarity\n      level_1\n      Unit\n    \n  \n  \n    \n      0\n      Common\n      336\n      900.0\n    \n    \n      1\n      Common\n      111\n      760.0\n    \n    \n      2\n      Uncommon\n      356\n      3417.0\n    \n    \n      3\n      Uncommon\n      181\n      1709.0\n    \n    \n      4\n      Rare\n      371\n      11533.0\n    \n    \n      5\n      Rare\n      366\n      5126.0\n    \n    \n      6\n      Epic\n      121\n      20183.0\n    \n    \n      7\n      Epic\n      396\n      17300.0\n    \n    \n      8\n      Exotic\n      401\n      129746.0\n    \n    \n      9\n      Exotic\n      106\n      77848.0\n    \n    \n      10\n      Legendary\n      441\n      518985.0\n    \n    \n      11\n      Legendary\n      431\n      116772.0\n    \n  \n\n\n\n\n\n# Keep those index numbers\ntmp = tmp.reset_index()\n\nThe index column is called level_1 which we’re going to simply insert into the original data.\n\n# Best items to sell per rarity:\nlootData.loc[ tmp['level_1'].tolist() ].sort_values(\"Rarity\", ascending=False)[['Unit', 'Name', 'Rarity', 'Loot']]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Loot\n    \n  \n  \n    \n      441\n      518985.0\n      K-Marks\n      Legendary\n      Alpha Crusher Head\n    \n    \n      431\n      116772.0\n      K-Marks\n      Legendary\n      Savage Marauder Head\n    \n    \n      401\n      129746.0\n      K-Marks\n      Exotic\n      Alpha Crusher Heart\n    \n    \n      106\n      77848.0\n      K-Marks\n      Exotic\n      Progenitor Slag\n    \n    \n      121\n      20183.0\n      K-Marks\n      Epic\n      NiC Oil Cannister\n    \n    \n      396\n      17300.0\n      K-Marks\n      Epic\n      Crusher Flesh\n    \n    \n      371\n      11533.0\n      K-Marks\n      Rare\n      Crusher Hide\n    \n    \n      366\n      5126.0\n      K-Marks\n      Rare\n      Mature Rattler Eyes\n    \n    \n      356\n      3417.0\n      K-Marks\n      Uncommon\n      Hardened Bone Plates\n    \n    \n      181\n      1709.0\n      K-Marks\n      Uncommon\n      Derelict Explosives\n    \n    \n      336\n      900.0\n      K-Marks\n      Common\n      Toxic Glands\n    \n    \n      111\n      760.0\n      K-Marks\n      Common\n      Progenitor Composite\n    \n  \n\n\n\n\nThere we go! Well, except that this doesn’t account for weight and what we really want is to carry the most value per weight since we’re limited in the game by the backpack size. Let’s do the same but for the per weight value instead.\n\ntmp = lootData.query(\"Name == 'K-Marks / Weight'\").groupby(\"Rarity\")['Unit'].nlargest(2).reset_index()\nlootData.loc[ tmp['level_1'].tolist() ].sort_values(\"Rarity\", ascending=False)[['Unit', 'Name', 'Rarity', 'Loot']]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Loot\n    \n  \n  \n    \n      443\n      17300.0\n      K-Marks / Weight\n      Legendary\n      Alpha Crusher Head\n    \n    \n      433\n      5839.0\n      K-Marks / Weight\n      Legendary\n      Savage Marauder Head\n    \n    \n      108\n      6487.0\n      K-Marks / Weight\n      Exotic\n      Progenitor Slag\n    \n    \n      403\n      6487.0\n      K-Marks / Weight\n      Exotic\n      Alpha Crusher Heart\n    \n    \n      123\n      4037.0\n      K-Marks / Weight\n      Epic\n      NiC Oil Cannister\n    \n    \n      118\n      3844.0\n      K-Marks / Weight\n      Epic\n      Letium Clot\n    \n    \n      373\n      1153.0\n      K-Marks / Weight\n      Rare\n      Crusher Hide\n    \n    \n      368\n      1025.0\n      K-Marks / Weight\n      Rare\n      Mature Rattler Eyes\n    \n    \n      458\n      570.0\n      K-Marks / Weight\n      Uncommon\n      Print Resin\n    \n    \n      358\n      380.0\n      K-Marks / Weight\n      Uncommon\n      Hardened Bone Plates\n    \n    \n      148\n      1000.0\n      K-Marks / Weight\n      Common\n      Old Currency\n    \n    \n      283\n      506.0\n      K-Marks / Weight\n      Common\n      Nutritional Bar\n    \n  \n\n\n\n\nAnd, now we see there are some important changes. The values in the Common Rarity are totally different!"
  },
  {
    "objectID": "posts/nbks/2022-10-05-cycle-loot-values-download.html#are-the-rarity-values-linear",
    "href": "posts/nbks/2022-10-05-cycle-loot-values-download.html#are-the-rarity-values-linear",
    "title": "Downloading and Exploring Loot in Cycle Frontier",
    "section": "Are the Rarity Values Linear?",
    "text": "Are the Rarity Values Linear?\nWe’ll aggregate over all the values in a category and then simply plot them.\n\nlootData.query(\"Name == 'K-Marks'\").groupby(\"Rarity\")['Unit'].mean().plot()\nplt.title(\"Mean K-Marks Per Rarity\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Mean K-Marks Per Rarity')\n\n\n\n\n\nWait, what? This looks terrible! Everything from Common to Epic is worth nothing compared to Exotic and Legendary. Maybe we’re dealing with some outlier problems? Let’s check the median just in case.\n\nlootData.query(\"Name == 'K-Marks'\").groupby(\"Rarity\")['Unit'].median().plot()\nplt.title(\"Median K-Marks Per Rarity\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Median K-Marks Per Rarity')\n\n\n\n\n\nThis is even worse! Legendary loot is definitely being affected by outliers but Exotic is so far and above better than everything else. Maybe we’re looking at the wrong values? Maybe we’re making the same mistake and we need to account for the per weight?\n\nlootData.query(\"Name == 'K-Marks / Weight'\").groupby(\"Rarity\")['Unit'].median().plot()\nplt.title(\"Median K-Marks/Weight Per Rarity\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Median K-Marks/Weight Per Rarity')\n\n\n\n\n\nI mean it’s better but not really. What is in this category?\n\nlootData.query(\"Rarity == 'Exotic'\").query(\"Name == 'K-Marks'\")\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Personal Quarters\n      Campaigns\n      Jobs\n      Printing\n      Loot\n    \n  \n  \n    \n      91\n      8650.0\n      K-Marks\n      Exotic\n      Yes x4\n      Yes x9\n      Yes\n      No\n      Charged Tharis Iron Ingot\n    \n    \n      106\n      77848.0\n      K-Marks\n      Exotic\n      Yes x6\n      Yes x9\n      Yes\n      No\n      Progenitor Slag\n    \n    \n      401\n      129746.0\n      K-Marks\n      Exotic\n      Yes x6\n      Yes x1\n      Yes\n      No\n      Alpha Crusher Heart\n    \n  \n\n\n\n\nThis category makes no sense and I don’t know why it exists. I’m not sure they understand what this category is for either looking at what is in here. But, that will be it for now."
  },
  {
    "objectID": "posts/nbks/2022-10-26-how-to-quickly-use-other-formats-in-python.html",
    "href": "posts/nbks/2022-10-26-how-to-quickly-use-other-formats-in-python.html",
    "title": "Collecting External Data for Python.",
    "section": "",
    "text": "While preparing for some upcoming blog posts taking material from Design and Analysis of Experiments With R by John Lawson, I wanted to convert the problems and solutions from R code to Python code. Diong this will require using the real data and - luckily - the data from the book is online on Github. Due to how these packages are, the data is uploaded and kept as binary data which we can use. Unfortunately, the data is in the .rda format which doesn’t convert easily into python.\nThere is a package for this to convert the data: pyreadr. Which we’re doing to use to convert the data into a dataframe Python understands. Sadly, this package doesn’t handle urls so we’ll need to download the data first. We could clone out the whole repository to collect the data but then we’d have to start manually managing the data - which I don’t want to do.\nAfter a bit of working around, we can use the tempfile builtin package from Python to create a temporary file to dump the data into. This is useful since these will be deleted after it’s .close() is called on the file. But, we’ll want a Named version since we want this accessible to the file system: > This function operates exactly as TemporaryFile() does, except that the file is guaranteed to have a visible name in the file system (on Unix, the directory entry is not unlinked). That name can be retrieved from the name attribute of the returned file-like object. Whether the name can be used to open the file a second time, while the named temporary file is still open, varies across platforms (it can be so used on Unix; it cannot on Windows).\nSource\nWe’ll use the requests library to pull the data from the internet since it’s builtin and easy to use.\n\n# !pip install pyreadr\nimport pyreadr as pyr\nimport tempfile as tmp\nimport requests as r\n\nOne caveat here is that you’ll need to rewind the read location in the file to read the temporary file otherwise you’ll get an LibrdataError: Unable to read from file.\n\nwith tmp.NamedTemporaryFile() as f:\n    something = r.get(\"https://github.com/cran/daewr/raw/master/data/Apo.rda\").content\n    f.write(something)\n    f.seek(0)\n    data = pyr.read_r(f.name)['Apo']\n\ndata.head(15).T\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n    \n  \n  \n    \n      lab\n      A\n      A\n      A\n      A\n      A\n      A\n      A\n      B\n      B\n      B\n      B\n      B\n      B\n      B\n      B\n    \n    \n      conc\n      1.195\n      1.144\n      1.167\n      1.249\n      1.177\n      1.217\n      1.187\n      1.155\n      1.173\n      1.171\n      1.175\n      1.153\n      1.139\n      1.185\n      1.144\n    \n  \n\n\n\n\nThere we go! You can use this as a simple way to collect data from the internet and feed it into a package which doesn’t support urls to read in data. You can expect its usage in the near future while I work through the textbook."
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "",
    "text": "Welcome back to the fourth post in the series. Last time, we were having issues with the data since there were missing values - including the Job that prompted the initial investigation. We’ll jump right into it picking up with the mismatch between naming in tables.\nWhile working through the list of problems I found reviewing the data, there is clearly an error in the Regex built before. The symptom of this was the Autoloader which has a hanging number when the group is pulled out.\n\ntasksSubset = siteJobs[1][[\"Name\", \"Description\", \"Tasks\"]].copy()\ntasksSubset = tasksSubset[ ~tasksSubset.Tasks.isna()]\ntasksSubset = tasksSubset[ ~tasksSubset.Tasks.str.contains(\"Kill\")]\n\nregex = r\"(\\d+\\s[\\w]+\\s[\\w]+)\"\ntmp = tasksSubset.Tasks.str.extractall(regex)\ntmp[tmp.reset_index()[0].str.contains('Autoloader').values]\n\n\n\n\n\n  \n    \n      \n      \n      0\n    \n    \n      \n      match\n      \n    \n  \n  \n    \n      8\n      0\n      1 Autoloader 5\n    \n    \n      32\n      0\n      2 Autoloader 8\n    \n  \n\n\n\n\nAfter some work, this was because the hanging [\\w]+ was too greedy and had to be toned down.\n\nnewRegex = r\"(\\d+\\s[\\w]+\\s?[a-zA-Z]+)\"\ntmp = tasksSubset.Tasks.str.extractall(newRegex)\ntmp[tmp.reset_index()[0].str.contains('Autoloader').values]\n\n\n\n\n\n  \n    \n      \n      \n      0\n    \n    \n      \n      match\n      \n    \n  \n  \n    \n      8\n      0\n      1 Autoloader\n    \n    \n      32\n      0\n      2 Autoloader\n    \n  \n\n\n\n\nThis didn’t end up being the only correction to the Regex; in fact, the Co-Tool Multitool was being broken apart since it has a - in the name. So, we had to update this to check and include the - on the split.\n\nnewRegex = r\"((\\d+\\s[\\w\\-]+\\s?[a-zA-Z]+))\"\n\nNow we’ll have our updated version of the breakLoot function we write before.\n\ndef breakLoot(taskString, index=0):\n    parts = taskString.split(' ', maxsplit=1)\n    if index == 0:\n        return int(parts[index])\n    elif index == 1:\n        return parts[index]\n    else:\n        # This shouldn't be called.\n        return None\n\ntasks = []\n\nfor index in range(0,3):\n    tasksSubset = siteJobs[index][[\"Name\", \"Description\", \"Tasks\"]].copy()\n    tasksSubset = tasksSubset[ ~tasksSubset.Tasks.isna()]\n    tasksSubset = tasksSubset[ ~tasksSubset.Tasks.str.contains(\"Kill\")]\n\n    newRegex = r\"((\\d+\\s[\\w\\-]+\\s?[a-zA-Z]+))\"\n    tmp = tasksSubset.Tasks.str.extractall(newRegex)\n\n    count = tmp.reset_index()[0].apply(breakLoot).values\n    aLoot = tmp.reset_index()[0].apply(breakLoot, index=1).values\n\n    tmp = tmp.assign(\n        count = count,\n        loot = aLoot\n    )\n\n    nameDescriptSlice = tasksSubset.loc[tmp.reset_index()[\"level_0\"], ['Name', 'Description']]\n\n    tmp = tmp.assign(\n        name = nameDescriptSlice.Name.values,\n        description = nameDescriptSlice.Description.values\n    )\n\n    taskSlice = tmp.reset_index().drop([\n        'level_0',\n        'match',\n        0\n    ], axis =1 )\n\n    taskSlice = taskSlice[['name', 'count', 'loot', 'description']]\n    tasks.append(taskSlice)\ntasks = pd.concat([*tasks])"
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#fix-item-names",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#fix-item-names",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Fix Item Names",
    "text": "Fix Item Names\nNow we can continue and move to fixing the name mismatches like intended. There was quite a few of these so I’ll only show the process for a few of them - and then the fix for all of them. We’ll pull the code from the previous post so we can start by using the 0 values and work backwards.\n\nallJobs = pd.concat([korolevRewards, icaRewards, osirisRewards])\nlootKMarks = loot.query('Name == \"K-Marks\"')\ntasks = tasks.copy()\ntaskLoot = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='outer')\ntaskLoot['Cost'] = taskLoot['count'] * taskLoot['Unit']\nresults = allJobs.query('Rewards == \"K-Marks\"').merge(\n    taskLoot[['name', 'Cost']].groupby('name').sum(), left_on=\"Job\", right_on=\"name\", how='left')\nresults['Balance'] = results['Units'] - results['Cost']\n\n# How many are there?\nlen(results.query(\"Cost == 0\"))\n\n30\n\n\nThere are 31 rows with a Cost of 0 which we’ll need to investigate. The first one which I had noticed and corrected in the previous post was the CPU’s so we’ll address this one first. We’ll need to find the values in the tasks table and the lootKMarks table so we can figure out where the disconnect is.\n\ntasks.loc[tasks.loot.str.contains(\"Master|Zero\")]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      3\n      Mining Bot\n      2\n      Zero Systems\n      Our engineers have designed an autonomous mini...\n    \n    \n      6\n      Insufficient Processing Power\n      1\n      Master Unit\n      Prospector! The Zero Systems CPU you brought u...\n    \n    \n      12\n      Automated Security\n      5\n      Zero Systems\n      We will have to build new turrets to help prot...\n    \n    \n      16\n      Classified I\n      2\n      Master Unit\n      Prospector! We need Derelict Explosives, Maste...\n    \n    \n      47\n      Upgrades\n      1\n      Master Unit\n      We want to take over an ICA Data Center. Stash...\n    \n    \n      9\n      Sabotage\n      4\n      Zero Systems\n      Prospector! Those Korolev simpletons are drill...\n    \n    \n      34\n      Spare Parts\n      4\n      Zero Systems\n      One of the Servers at Starport has been failin...\n    \n    \n      40\n      NEW-Hard-ICA-Gather-6\n      1\n      Zero Systems\n      DESCRIPTION MISSING\n    \n    \n      11\n      Data Center Upgrades\n      2\n      Master Unit\n      Turns out our data center was not powerful eno...\n    \n  \n\n\n\n\n\nlootKMarks.loc[lootKMarks.Loot.str.contains(\"CPU\")]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Personal Quarters\n      Campaigns\n      Jobs\n      Printing\n      Loot\n    \n  \n  \n    \n      131\n      3845.0\n      K-Marks\n      Epic\n      Yes x3\n      Yes x3\n      Yes\n      No\n      Master Unit CPU\n    \n    \n      321\n      506.0\n      K-Marks\n      Rare\n      Yes x37\n      Yes x6\n      Yes\n      No\n      Zero Systems CPU\n    \n  \n\n\n\n\nThe tasks table has a shortened version of its name so the easiest way to fix this would be to append CPU to the loot name in the tasks table.\n\ntasks.loc[ tasks.loot == \"Master Unit\", 'loot'] = 'Master Unit CPU'\ntasks.loc[ tasks.loot == \"Master Unit CPU\", 'loot']\n\n6     Master Unit CPU\n16    Master Unit CPU\n47    Master Unit CPU\n11    Master Unit CPU\nName: loot, dtype: object\n\n\nAnother was the Pure Focus Crystals which has the same problem of having the name truncated; this is a trend among all the affected materials with higher tiered materials. Those will all be included in the final correction and will skip showing the process; it’s literally just the same and tedious.\n\n# Missing the Crystal part\ntasks.loc[tasks.loot.str.contains(\"Pure Focus\")]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      25\n      Geologist\n      1\n      Pure Focus\n      You got time for a job, Prospector? The sample...\n    \n    \n      26\n      Industry Secret\n      3\n      Pure Focus\n      Hm. Interesting. The Pure Focus Crystals you b...\n    \n    \n      28\n      Laser Rifles\n      3\n      Pure Focus\n      We are prototyping a new laser rifle that can ...\n    \n    \n      16\n      Arms Race\n      4\n      Pure Focus\n      Prospector. It seems like Korolev is working o...\n    \n  \n\n\n\n\n\nlootKMarks.loc[lootKMarks.Loot.str.contains(\"Crystal\")]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Personal Quarters\n      Campaigns\n      Jobs\n      Printing\n      Loot\n    \n  \n  \n    \n      56\n      961.0\n      K-Marks\n      Rare\n      Yes x19\n      Yes x10\n      Yes\n      No\n      Focus Crystal\n    \n    \n      61\n      2883.0\n      K-Marks\n      Epic\n      Yes x6\n      Yes x9\n      Yes\n      No\n      Pure Focus Crystal\n    \n    \n      66\n      38924.0\n      K-Marks\n      Legendary\n      Yes x6\n      Yes x9\n      Yes\n      No\n      Polirium Crystal\n    \n    \n      81\n      1709.0\n      K-Marks\n      Epic\n      Yes x36\n      Yes x9\n      Yes\n      No\n      Teratomorphic Crystal Core\n    \n  \n\n\n\n\nThis one was a surprise since truncating it doesn’t make any sense and gains nothing but the Magnetic Field Stabilizer is being truncated as well.\n\n# Missing the Stabilizer:\ntasks.loc[tasks.loot.str.contains(\"Magnetic\")]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      42\n      Stability is Key\n      1\n      Magnetic Field\n      Our Comms are jammed. We need you to stash a M...\n    \n    \n      43\n      Stability is Key\n      1\n      Magnetic Field\n      Our Comms are jammed. We need you to stash a M...\n    \n    \n      44\n      Stability is Key\n      1\n      Magnetic Field\n      Our Comms are jammed. We need you to stash a M...\n    \n    \n      28\n      Storm Interference\n      2\n      Magnetic Field\n      Prospector, the Storm has been distorting all ...\n    \n    \n      2\n      Surveillance Center\n      2\n      Magnetic Field\n      We want to expand our surveillance operations ...\n    \n  \n\n\n\n\n\nlootKMarks.loc[lootKMarks.Loot.str.contains(\"Magnetic\")]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Personal Quarters\n      Campaigns\n      Jobs\n      Printing\n      Loot\n    \n  \n  \n    \n      171\n      338.0\n      K-Marks\n      Uncommon\n      Yes x71\n      Yes x25\n      Yes\n      No\n      Magnetic Field Stabilizer\n    \n  \n\n\n\n\nLastly, we’ll show the NiC Oil which was a problem from the previous post which is missing the Cannister at the end again.\n\ntasks.loc[tasks.loot.str.contains(\"Oil\")]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      41\n      Striking Big\n      10\n      NiC Oil\n      Damn it! We ran out of Fuel for our Radiation ...\n    \n  \n\n\n\n\n\nlootKMarks.loc[lootKMarks.Loot.str.contains(\"Oil\")]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Personal Quarters\n      Campaigns\n      Jobs\n      Printing\n      Loot\n    \n  \n  \n    \n      121\n      20183.0\n      K-Marks\n      Epic\n      Yes x103\n      Yes x10\n      Yes\n      No\n      NiC Oil Cannister\n    \n  \n\n\n\n\nThere we go! Now, we’ll update the data again and run the merge once more.\n\n# Most corrections:\ntasks.loc[ tasks.loot == \"Master Unit\", 'loot'] = 'Master Unit CPU'\ntasks.loc[ tasks.loot == \"Zero Systems\", 'loot'] = 'Zero Systems CPU'\ntasks.loc[ tasks.loot == \"Pure Focus\", 'loot'] = 'Pure Focus Crystal'\ntasks.loc[ tasks.loot == \"Heavy Mining\", 'loot'] = 'Heavy Mining Tool'\ntasks.loc[ tasks.loot == \"Magnetic Field\", 'loot'] = 'Magnetic Field Stabilizer'\ntasks.loc[ tasks.loot == \"Brittle Titan\", 'loot'] = 'Brittle Titan Ore'\ntasks.loc[ tasks.loot == \"NiC Oil\", 'loot'] = 'NiC Oil Cannister'\ntasks.loc[ tasks.loot == \"Charged Spinal\", 'loot'] = 'Charged Spinal Base'\ntasks.loc[ tasks.loot == \"Hardened Bone\", 'loot'] = 'Hardened Bone Plates'\ntasks.loc[ tasks.loot == \"Pale Ivy\", 'loot'] = 'Pale Ivy Blossom'\ntasks.loc[ tasks.loot == \"Glowy Brightcap\", 'loot'] = 'Glowy Brightcap Mushroom'\ntasks.loc[ tasks.loot == \"Blue Runner\", 'loot'] = 'Blue Runner Egg'\ntasks.loc[ tasks.loot == \"Magic\", 'loot'] = 'Magic-GROW Fertilizer'\ntasks.loc[ tasks.loot == \"Letium\", 'loot'] = 'Letium Clot'\ntasks.loc[ tasks.loot == \"Azure Tree\", 'loot'] = 'Azure Tree Bark'\n\n\ntasks = tasks.copy()\ntaskLoot = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='outer')\ntaskLoot['Cost'] = taskLoot['count'] * taskLoot['Unit']\nresults = allJobs.query('Rewards == \"K-Marks\"').merge(\n    taskLoot[['name', 'Cost']].groupby('name').sum(), left_on=\"Job\", right_on=\"name\", how='left')\nresults['Balance'] = results['Units'] - results['Cost']\n\nlen(results.query(\"Cost == 0\"))\n\n13\n\n\nWe’ve made some good progress so far but we’ve still got quite a few jobs to update. Let’s check the list of missing valuse once more and see if there is a new pattern here.\n\ntmp = tasks.merge(results[['Job', 'Cost']], left_on=\"name\", right_on=\"Job\", how='left').query(\"Cost == 0\")\ntmp\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Job\n      Cost\n    \n  \n  \n    \n      40\n      Excavation Gear\n      1\n      Heavy Mining Tool\n      For excavations, we need you to stash a Heavy ...\n      Excavation Gear\n      0.0\n    \n    \n      46\n      And two smoking Barrels\n      1\n      PKR Maelstrom\n      Prospector. Get down there and stash a PKR Mae...\n      And two smoking Barrels\n      0.0\n    \n    \n      47\n      And two smoking Barrels\n      200\n      Shotgun Ammo\n      Prospector. Get down there and stash a PKR Mae...\n      And two smoking Barrels\n      0.0\n    \n    \n      74\n      Data Drive II\n      1\n      Rare Data\n      The Data you brought us was helpful, but we ne...\n      Data Drive II\n      0.0\n    \n    \n      75\n      Data Drive III\n      2\n      Rare Data\n      Good work last time, Prospector. The Data was ...\n      Data Drive III\n      0.0\n    \n    \n      76\n      Data Drive IV\n      1\n      Epic Data\n      In order to be able to predict Storm Behaviour...\n      Data Drive IV\n      0.0\n    \n    \n      77\n      Data Drive V\n      1\n      Legendary Data\n      Yes! The more precise Data you brought us was ...\n      Data Drive V\n      0.0\n    \n    \n      78\n      Data Drive VI\n      3\n      Legendary Data\n      We're finding more than just Storm data now......\n      Data Drive VI\n      0.0\n    \n    \n      82\n      Grenadier\n      1\n      Frag Grenade\n      Prospector. You have heard of Badum's Dead Dro...\n      Grenadier\n      0.0\n    \n    \n      86\n      Ammo Supplies\n      1000\n      Medium Ammo\n      Our Field Agents need more Ammo if they are to...\n      Ammo Supplies\n      0.0\n    \n    \n      87\n      Data Drop\n      2\n      Rare Data\n      Prospector. One of our Scientists is convinced...\n      Data Drop\n      0.0\n    \n    \n      90\n      Provide an Advocate\n      1\n      Advocate at\n      Our field agent requested better gear to take ...\n      Provide an Advocate\n      0.0\n    \n    \n      129\n      Loadout Drop\n      1\n      Rare Shield\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      130\n      Loadout Drop\n      1\n      Rare Helmet\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      131\n      Loadout Drop\n      1\n      Rare Backpack\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      138\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NEW-Hard-Osiris-EliteCrusher-1\n      0.0\n    \n  \n\n\n\n\nThe biggest standout problem here now is all those data drive quests so lets resolve those next."
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#data-drives",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#data-drives",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Data Drives:",
    "text": "Data Drives:\nWe have another problem though since none of the tables we have actually contains the data we’re after. So, we’re back to the wiki to do some more scraping work. If we also look at the names of the data drives we’re going to have another problem soon - which you’ll see when we download the data.\n\n# Missing the Data Drive\ntasks.loc[tasks.loot.str.contains(\"Data\")]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      19\n      Data Drive II\n      1\n      Rare Data\n      The Data you brought us was helpful, but we ne...\n    \n    \n      20\n      Data Drive III\n      2\n      Rare Data\n      Good work last time, Prospector. The Data was ...\n    \n    \n      21\n      Data Drive IV\n      1\n      Epic Data\n      In order to be able to predict Storm Behaviour...\n    \n    \n      22\n      Data Drive V\n      1\n      Legendary Data\n      Yes! The more precise Data you brought us was ...\n    \n    \n      23\n      Data Drive VI\n      3\n      Legendary Data\n      We're finding more than just Storm data now......\n    \n    \n      32\n      Data Drop\n      2\n      Rare Data\n      Prospector. One of our Scientists is convinced...\n    \n  \n\n\n\n\nLike the previous posts, scraping is a tedious process of matching keywords and pulling the right tables so that’s getting skipped; it’s just mostly trial and error.\n\nurlDataDrives = 'https://thecyclefrontier.wiki/wiki/Utilities#Data_Drives-0'\nsiteDrive = pd.read_html(urlDataDrives, attrs={\"class\":\"zebra\"})[2]\n\nWe’ll use a modified version of the code we wrote before for parsing the loot table for this.\n\ndriveSubset = siteDrive[\n    ['Image', 'Name', 'Rarity', 'Weight']\n    ].copy()\n\ndriveSubset = driveSubset.assign(\n    Loot = np.NaN\n)\n\nindex = range( 0, len(driveSubset) - 2, 3)\noffset = np.array([1, 2])\n\nfor i in index:\n    \n    aLoot = driveSubset.iloc[i, 1]\n    indexes = i + offset\n    driveSubset.iloc[indexes, 4] = aLoot\n\n\ntmp = driveSubset.iloc[:, 1:4]\ntmp = tmp.fillna(method=\"ffill\")\ndriveSubset.iloc[:, 1:4] = tmp\n\ncutNA = driveSubset.Loot.isna()\ndriveData = driveSubset[ ~cutNA ]\ndriveData = driveData.rename(columns={'Image':'Unit', 'Name':'Reward'})\ndriveData['Rarity'] = pd.Categorical(\n    driveData.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']\n)\ndriveData['Label'] = driveData['Rarity'].astype('str') + ' Data'\ndrives = driveData\n\ndrives\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Weight\n      Loot\n      Label\n    \n  \n  \n    \n      1\n      30.0\n      K-Marks\n      Common\n      15.0\n      Data Drive Tier 1\n      Common Data\n    \n    \n      2\n      0.0\n      Reputation\n      Common\n      15.0\n      Data Drive Tier 1\n      Common Data\n    \n    \n      4\n      1013.0\n      K-Marks\n      Uncommon\n      15.0\n      Data Drive Tier 2\n      Uncommon Data\n    \n    \n      5\n      1.0\n      Reputation\n      Uncommon\n      15.0\n      Data Drive Tier 2\n      Uncommon Data\n    \n    \n      7\n      2531.0\n      K-Marks\n      Rare\n      15.0\n      Data Drive Tier 3\n      Rare Data\n    \n    \n      8\n      3.0\n      Reputation\n      Rare\n      15.0\n      Data Drive Tier 3\n      Rare Data\n    \n    \n      10\n      6075.0\n      K-Marks\n      Epic\n      15.0\n      Data Drive Tier 4\n      Epic Data\n    \n    \n      11\n      6.0\n      Reputation\n      Epic\n      15.0\n      Data Drive Tier 4\n      Epic Data\n    \n    \n      13\n      10252.0\n      K-Marks\n      Legendary\n      15.0\n      Data Drive Tier 5\n      Legendary Data\n    \n    \n      14\n      10.0\n      Reputation\n      Legendary\n      15.0\n      Data Drive Tier 5\n      Legendary Data\n    \n  \n\n\n\n\nSince this is the second time that we’ve needed this - and we’re going to need this again - we should write a function to wrap this whole process.\n\n# this is the function, where:\n## siteData: the table from the scraped site\n## columns: the columns you want to keep from the scraped data\n## adjust: the count from the bottom containing an index\n## step: how many rows between values we care about\n## offset: how many rewards are there?\ndef extractSite(siteData, columns, adjust, step,  offset):\n    if not isinstance(columns, list):\n        print(\"Columns argument must be a list.\")\n        return None\n    siteSubset = siteData[columns].copy()\n\n    siteSubset = siteSubset.assign(\n        Loot = np.NaN\n    )\n\n    # Some extra error handling \n    if not isinstance(adjust, int):\n        print(\"adjust argument must be an int.\")\n        return None\n    if not isinstance(step, int):\n        print(\"step argument must be an int.\")\n        return None\n    if not isinstance(offset, list):\n        print(\"offset argument must be a list.\")\n        return None\n    \n    index = range( 0, len(siteSubset) - adjust, step)\n    offset = np.array(offset)\n\n    for i in index:\n        aLoot = siteSubset.iloc[i, 1]\n        indexes = i + offset\n        siteSubset.iloc[indexes, len(siteSubset.columns)-1] = aLoot\n\n    tmp = siteSubset.iloc[:, 1:len(siteSubset.columns)]\n    tmp = tmp.fillna(method=\"ffill\")\n    siteSubset.iloc[:, 1:len(siteSubset.columns)-1] = tmp\n\n    cutNA = siteSubset.Loot.isna()\n    returnData = siteSubset[ ~cutNA ]\n    returnData = returnData.rename(columns={'Image':'Unit', 'Name':'Reward'})\n\n    return returnData\n\nNow we’ll sanity check this to make sure it works.\n\ndrives = extractSite(siteDrive, ['Image', 'Name', 'Rarity', 'Weight'], 2, 3, [1, 2])\ndrives['Rarity'] = pd.Categorical(\n        drives.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']\n    )\ndrives\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Weight\n      Loot\n    \n  \n  \n    \n      1\n      30.0\n      K-Marks\n      Common\n      15.0\n      Data Drive Tier 1\n    \n    \n      2\n      0.0\n      Reputation\n      Common\n      15.0\n      Data Drive Tier 1\n    \n    \n      4\n      1013.0\n      K-Marks\n      Uncommon\n      15.0\n      Data Drive Tier 2\n    \n    \n      5\n      1.0\n      Reputation\n      Uncommon\n      15.0\n      Data Drive Tier 2\n    \n    \n      7\n      2531.0\n      K-Marks\n      Rare\n      15.0\n      Data Drive Tier 3\n    \n    \n      8\n      3.0\n      Reputation\n      Rare\n      15.0\n      Data Drive Tier 3\n    \n    \n      10\n      6075.0\n      K-Marks\n      Epic\n      15.0\n      Data Drive Tier 4\n    \n    \n      11\n      6.0\n      Reputation\n      Epic\n      15.0\n      Data Drive Tier 4\n    \n    \n      13\n      10252.0\n      K-Marks\n      Legendary\n      15.0\n      Data Drive Tier 5\n    \n    \n      14\n      10.0\n      Reputation\n      Legendary\n      15.0\n      Data Drive Tier 5\n    \n  \n\n\n\n\nPerfect! Now we just add this to our list of adjustments right? Sadly no. If you look at the names of the drives you’ll find that we’re not quite there. The names of the drives were renamed in Season 2 but the values in our tasks were not updated from their previous values. This is not too hard to update since the old drive names were just the Rarity + Data and we have that so we’ll just need a new column.\n\ndrives['Loot'] = drives['Rarity'].astype('str') + ' Data'\ndrives\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Weight\n      Loot\n    \n  \n  \n    \n      1\n      30.0\n      K-Marks\n      Common\n      15.0\n      Common Data\n    \n    \n      2\n      0.0\n      Reputation\n      Common\n      15.0\n      Common Data\n    \n    \n      4\n      1013.0\n      K-Marks\n      Uncommon\n      15.0\n      Uncommon Data\n    \n    \n      5\n      1.0\n      Reputation\n      Uncommon\n      15.0\n      Uncommon Data\n    \n    \n      7\n      2531.0\n      K-Marks\n      Rare\n      15.0\n      Rare Data\n    \n    \n      8\n      3.0\n      Reputation\n      Rare\n      15.0\n      Rare Data\n    \n    \n      10\n      6075.0\n      K-Marks\n      Epic\n      15.0\n      Epic Data\n    \n    \n      11\n      6.0\n      Reputation\n      Epic\n      15.0\n      Epic Data\n    \n    \n      13\n      10252.0\n      K-Marks\n      Legendary\n      15.0\n      Legendary Data\n    \n    \n      14\n      10.0\n      Reputation\n      Legendary\n      15.0\n      Legendary Data\n    \n  \n\n\n\n\n\n# drives\npd.concat(\n    [lootKMarks.rename({\"Name\":\"Reward\"}, axis=1)[['Unit', 'Reward', 'Rarity', 'Loot']],\n    drives.query('Reward == \"K-Marks\"').drop(\"Weight\", axis=1)])\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      1\n      150.0\n      K-Marks\n      Common\n      Flawed Veltecite\n    \n    \n      6\n      570.0\n      K-Marks\n      Uncommon\n      Cloudy Veltecite\n    \n    \n      11\n      854.0\n      K-Marks\n      Rare\n      Clear Veltecite\n    \n    \n      16\n      1922.0\n      K-Marks\n      Epic\n      Pure Veltecite\n    \n    \n      21\n      6487.0\n      K-Marks\n      Legendary\n      Veltecite Heart\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1\n      30.0\n      K-Marks\n      Common\n      Common Data\n    \n    \n      4\n      1013.0\n      K-Marks\n      Uncommon\n      Uncommon Data\n    \n    \n      7\n      2531.0\n      K-Marks\n      Rare\n      Rare Data\n    \n    \n      10\n      6075.0\n      K-Marks\n      Epic\n      Epic Data\n    \n    \n      13\n      10252.0\n      K-Marks\n      Legendary\n      Legendary Data\n    \n  \n\n99 rows × 4 columns\n\n\n\nAdd that to our pipeline and let’s see where we are now\n\nallJobs = pd.concat([korolevRewards, icaRewards, osirisRewards])\nlootKMarks = loot.query('Name == \"K-Marks\"')\n\n# Most corrections:\ntasks.loc[ tasks.loot == \"Master Unit\", 'loot'] = 'Master Unit CPU'\ntasks.loc[ tasks.loot == \"Zero Systems\", 'loot'] = 'Zero Systems CPU'\ntasks.loc[ tasks.loot == \"Pure Focus\", 'loot'] = 'Pure Focus Crystal'\ntasks.loc[ tasks.loot == \"Heavy Mining\", 'loot'] = 'Heavy Mining Tool'\ntasks.loc[ tasks.loot == \"Magnetic Field\", 'loot'] = 'Magnetic Field Stabilizer'\ntasks.loc[ tasks.loot == \"Brittle Titan\", 'loot'] = 'Brittle Titan Ore'\ntasks.loc[ tasks.loot == \"NiC Oil\", 'loot'] = 'NiC Oil Cannister'\ntasks.loc[ tasks.loot == \"Charged Spinal\", 'loot'] = 'Charged Spinal Base'\ntasks.loc[ tasks.loot == \"Hardened Bone\", 'loot'] = 'Hardened Bone Plates'\ntasks.loc[ tasks.loot == \"Pale Ivy\", 'loot'] = 'Pale Ivy Blossom'\ntasks.loc[ tasks.loot == \"Glowy Brightcap\", 'loot'] = 'Glowy Brightcap Mushroom'\ntasks.loc[ tasks.loot == \"Blue Runner\", 'loot'] = 'Blue Runner Egg'\ntasks.loc[ tasks.loot == \"Magic\", 'loot'] = 'Magic-GROW Fertilizer'\ntasks.loc[ tasks.loot == \"Letium\", 'loot'] = 'Letium Clot'\ntasks.loc[ tasks.loot == \"Azure Tree\", 'loot'] = 'Azure Tree Bark'\ntasks = tasks.copy()\n\n# Extract Stuff here for now:\ndrives = extractSite(siteDrive, ['Image', 'Name', 'Rarity', 'Weight'], 2, 3, [1, 2])\ndrives['Rarity'] = pd.Categorical(\n        drives.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']\n)\ndrives['Loot'] = drives['Rarity'].astype('str') + ' Data'\n\nlootKMarks = pd.concat(\n    [lootKMarks.rename({\"Name\":\"Reward\"}, axis=1)[['Unit', 'Reward', 'Rarity', 'Loot']],\n    drives.query('Reward == \"K-Marks\"').drop(\"Weight\", axis=1)])\n\ntaskLoot = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='outer')\ntaskLoot['Cost'] = taskLoot['count'] * taskLoot['Unit']\nresults = allJobs.query('Rewards == \"K-Marks\"').merge(\n    taskLoot[['name', 'Cost']].groupby('name').sum(), left_on=\"Job\", right_on=\"name\", how='left')\nresults['Balance'] = results['Units'] - results['Cost']\n\nlen(results.query(\"Cost == 0\"))\n\n7\n\n\nThat’s much better! So, what’s left to do?\n\ntmp = tasks.merge(results[['Job', 'Cost']], left_on=\"name\", right_on=\"Job\", how='left').query(\"Cost == 0\")\ntmp\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Job\n      Cost\n    \n  \n  \n    \n      40\n      Excavation Gear\n      1\n      Heavy Mining Tool\n      For excavations, we need you to stash a Heavy ...\n      Excavation Gear\n      0.0\n    \n    \n      46\n      And two smoking Barrels\n      1\n      PKR Maelstrom\n      Prospector. Get down there and stash a PKR Mae...\n      And two smoking Barrels\n      0.0\n    \n    \n      47\n      And two smoking Barrels\n      200\n      Shotgun Ammo\n      Prospector. Get down there and stash a PKR Mae...\n      And two smoking Barrels\n      0.0\n    \n    \n      82\n      Grenadier\n      1\n      Frag Grenade\n      Prospector. You have heard of Badum's Dead Dro...\n      Grenadier\n      0.0\n    \n    \n      86\n      Ammo Supplies\n      1000\n      Medium Ammo\n      Our Field Agents need more Ammo if they are to...\n      Ammo Supplies\n      0.0\n    \n    \n      90\n      Provide an Advocate\n      1\n      Advocate at\n      Our field agent requested better gear to take ...\n      Provide an Advocate\n      0.0\n    \n    \n      129\n      Loadout Drop\n      1\n      Rare Shield\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      130\n      Loadout Drop\n      1\n      Rare Helmet\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      131\n      Loadout Drop\n      1\n      Rare Backpack\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      138\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NEW-Hard-Osiris-EliteCrusher-1\n      0.0\n    \n  \n\n\n\n\n\nAdd the Guns\nWe’re going to move to getting the gun data included since we already actualy have it. This was part of another post which was done - not included in the series.\n\nurl = \"https://thecyclefrontier.wiki/wiki/Weapons\"\nsiteGun = pd.read_html(url, attrs={\"class\":\"zebra\"})[0]\n\ngunData = siteGun[~siteGun.Type.isna()]\nindx = gunData['Proj. Speed'] == 'Hitscan'\ngunData.loc[indx, 'Proj. Speed'] = np.NaN\n\ngunData = gunData.assign(\n    Unit = gunData['Sell Value'].str.replace(' K-Marks', '').astype('float'),\n    Reward = \"K-Marks\",\n    Loot = gunData['Name']\n)\n\n# # This removes the legendary weapons\n# data = data.query('Faction != \"Printing\"')\n\nguns = gunData\nguns[['Unit', 'Reward', 'Rarity', 'Loot']].head(15)\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      0\n      17429.0\n      K-Marks\n      Epic\n      Advocate\n    \n    \n      3\n      524.0\n      K-Marks\n      Common\n      AR-55 Autorifle\n    \n    \n      6\n      12341.0\n      K-Marks\n      Epic\n      Asp Flechette Gun\n    \n    \n      9\n      371.0\n      K-Marks\n      Common\n      B9 Trenchgun\n    \n    \n      12\n      63080.0\n      K-Marks\n      Exotic\n      Basilisk\n    \n    \n      15\n      1918.0\n      K-Marks\n      Uncommon\n      Bulldog\n    \n    \n      18\n      2052.0\n      K-Marks\n      Common\n      C-32 Bolt Action\n    \n    \n      21\n      17429.0\n      K-Marks\n      Epic\n      Gorgon\n    \n    \n      24\n      16805.0\n      K-Marks\n      Exotic\n      Hammer\n    \n    \n      27\n      7143.0\n      K-Marks\n      Rare\n      ICA Guarantee\n    \n    \n      30\n      228.0\n      K-Marks\n      Common\n      K-28\n    \n    \n      33\n      325513.0\n      K-Marks\n      Legendary\n      KARMA-1\n    \n    \n      36\n      22781.0\n      K-Marks\n      Epic\n      KBR Longshot\n    \n    \n      39\n      94459.0\n      K-Marks\n      Exotic\n      Kinetic Arbiter\n    \n    \n      42\n      1918.0\n      K-Marks\n      Uncommon\n      KM-9 'Scrapper'\n    \n  \n\n\n\n\nNow we have the gun data to be added to the loot table but if you were paying attention you may have noticed something is wrong with one of our values.\n\n# Not sure what this is from.\ntasks.loc[tasks.loot.str.contains(' at')]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      35\n      Provide an Advocate\n      1\n      Advocate at\n      Our field agent requested better gear to take ...\n    \n  \n\n\n\n\nThe Advocate in this row is labeled as Advocate at which is another problem. In this instance, instead of trying to deal with the regex we’re just going to update that single value.\n\ntasks.loc[tasks.loot.str.contains('Advocate at'), 'loot'] = 'Advocate'\ntasks.loc[tasks.loot.str.contains('Advocate')]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      35\n      Provide an Advocate\n      1\n      Advocate\n      Our field agent requested better gear to take ...\n    \n  \n\n\n\n\nAnd, make sure that the append works as intended:\n\npd.concat(\n    [lootKMarks.rename({\"Name\":\"Reward\"}, axis=1)[['Unit', 'Reward', 'Rarity', 'Loot']],\n    drives.query('Reward == \"K-Marks\"').drop(\"Weight\", axis=1),\n    guns[['Unit', 'Reward', 'Rarity', 'Loot']]]\n)\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      1\n      150.0\n      K-Marks\n      Common\n      Flawed Veltecite\n    \n    \n      6\n      570.0\n      K-Marks\n      Uncommon\n      Cloudy Veltecite\n    \n    \n      11\n      854.0\n      K-Marks\n      Rare\n      Clear Veltecite\n    \n    \n      16\n      1922.0\n      K-Marks\n      Epic\n      Pure Veltecite\n    \n    \n      21\n      6487.0\n      K-Marks\n      Legendary\n      Veltecite Heart\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      63\n      371.0\n      K-Marks\n      Common\n      S-576 PDW\n    \n    \n      66\n      1179.0\n      K-Marks\n      Uncommon\n      Scarab\n    \n    \n      69\n      12341.0\n      K-Marks\n      Epic\n      Shattergun\n    \n    \n      72\n      34172.0\n      K-Marks\n      Exotic\n      Voltaic Brute\n    \n    \n      75\n      270540.0\n      K-Marks\n      Legendary\n      Zeus Beam\n    \n  \n\n125 rows × 4 columns"
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-backpacks",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-backpacks",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Deal With Backpacks",
    "text": "Deal With Backpacks\nWe’re going to try to fix the Backpacks, the Shields and the Helmets toegther since they’re all on the same page of the wiki. Again, this data was not part of the previous tables that we had so we’ll need to go get it.\n\ngearUrl = 'https://thecyclefrontier.wiki/wiki/Gear'\nsite = pd.read_html(gearUrl)\n\nThis was tricky to collect - which is only why I’m pointing out how it was done. While working out how to collect the data from the website, there was no good strategy to collect just the data that I wanted. What you will see is that I use hard coded values to pull out where each table is.\n\nsiteBackPacks = site[0]\nsiteHelmet = site[10]\nsiteShield = site[22]\n\nWhat I did was enumerate throught all the tables collected to find which results had outlier sizes. I then pulled those to make sure they were what I was after like this:\n\n# 10\n# 22\nlist(enumerate(map(len, site)))\n\n[(0, 22),\n (1, 1),\n (2, 1),\n (3, 3),\n (4, 1),\n (5, 4),\n (6, 1),\n (7, 4),\n (8, 1),\n (9, 1),\n (10, 46),\n (11, 1),\n (12, 1),\n (13, 3),\n (14, 4),\n (15, 4),\n (16, 3),\n (17, 4),\n (18, 4),\n (19, 3),\n (20, 4),\n (21, 4),\n (22, 43),\n (23, 1),\n (24, 1),\n (25, 3),\n (26, 4),\n (27, 4),\n (28, 4),\n (29, 4),\n (30, 4),\n (31, 4),\n (32, 4)]\n\n\nIt is not pretty but web scraping rarely is. And, it works. Next we’ll start with the backup packs; with some tweaking of the values to the extractSite function which was defined earlier, this becomes really easy.\n\nbackpacks = extractSite(\n    siteData = siteBackPacks.loc[siteBackPacks.Name.str.contains(\"Backpack|K-Marks\")],\n    columns = ['Image', 'Name', 'Rarity', 'Space', 'Sale Price'],\n    adjust = 2, \n    step = 3,\n    offset = [1, 2])\n\nbackpacks\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Space\n      Sale Price\n      Loot\n    \n  \n  \n    \n      1\n      600.0\n      K-Marks\n      Common\n      200.0\n      180  K-Marks\n      Small Backpack\n    \n    \n      2\n      180.0\n      K-Marks\n      Common\n      200.0\n      180  K-Marks\n      Small Backpack\n    \n    \n      4\n      2700.0\n      K-Marks\n      Uncommon\n      250.0\n      810  K-Marks\n      Medium Backpack\n    \n    \n      7\n      810.0\n      K-Marks\n      Uncommon\n      250.0\n      810  K-Marks\n      Medium Backpack\n    \n    \n      9\n      6100.0\n      K-Marks\n      Rare\n      300.0\n      1,830  K-Marks\n      Large Backpack\n    \n    \n      13\n      1830.0\n      K-Marks\n      Rare\n      300.0\n      1,830  K-Marks\n      Large Backpack\n    \n    \n      15\n      12000.0\n      K-Marks\n      Epic\n      350.0\n      4,000  K-Marks\n      Heavy Duty Backpack\n    \n    \n      19\n      4000.0\n      K-Marks\n      Epic\n      350.0\n      4,000  K-Marks\n      Heavy Duty Backpack\n    \n  \n\n\n\n\nLooking at these results though, what we’re really after is the Sale Price and so we’ll need to move some stuff around.\n\nbackpacks = backpacks.assign(\n    Unit = backpacks['Sale Price'].str.replace(' K-Marks', '').str.replace(',', '').astype('float')\n)\n\nbackpacks = backpacks.drop([\"Sale Price\", \"Space\"], axis=1)\nbackpacks\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      1\n      180.0\n      K-Marks\n      Common\n      Small Backpack\n    \n    \n      2\n      180.0\n      K-Marks\n      Common\n      Small Backpack\n    \n    \n      4\n      810.0\n      K-Marks\n      Uncommon\n      Medium Backpack\n    \n    \n      7\n      810.0\n      K-Marks\n      Uncommon\n      Medium Backpack\n    \n    \n      9\n      1830.0\n      K-Marks\n      Rare\n      Large Backpack\n    \n    \n      13\n      1830.0\n      K-Marks\n      Rare\n      Large Backpack\n    \n    \n      15\n      4000.0\n      K-Marks\n      Epic\n      Heavy Duty Backpack\n    \n    \n      19\n      4000.0\n      K-Marks\n      Epic\n      Heavy Duty Backpack\n    \n  \n\n\n\n\nWe have the same problem which we had for the Data Drives: the the old name Rare Backpack is in the tasks table but is not how they are named here. Again, we’re just going to steal and modify the solution we had before and apply it here.\n\nbackpacks['Loot'] = backpacks['Rarity'].astype('str') + ' Backpack'\n\nTime to add them to the pipeline.\n\npd.concat([\n    lootKMarks.rename({\"Name\":\"Reward\"}, axis=1)[['Unit', 'Reward', 'Rarity', 'Loot']],\n    drives.query('Reward == \"K-Marks\"').drop(\"Weight\", axis=1),\n    guns[['Unit', 'Reward', 'Rarity', 'Loot']],\n    backpacks\n])\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      1\n      150.0\n      K-Marks\n      Common\n      Flawed Veltecite\n    \n    \n      6\n      570.0\n      K-Marks\n      Uncommon\n      Cloudy Veltecite\n    \n    \n      11\n      854.0\n      K-Marks\n      Rare\n      Clear Veltecite\n    \n    \n      16\n      1922.0\n      K-Marks\n      Epic\n      Pure Veltecite\n    \n    \n      21\n      6487.0\n      K-Marks\n      Legendary\n      Veltecite Heart\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      7\n      810.0\n      K-Marks\n      Uncommon\n      Uncommon Backpack\n    \n    \n      9\n      1830.0\n      K-Marks\n      Rare\n      Rare Backpack\n    \n    \n      13\n      1830.0\n      K-Marks\n      Rare\n      Rare Backpack\n    \n    \n      15\n      4000.0\n      K-Marks\n      Epic\n      Epic Backpack\n    \n    \n      19\n      4000.0\n      K-Marks\n      Epic\n      Epic Backpack\n    \n  \n\n133 rows × 4 columns\n\n\n\nThis is much better. So, how many are missing now?\n\nlen(results.query(\"Cost == 0\"))\n\n4\n\n\nMuch better. But, there is actually a new problem here: masking missing values. If we look back at our data from before we’ll see that there was Helment and Shield information missing and now it’s gone! Since we’ve fixed some of the values for those jobs the Balance is no longer 0 and thefore we’ve lost track of it! Let’s step back and look at the merged result and look for mistakes. And, upon doing this we run into our first masked problem.\n\ntmp = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='outer')\ntmp\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Loot\n      Unit\n    \n  \n  \n    \n      0\n      New Mining Tools\n      2.0\n      Hydraulic Piston\n      We are producing new Mining Tools for new Pros...\n      Hydraulic Piston\n      338.0\n    \n    \n      1\n      Excavator Improvements\n      3.0\n      Hydraulic Piston\n      The suspension on our mining excavators need i...\n      Hydraulic Piston\n      338.0\n    \n    \n      2\n      New Mining Tools\n      10.0\n      Hardened Metals\n      We are producing new Mining Tools for new Pros...\n      Hardened Metals\n      150.0\n    \n    \n      3\n      Automated Security\n      16.0\n      Hardened Metals\n      We will have to build new turrets to help prot...\n      Hardened Metals\n      150.0\n    \n    \n      4\n      Air Lock Upgrades\n      12.0\n      Hardened Metals\n      Our engineers designed a safer airlock system ...\n      Hardened Metals\n      150.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      211\n      NaN\n      NaN\n      NaN\n      NaN\n      Common Backpack\n      180.0\n    \n    \n      212\n      NaN\n      NaN\n      NaN\n      NaN\n      Uncommon Backpack\n      810.0\n    \n    \n      213\n      NaN\n      NaN\n      NaN\n      NaN\n      Uncommon Backpack\n      810.0\n    \n    \n      214\n      NaN\n      NaN\n      NaN\n      NaN\n      Epic Backpack\n      4000.0\n    \n    \n      215\n      NaN\n      NaN\n      NaN\n      NaN\n      Epic Backpack\n      4000.0\n    \n  \n\n216 rows × 6 columns\n\n\n\nThere are all these NaN values at the bottom which were attached and included due to the outer including all the rows. Can we simply change this over to a left?\n\ntmp = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='left')\ntmp.loc[tmp.Loot.isna()]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Loot\n      Unit\n    \n  \n  \n    \n      39\n      Excavation Gear\n      1\n      Heavy Mining Tool\n      For excavations, we need you to stash a Heavy ...\n      NaN\n      NaN\n    \n    \n      46\n      And two smoking Barrels\n      200\n      Shotgun Ammo\n      Prospector. Get down there and stash a PKR Mae...\n      NaN\n      NaN\n    \n    \n      79\n      Grenadier\n      1\n      Frag Grenade\n      Prospector. You have heard of Badum's Dead Dro...\n      NaN\n      NaN\n    \n    \n      83\n      Ammo Supplies\n      1000\n      Medium Ammo\n      Our Field Agents need more Ammo if they are to...\n      NaN\n      NaN\n    \n    \n      126\n      Loadout Drop\n      1\n      Rare Shield\n      One of our more... lethal assets on Fortuna re...\n      NaN\n      NaN\n    \n    \n      127\n      Loadout Drop\n      1\n      Rare Helmet\n      One of our more... lethal assets on Fortuna re...\n      NaN\n      NaN\n    \n    \n      136\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NaN\n      NaN\n    \n  \n\n\n\n\nThis is more in line what I’d have expected. We’ll temporarily keep this - but mostly for discussion purposes. The problem here is that when you look through the wiki there are no sell values for the Shields and Helmets. We cannot get these from the wiki which means we cannot automate it. We could do this manually but then I’d have to constantly update this value when it changes - which I’m trying to avoid. This looks to be a case where we’re going to need to remove this job for now.\n\n# Adding this to the pipeline:\nidx = tmp.name.str.contains(\"Loadout Drop\")\ntmp = tmp.loc[-idx]\ntmp\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Loot\n      Unit\n    \n  \n  \n    \n      0\n      New Mining Tools\n      2\n      Hydraulic Piston\n      We are producing new Mining Tools for new Pros...\n      Hydraulic Piston\n      338.0\n    \n    \n      1\n      New Mining Tools\n      10\n      Hardened Metals\n      We are producing new Mining Tools for new Pros...\n      Hardened Metals\n      150.0\n    \n    \n      2\n      Explosive Excavation\n      4\n      Derelict Explosives\n      One of our mines collapsed with valuable equip...\n      Derelict Explosives\n      1709.0\n    \n    \n      3\n      Mining Bot\n      2\n      Zero Systems CPU\n      Our engineers have designed an autonomous mini...\n      Zero Systems CPU\n      506.0\n    \n    \n      4\n      Mining Bot\n      3\n      Ball Bearings\n      Our engineers have designed an autonomous mini...\n      Ball Bearings\n      338.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      136\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NaN\n      NaN\n    \n    \n      137\n      Flexible Sealant\n      8\n      Resin Gun\n      A number of our weather balloons took damage f...\n      Resin Gun\n      759.0\n    \n    \n      138\n      Indigenous Fruit\n      4\n      Indigenous Fruit\n      Ah, Prospector, have you come across any Indig...\n      Indigenous Fruit\n      759.0\n    \n    \n      139\n      Indigenous Fruit\n      4\n      Biological Sampler\n      Ah, Prospector, have you come across any Indig...\n      Biological Sampler\n      1139.0\n    \n    \n      140\n      Don't get crushed\n      3\n      Crusher Hide\n      Gear up, Prospector! We need Crusher Skins for...\n      Crusher Hide\n      11533.0\n    \n  \n\n137 rows × 6 columns\n\n\n\nOk, so it looks like next will be solving the ammo listings.\n\ntmp.loc[tmp.Loot.isna()]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Loot\n      Unit\n    \n  \n  \n    \n      39\n      Excavation Gear\n      1\n      Heavy Mining Tool\n      For excavations, we need you to stash a Heavy ...\n      NaN\n      NaN\n    \n    \n      46\n      And two smoking Barrels\n      200\n      Shotgun Ammo\n      Prospector. Get down there and stash a PKR Mae...\n      NaN\n      NaN\n    \n    \n      79\n      Grenadier\n      1\n      Frag Grenade\n      Prospector. You have heard of Badum's Dead Dro...\n      NaN\n      NaN\n    \n    \n      83\n      Ammo Supplies\n      1000\n      Medium Ammo\n      Our Field Agents need more Ammo if they are to...\n      NaN\n      NaN\n    \n    \n      136\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NaN\n      NaN"
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-ammo",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-ammo",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Deal with Ammo",
    "text": "Deal with Ammo\n\nammoUrl = \"https://thecyclefrontier.wiki/wiki/Ammo\"\nammo = pd.read_html(ammoUrl)[0]\n\nWe’ve already done this before so this is simply here to show it was done. And, you add it to the pipeline just the same.\n\nammo = ammo.rename(\n    {\"Item Name\":\"Loot\", \"Sell Value\":\"Unit\"}, axis=1\n    ).assign(\n        Reward = \"K-Marks\",\n        Rarity = pd.Categorical(\n            ammo.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary'])\n    )[['Unit', 'Reward', 'Rarity', 'Loot']]"
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-the-heavy-mining-tool",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-the-heavy-mining-tool",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Deal with The Heavy Mining Tool",
    "text": "Deal with The Heavy Mining Tool\nSadly, this tool is on its own page so we’ll need something custom again. There is a table here we can pull but it’s oriented incorrectly for our use.\n\nsite = pd.read_html(\"https://thecyclefrontier.wiki/wiki/Heavy_Mining_Tool\")\nminerData = site[0]\nminerData\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      Description\n      Allows faster mining of materials.\n    \n    \n      1\n      Rarity\n      Common\n    \n    \n      2\n      Weight\n      30\n    \n    \n      3\n      Buy Value\n      600\n    \n    \n      4\n      Sell Value\n      180\n    \n    \n      5\n      Faction Points\n      2\n    \n  \n\n\n\n\nThankfully, a data frame has the Transpose function that a matrix does. A simple explanation of this function is that it swaps the rows to columns and columns to rows.\n\nminerData[0].T\n\n0       Description\n1            Rarity\n2            Weight\n3         Buy Value\n4        Sell Value\n5    Faction Points\nName: 0, dtype: object\n\n\nWe’re going to extract the rows values to create a new dataframe object. Since the Heaving Mining Tool designation is missing from the data then we’ll need to add it ourself.\n\nrow = minerData[1].T.to_list() + ['Heavy Mining Tool']\nrow\n\n['Allows faster mining of materials.',\n 'Common',\n '30',\n '600',\n '180',\n '2',\n 'Heavy Mining Tool']\n\n\n\ncolumns = minerData[0].to_list() + ['Loot']\ncolumns\n\n['Description',\n 'Rarity',\n 'Weight',\n 'Buy Value',\n 'Sell Value',\n 'Faction Points',\n 'Loot']\n\n\nAnd, join them together.\n\nmineTool = pd.DataFrame(columns = columns)\nmineTool.loc[0] = row\nmineTool = mineTool.assign(\n    Reward = \"K-Marks\",\n    Rarity = pd.Categorical(\n        mineTool.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']\n    ),\n    Unit = mineTool['Sell Value'].astype(int),\n)[['Unit', 'Reward', 'Rarity', 'Loot']]\nmineTool\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      0\n      180\n      K-Marks\n      Common\n      Heavy Mining Tool\n    \n  \n\n\n\n\nAnd, add it to the pipeline.\n\nlootKMarks = pd.concat([\n    lootKMarks.rename({\"Name\":\"Reward\"}, axis=1)[['Unit', 'Reward', 'Rarity', 'Loot']],\n    drives.query('Reward == \"K-Marks\"').drop(\"Weight\", axis=1),\n    guns[['Unit', 'Reward', 'Rarity', 'Loot']],\n    backpacks,\n    ammo,\n    mineTool\n])\n\n# Drop this quest\nidx = tasks.name.str.contains(\"Loadout Drop\")\ntasks = tasks.loc[-idx]\n\ntaskLoot = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='outer')\ntaskLoot['Cost'] = taskLoot['count'] * taskLoot['Unit']\nresults = allJobs.query('Rewards == \"K-Marks\"').merge(\n    taskLoot[['name', 'Cost']].groupby('name').sum(), left_on=\"Job\", right_on=\"name\", how='left')\nresults['Balance'] = results['Units'] - results['Cost']\n\n\nresults.query(\"Cost == 0\")\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Cost\n      Balance\n    \n  \n  \n    \n      77\n      2400\n      K-Marks\n      Grenadier\n      0.0\n      2400.0\n    \n    \n      138\n      155000\n      K-Marks\n      NEW-Hard-Osiris-EliteCrusher-1\n      0.0\n      155000.0\n    \n  \n\n\n\n\n\ntmp = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='left')\ntmp.loc[tmp.Loot.isna()]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Loot\n      Unit\n    \n  \n  \n    \n      93\n      Grenadier\n      1\n      Frag Grenade\n      Prospector. You have heard of Badum's Dead Dro...\n      NaN\n      NaN\n    \n    \n      151\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NaN\n      NaN"
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#conclusions",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#conclusions",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Conclusions",
    "text": "Conclusions\nThis is the best we’ll get I suppose. Now, back to our check: How bad is our good old job And two smoking Barrels?\n\nresults.loc[results.Job.str.contains(\"Barrel\")]\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Cost\n      Balance\n    \n  \n  \n    \n      39\n      19000\n      K-Marks\n      And two smoking Barrels\n      40716.0\n      -21716.0\n    \n  \n\n\n\n\nAnd, there we go! So, how many jobs have a negative balance?\n\nresults.query(\"Balance < 0 \")\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Cost\n      Balance\n    \n  \n  \n    \n      39\n      19000\n      K-Marks\n      And two smoking Barrels\n      40716.0\n      -21716.0\n    \n    \n      70\n      9500\n      K-Marks\n      Data Drive III\n      10124.0\n      -624.0\n    \n    \n      73\n      56000\n      K-Marks\n      Data Drive VI\n      61512.0\n      -5512.0\n    \n    \n      81\n      17000\n      K-Marks\n      Ammo Supplies\n      69000.0\n      -52000.0\n    \n  \n\n\n\n\nThat’s not as bad as I expected. What is the average job balance?\n\nresults.Balance.mean(), results.Balance.median()\n\n(13300.702127659575, 11256.0)\n\n\nSo, what jobs are above the mean?\n\nresults.query(f\"Balance >= {results.Balance.mean()}\")\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Cost\n      Balance\n    \n  \n  \n    \n      5\n      25000\n      K-Marks\n      Excavator Improvements\n      4306.0\n      20694.0\n    \n    \n      6\n      37000\n      K-Marks\n      A new type of Alloy\n      15468.0\n      21532.0\n    \n    \n      7\n      27000\n      K-Marks\n      Automated Security\n      4930.0\n      22070.0\n    \n    \n      9\n      52000\n      K-Marks\n      Classified I\n      27030.0\n      24970.0\n    \n    \n      15\n      27000\n      K-Marks\n      Geologist\n      6727.0\n      20273.0\n    \n    \n      16\n      29000\n      K-Marks\n      Industry Secret\n      8649.0\n      20351.0\n    \n    \n      17\n      34000\n      K-Marks\n      Veltecite Hearts\n      12974.0\n      21026.0\n    \n    \n      18\n      40000\n      K-Marks\n      Laser Rifles\n      18259.0\n      21741.0\n    \n    \n      19\n      47000\n      K-Marks\n      Classified II\n      23922.0\n      23078.0\n    \n    \n      20\n      41000\n      K-Marks\n      Unlimited Power\n      17377.0\n      23623.0\n    \n    \n      32\n      27000\n      K-Marks\n      Meteor Core\n      7594.0\n      19406.0\n    \n    \n      33\n      30000\n      K-Marks\n      Shards of pure Power\n      8104.0\n      21896.0\n    \n    \n      34\n      48000\n      K-Marks\n      Battery Day\n      22782.0\n      25218.0\n    \n    \n      41\n      28000\n      K-Marks\n      Upgrades\n      3845.0\n      24155.0\n    \n    \n      43\n      140000\n      K-Marks\n      Exclusive Drilling Rights\n      115330.0\n      24670.0\n    \n    \n      44\n      32000\n      K-Marks\n      Stocking Up\n      9120.0\n      22880.0\n    \n    \n      45\n      26000\n      K-Marks\n      Shock and Awe\n      11385.0\n      14615.0\n    \n    \n      50\n      37000\n      K-Marks\n      Sabotage\n      15696.0\n      21304.0\n    \n    \n      51\n      28000\n      K-Marks\n      Old Currency\n      6000.0\n      22000.0\n    \n    \n      52\n      26000\n      K-Marks\n      Recoil Compensation\n      9390.0\n      16610.0\n    \n    \n      54\n      45000\n      K-Marks\n      New 3D Printer\n      21362.0\n      23638.0\n    \n    \n      55\n      33000\n      K-Marks\n      Arms Race\n      11532.0\n      21468.0\n    \n    \n      56\n      35000\n      K-Marks\n      Energy Crisis\n      18984.0\n      16016.0\n    \n    \n      71\n      26000\n      K-Marks\n      Data Drive IV\n      12150.0\n      13850.0\n    \n    \n      76\n      33000\n      K-Marks\n      A Solution\n      10125.0\n      22875.0\n    \n    \n      82\n      25000\n      K-Marks\n      Data Drop\n      10124.0\n      14876.0\n    \n    \n      83\n      26000\n      K-Marks\n      Field Supplies\n      3000.0\n      23000.0\n    \n    \n      84\n      25000\n      K-Marks\n      Spare Parts\n      2024.0\n      22976.0\n    \n    \n      90\n      29000\n      K-Marks\n      NEW-Hard-ICA-Gather-6\n      9142.0\n      19858.0\n    \n    \n      91\n      227000\n      K-Marks\n      Striking Big\n      201830.0\n      25170.0\n    \n    \n      93\n      25000\n      K-Marks\n      A Craving\n      4048.0\n      20952.0\n    \n    \n      97\n      26000\n      K-Marks\n      Cretins\n      6300.0\n      19700.0\n    \n    \n      98\n      27000\n      K-Marks\n      Sensor Array Repairs\n      6908.0\n      20092.0\n    \n    \n      99\n      32000\n      K-Marks\n      A new Energy Source\n      11289.0\n      20711.0\n    \n    \n      100\n      30000\n      K-Marks\n      Data Center Upgrades\n      7690.0\n      22310.0\n    \n    \n      101\n      30000\n      K-Marks\n      Keep the Experiment running\n      7200.0\n      22800.0\n    \n    \n      102\n      61000\n      K-Marks\n      Safety Measures\n      36309.0\n      24691.0\n    \n    \n      122\n      31000\n      K-Marks\n      Harvest Day\n      6583.0\n      24417.0\n    \n    \n      125\n      37000\n      K-Marks\n      Core Values\n      15188.0\n      21812.0\n    \n    \n      126\n      45000\n      K-Marks\n      Promising Results\n      21266.0\n      23734.0\n    \n    \n      130\n      24000\n      K-Marks\n      Stormy Samples\n      2700.0\n      21300.0\n    \n    \n      131\n      25000\n      K-Marks\n      Stormy Samples II\n      2024.0\n      22976.0\n    \n    \n      132\n      33000\n      K-Marks\n      Loadout Drop\n      7320.0\n      25680.0\n    \n    \n      136\n      33000\n      K-Marks\n      NEW-Hard-Osiris-Gather-7\n      12816.0\n      20184.0\n    \n    \n      138\n      155000\n      K-Marks\n      NEW-Hard-Osiris-EliteCrusher-1\n      0.0\n      155000.0\n    \n  \n\n\n\n\nPerfect! Next we’ll loop back around to automating the pipeline and uploading the data to Kaggle."
  },
  {
    "objectID": "posts/nbks/2022-11-09-fp-discounted-cash-flows.html",
    "href": "posts/nbks/2022-11-09-fp-discounted-cash-flows.html",
    "title": "Financial Planner Review - Discounting Cash Flows",
    "section": "",
    "text": "Among the list of formulas I learned in the Financial Analyst coures, this is valuation is the one I instintively distrusted. Discounted Cash Flows is a way to evaluate the future returns on an investement based on: 1. The initial Investment. 2. The Expected Cashflow per each year.\nThe problem I have here is that the Expected Cashflows are purely guesses about what we belive the numbers to be in the future. While the example in the class picked - as far as I can tell - random numbers to fill them in, I would hope that the numbers provided here would be based on research of other similar projects. At least, I would hope in the real world this is was happens."
  },
  {
    "objectID": "posts/nbks/2022-11-09-fp-discounted-cash-flows.html#discounting-cash-flows",
    "href": "posts/nbks/2022-11-09-fp-discounted-cash-flows.html#discounting-cash-flows",
    "title": "Financial Planner Review - Discounting Cash Flows",
    "section": "Discounting Cash Flows",
    "text": "Discounting Cash Flows\nThe formula for this is: > \\(DCF= (CF_1)/(1 + r)^1 + (CF_2)/(1 + r)^2 + .. + (CF_n)/(1 + r)^n\\)\n… where: \n\nCF stands for Cash Flow\nr is the interest rate\nn is the period of discount.\n\nThis all follows from the simple idea that money now is more valuble than money in the future. If given the choice between $100 now and $100 in a year then you’d obviously take the $100 now. This also applies to money we earn in the future: $60 now is better than the $60 we’d earn in the future. So, example time.\nWe’re going to use a similar example to the class: Imagine that we’re considering inveseting money into a venture. We’re going to do something a bit more modern and say we’re investing in a growing Online Streamer with the expectation that we’ll get some of the money in turn. A deal is worked out and you’ll be getting some kind of slice of the money they make in exchange. They’re going to ask for $500 in investment. For the interest rate, we’ll use the Inflation Rate since this is not a loan. Right now, it’s 8.2% so we’ll use that as our Interest Rate; Sometimes this also called the Discount Rate and Inflation certainly applies.\nWe’ll use random numbers over the span of 6 years and then calculate if this was a good idea.\n\nfrom random import randint\n\nyear = range(0,6)\ninterestRate = .082\n\npredictions = [ randint(0, 300) for _ in range(5)]\ncashFlow = [-500] + predictions\ncashFlow\n\n[-500, 195, 127, 44, 42, 196]\n\n\nWe’ll need a function to calculate the Present Value for each term in the formula. For this, we’ll just write up a quick lambda function in python.\n\nPV = (lambda f,i,n: f/(1+i)**n)\nPV(30, interestRate, 1)\n\n27.726432532347502\n\n\nWe’ll usually see this in a table so we’ll add all this to a Data frame.\n\ndata = pd.DataFrame(\n    {\"Year\":year,\n    \"Cash\":cashFlow,\n    'Pv':repeat(0,len(year))\n})\ndata\n\n\n\n\n\n  \n    \n      \n      Year\n      Cash\n      Pv\n    \n  \n  \n    \n      0\n      0\n      -500\n      0\n    \n    \n      1\n      1\n      195\n      0\n    \n    \n      2\n      2\n      127\n      0\n    \n    \n      3\n      3\n      44\n      0\n    \n    \n      4\n      4\n      42\n      0\n    \n    \n      5\n      5\n      196\n      0\n    \n  \n\n\n\n\n… and now we can iterate through the rows and fill in the Present Value per year.\n\n\nfor _,r in data.iterrows():\n    data.loc[r.Year, 'PV'] = round(PV(r.Cash, interestRate, r.Year), 2 )\n\nLastly, we’ll take the sum to see if it was worth it.\n\ndata.PV.sum()\n\n-13.749999999999972\n\n\nLooks like we lost about $14 which is not that surprising since making money in Streaming can be quite challenging."
  },
  {
    "objectID": "posts/nbks/2022-11-16-does-faker-cc-pass-luhns-algo.html",
    "href": "posts/nbks/2022-11-16-does-faker-cc-pass-luhns-algo.html",
    "title": "Does The Faker Package Pass Luhn?",
    "section": "",
    "text": "While taking a class about anonymizing data for public release, Rebeca Gonzalez discussed using the Faker python package to substitute real values for fake values to protect users. One of the functions listed is to generate fake credit card numbers for data.If you’re not aware, there is a patented algorithm which must pass for a generated Credit Card number written by Hans Peter Luhn from IBM. The Algorithm is not complicated and we’ll work through it now.\nLuhn’s Algorithm: 1. The algorithm first should split the number into two parts: the digits and the check value. 2. For the digits and moving left to right: 1. If Odd position then multiply by 2 and return that number; if bigger than 10 return a modulo of it. 2. If Even then simply return that number. 3. Sum the digits after the above computation. 4. Take the modulo 10 of the sum and subtract it from 10. 5. Compare the last digit against the excluded digit and they should match.\nWe will skip the first step since it will be an easy split for now. We’ll want a function which deals with step 2.\n\n# Apply to each Digit\ndef luhnSingleDigit( digit, even = True):\n    if even:\n       # if even, modify and return\n        n =  int(digit) * 2\n        if n == 10: return 0\n        if n >= 10: return 1 + n % 10\n        return n\n    else:\n        # If position is odd, return itself\n        return int(digit)\n\nluhnSingleDigit(8), luhnSingleDigit(8, even = False)\n\n(7, 8)\n\n\nNow we’ll need to iterate through each digits and so we’ll need a fake credit card number. I will reverse the order of the digits since it will be easier than trying to count backwards when we iterate through the digits.\n\nfrom faker import Faker\nfake = Faker()\ncc = fake.credit_card_number()\n\n# reverse the digits\ndigits, parity = cc[:-1:][::-1], cc[-1]\ndigits, parity \n\n('56086526834343', '5')\n\n\nWe will iterate through the digits via list comprehension in python - which is how we’re going to do next. You can have an if-else included in a list comprehension which we’ll need to tell the code which flagged version to use.\n\n# Test the filter and make sure it works\n[0 if i % 2 == 1 else x for i, x in enumerate(range(20))]\n\n[0, 0, 2, 0, 4, 0, 6, 0, 8, 0, 10, 0, 12, 0, 14, 0, 16, 0, 18, 0]\n\n\nNow we’ll throw the function in there to test it.\n\n[ luhnSingleDigit(x, even = False) if i % 2 == 1 else luhnSingleDigit(x) for i, x in enumerate(digits) ]\n\n[0, 6, 0, 8, 3, 5, 4, 6, 7, 3, 8, 3, 8, 3]\n\n\nNext we have to sum them all together and take the last digit.\n\ntheSum = sum([ luhnSingleDigit(x, even = False) if i % 2 == 1 else luhnSingleDigit(x) for i, x in enumerate(digits) ])\ntheSum\n\n64\n\n\nLastly, we do the modulo 10, subtract 10 from the computed sum and compare it to the parity digit\n\ntheSum, 10 - theSum % 10, 10 - theSum % 10 == parity\n\n(64, 6, False)\n\n\nIt was not valid credit card number!  Let’s get those steps into a function and time it to see how long it takes. And, let’s get about 100 of these numbers to check just in case it was a fluke.\n\ndef confirmLuhn(number):\n    digits, parity = cc[:-1:][::-1], cc[-1]\n    theSum = sum([\n        luhnSingleDigit(x, even = False) if i % 2 == 1\n        else luhnSingleDigit(x)\n        for i, x in enumerate(digits) ]\n    )\n\n    return 10 - theSum % 10 == parity\n\nOne thing I noticed from the output in the lectures was that there were different length fake card numbers; is that the case here too?\n\nimport numpy as np\nnumbers = [ fake.credit_card_number() for _ in range(100)]\nnp.mean([len(n) for n in numbers])\n\n15.47\n\n\nThat’s correct. Some of them are simply too short to be a valid credit card number. I’m not sure why this is but that’s beyond the scope of this post so we will simply filter them and move on.\n\nfilteredNumbers = list(filter( lambda x: len(x) == 16, numbers))\nlen(filteredNumbers)\n\n51\n\n\nOuch. That’s a lot misses. Ok, on to the main event!\n\n%timeit sum([confirmLuhn(c) for c in filteredNumbers])\n\n179 µs ± 820 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\nsum([confirmLuhn(c) for c in filteredNumbers])\n\n0\n\n\nSo, it looks like there were no valid credit card numbers in the list but at least it was fast. Good to know that the numbers being generated are not constricted to valid numbers."
  },
  {
    "objectID": "posts/nbks/2023-03-18-valorant-color-crosshair-part-two.html",
    "href": "posts/nbks/2023-03-18-valorant-color-crosshair-part-two.html",
    "title": "Can We Find A Better Crosshair In Valorant: Part Two?",
    "section": "",
    "text": "If you did not read the previous post then I would recommend reading about the inspiration for the post and how we got here. In fact, I strongly recommend it since I’m going to be skipping some of the basic discussion and hopping right into getting other maps working."
  },
  {
    "objectID": "posts/nbks/2023-03-18-valorant-color-crosshair-part-two.html#image-average-function",
    "href": "posts/nbks/2023-03-18-valorant-color-crosshair-part-two.html#image-average-function",
    "title": "Can We Find A Better Crosshair In Valorant: Part Two?",
    "section": "Image Average Function",
    "text": "Image Average Function\nWe already went over how to read an image but we’ll do it again. Pytorch has the function read_image() to read an image into a torch tensor. And, we’ll need to adjust the dimensions so we can see the images.\n\nplt.axis('off')\nplt.imshow(read_image(str(dataDirectory/mapSubset.image.iloc[0])).moveaxis(0,2));\n# Hi Alex!\n\n\n\n\nWe are going to keep using the mean since this is acceptable for our use case. Calculating this is simple enough:\n\nread_image(str(dataDirectory/mapSubset.image.iloc[0])).moveaxis(0,2).numpy().mean(axis=(0,1))\n\narray([109.47447097,  69.39480131,  63.72558256])\n\n\nPerfect. So, now we have all the pieces we need to calcuate the mean image per map.\n\nmapColors = mapSubset.groupby('map', group_keys=True)\\\n    .apply(lambda x:\n        tuple(\n            read_image(\n            str(dataDirectory/x.image.values[0])).moveaxis(0,2).numpy().mean(axis=(0,1))\n        )\n)\n\nfor i,color in enumerate(mapColors):\n    print(f\"The Map {mapColors.index[i]} has a mean color of {color}\")\n\nThe Map Ascent has a mean color of (109.4744709683642, 69.3948013117284, 63.725582561728395)\nThe Map Bind has a mean color of (114.81199025848765, 113.48977719907407, 130.3785411844136)\nThe Map Breeze has a mean color of (113.30181327160494, 91.05116560570988, 93.52008969907408)\nThe Map Haven has a mean color of (72.52671730324074, 57.23288917824074, 59.851847993827164)\nThe Map Icebox has a mean color of (94.04369695216049, 86.80147665895062, 83.2131568287037)\n\n\nThose tuples are not of much value so we will convert them into images and show them all off.\n\nfrom matplotlib import pyplot\nfor i,color in enumerate(mapColors):\n    height, width, channels = 100, 100, 3\n    image_blank_new = np.zeros((height, width, channels), np.uint8)\n    image_blank_new[::] = color\n    plt.axis('off')\n    plt.title(mapColors.index[i])\n    pyplot.imshow(image_blank_new)\n    pyplot.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI notice that our Icebox is not the same result as we got last time. Perhaps in another post we’ll explore the why for that. What we need to do now is take the color data and create a high contrast color as we had done before. We’ll go ahead and do this per map and we can reuse the for loop for this.\n\nfrom PIL import ImageEnhance\n\nfor i,color in enumerate(mapColors):\n    # Generate the initial image\n    height, width, channels = 100, 100, 3\n    image_blank_new = np.zeros((height, width, channels), np.uint8)\n    image_blank_new[::] = color\n\n    # Enhance the contrast\n    im_new = Image.fromarray(image_blank_new)\n    imC_new = ImageEnhance.Contrast(im_new).enhance(10)\n\n    # Generate the comparison image\n    image_blank_final = np.zeros((height, width, channels), np.uint8)\n    image_blank_final[0:100,0:50] = color\n    image_blank_final[0:100,50:100] = np.array(imC_new)[0][0]\n\n    # Plotting part:\n    plt.axis('off');\n    plt.title(mapColors.index[i]);\n    pyplot.imshow(image_blank_final);\n    pyplot.show();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd, lastly we’ll get the RGB values for the colors in case anyone wants to try this out!\n\nfrom matplotlib.colors import rgb2hex\n\nfor i,color in enumerate(mapColors):\n    height, width, channels = 100, 100, 3\n    image_blank_new = np.zeros((height, width, channels), np.uint8)\n    image_blank_new[::] = color\n    cxColor = rgb2hex(np.array(image_blank_new).mean(axis=0)[0]/255)\n    print(f\"Use the crosshair color {cxColor} for {mapColors.index[i]}\")\n\nUse the crosshair color #6d453f for Ascent\nUse the crosshair color #727182 for Bind\nUse the crosshair color #715b5d for Breeze\nUse the crosshair color #48393b for Haven\nUse the crosshair color #5e5653 for Icebox"
  },
  {
    "objectID": "posts/nbks/2022-12-06-cycle-jobs-part-five.html",
    "href": "posts/nbks/2022-12-06-cycle-jobs-part-five.html",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Five.",
    "section": "",
    "text": "In the previuos post, we dealt with missing values and finally asking some last minute questions. In this post, we’re going to follow in the footsteps of Robert Ritz’s post about getting a dataset updated automatically to avoid cluttering up Kaggle with old datasets which are not very useful being out of date.\nNote that this post is about getting the data uploaded and not about doing any sort of Analysis."
  },
  {
    "objectID": "posts/nbks/2022-11-02-simple-hightlight-over-exiting-plot.html",
    "href": "posts/nbks/2022-11-02-simple-hightlight-over-exiting-plot.html",
    "title": "Simple Function for Highlight Box Effect",
    "section": "",
    "text": "While browsing around for inspiration, I found this wonderful post by someone named SonyMonthonaRK on Medium; the post is here since it contains more than what I’ll be talking about. The graph is simple aside from the red highlighted box around relevant information: seen below: \nI’ve seen this effect before but never actually seen the code for it. Thankfully, he provided the a link to it on their Github page since I wanted to learn this trick. The Good news is that the effect is not complicated; the bad news is that it’s implementation here is conditional and ugly. We’re going to copy it here just for reference:\nplt.figure(figsize=(15,10)) # mengatur ukuran figure\nsns.lineplot(x='month', y='average_num_booking', hue='hotel_type', color= 'gray',\n             size=\"hotel_type\", sizes=(2.5, 2.5), data=df1_gr) # plot awal menggunakan lineplot dari library seaborn\n\nplt.tick_params(axis='both', which='major', labelsize=14) # memperbesar ukuran x-y axis label\nplt.grid() # menambahkan gridline\nplt.legend(title='Hotel Type', title_fontsize=15, prop={'size':13}) # mengatur judul dan ukuran font pada legenda\n\nplt.xlabel('Arrival Month', fontsize=15) # mengatur title pada x-axis \nplt.ylabel('Average Number of Booking', fontsize=15) # mengatur title pada y-axis\nplt.ylim(0, 4800) # membatasi y axis\n\nplt.axvline(4, ls='--', color='red') # membuat garis vertikal untuk menghighlight insight\nplt.axvline(6, ls='--', color='red') # membuat garis vertikal untuk menghighlight insight\nplt.text(x=4.5, y=4400, s='Holiday \\nSeason', fontsize=16, color='red') # menambahkan teks keterangan\nplt.stackplot(np.arange(4,7,1), [[4800]], color='grey', alpha=0.3) # memberikan blok warna pada area yang dihighlight 2 garis vertikal\n\nplt.axvline(9, ls='--', color='red') # membuat garis vertikal untuk menghighlight insight\nplt.axvline(11, ls='--', color='magenta') # membuat garis vertikal untuk menghighlight insight\nplt.text(x=9.5, y=4400, s='Holiday \\nSeason', fontsize=16, color='red') # menambahkan teks keterangan\nplt.stackplot(np.arange(9,12,1), [[4800]], color='grey', alpha=0.3) # memberikan blok warna pada\n\nplt.text(x=-0.5, y=5200, s=\"Both Hotels Have More Guests During Holiday Season\", \n         fontsize=20, fontweight='bold') # memberikan judul yang informatif\nplt.text(x=-0.5, y=4850, s=\"City Hotel has the decreased guests in August and September, while both hotels have less \\ncustomer during not holiday season (Jan-Mar)\", \n         fontsize=18) # memberikan keterangan tambahan atas judul\n\nplt.tight_layout() # mengatur layout dari visualisasi agar tidak terpotong\nThis is a lot of - specialized and inflexible - code to add an affect to a graph. If I wanted to add this to my own graphs then I would have to do quite a bit of work and math and customization. I want this on any graph at any point without doing all this work. So, let us take it apart and make a function we can all use. We’re not going to use their data since we want to apply it to any random set of data so we’re using random data.\n\n# our pretend dataset\ndata = np.random.randn(50)\ndata = pd.DataFrame(data, columns=['Random'])\n\nMuch of that is boilerplate and repetition so the summary of a single application would be:\n# Initialize the lineplot\n\n# Add the left dotted line\n\n# Add the right dotted line.\n\n# Add the notated text\n\n# Add a stackplot to fill the space between the lines\nWe’ll cover the plot first since it’s the easiest and can go outside of the function we’re trying to build. We’ll use the seaborn lineplot the same. We’ll need use an index similar to the example post while we build this - so a simple range:\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nWe’ll want to save the graph reference as we’ll need it for later.\nLooking at the docs for the horizintal line function, we need: 1. The x tick value. 2. The line display type. 3. The color\nWe’re going to also modify the line width in our example and fix it; you are welcome to modify this function to accept a thicker value but I like it at 1 and have no current reason to change it. We will need the left and right line points passed to the function as a starting point; we’ll hold the other values constant for now.\n\ndef building(p = None, xCoord = None):\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')    \n\n\nplt.figure(figsize=(12,5))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5))\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nGood start. The next problem will be adding the text inside the box and how to position the text automatically. We could brute force - and in some instances we might have no other choice - but that is not convenient while we’re automating the function. How I solved this is to think about what we’re doing in mathematical terms. 1. Find the center of the x values range. 2. Translate the text to the left based off the figure size and the length of the text.\nSo, first we find the middle point by taking the difference of both sides, dividing it by two and then adding that to the left side.\n\ndef building(p = None, xCoord = None, text=None, xText=None):\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n\n    # Ignore this for now; we simply need it\n    y_min, y_max = p.get_ylim()\n    yPos = y_max*.8\n\n    xText = xCoord[0] + (xCoord[1] - xCoord[0])/2\n    plt.text(x=xText, y=yPos, s=text, fontsize=12, color='red')    \n\n\nplt.figure(figsize=(12,5))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5), \"Solid\")\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nNext, we’ll need to get the figure size as well as the length of the text. Thankfully, matplotlib allows you to get the dimensions of the figure using .figure.get_size_inches().\n\ndef building(p = None, xCoord = None, text=None, xText=None):\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n\n    # Ignore this for now; we simply need it\n    y_min, y_max = p.get_ylim()\n    yPos = y_max*.8\n\n    width, _ = p.figure.get_size_inches()\n    lenText = len(text)\n    \n    xText = xCoord[0] + (xCoord[1] - xCoord[0])/2\n    plt.text(x=xText, y=yPos, s=text, fontsize=12, color='red')    \n\n\nplt.figure(figsize=(12,5))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5), \"Solid\")\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nNow that we have these values, we can think about how to move them. The answer is this formula: lenText * 3/width.\n\ndef building(p = None, xCoord = None, text=None, xText=None):\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n\n    # Ignore this for now; we simply need it\n    y_min, y_max = p.get_ylim()\n    yPos = y_max*.8\n\n    width, _ = p.figure.get_size_inches()\n    lenText = len(text)\n\n    xText = xCoord[0]  + (xCoord[1] - xCoord[0])/2 - 3 * lenText/width\n    plt.text(x=xText, y=yPos, s=text, fontsize=12, color='red')    \n\n\nplt.figure(figsize=(10,5))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5), \"Solid\")\n\n# Showing you can simply run this more than once\n# to add another box highlight\nbuilding(p, (20,40), \"Something Else\")\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nSo, why does this work? Frankily, I’m not sure quite sure where the three comes from here. Normalizing the number of characters by the figures width make sense but that alone is not enough; you can see this in the below example.\n\ndef building(p = None, xCoord = None, text=None, xText=None):\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n\n    # Ignore this for now; we simply need it\n    y_min, y_max = p.get_ylim()\n    yPos = y_max*.8\n\n    width, _ = p.figure.get_size_inches()\n    lenText = len(text)\n\n    xText = xCoord[0]  + (xCoord[1] - xCoord[0])/2 - lenText/width\n    plt.text(x=xText, y=yPos, s=text, fontsize=12, color='red')    \n\n\nplt.figure(figsize=(10,5))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5), \"Solid\")\n\n# Showing you can simply run this more than once\n# to add another box highlight\nbuilding(p, (20,40), \"Something Else\")\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nAnyways, we’ll leave what works for now and move to the last problem which is much easier to solve: the stackplot. If you’re not familiar with these graphs then it’s more than likely you’re unfamiliar with the name and not the plot itself. These are used to show percentage change of different categories - usually over time. A good example would be the follow image from the website Geeksforgeeks: \nExcept what we want is one box full of a single color: grey. Maybe there is a better way to do this but we’re going to base our work off the post and it will work anyways. For a stackplot we need: 1. The coordinates to fill. 2. The values.\nIn the example, the height is filled with a known value: 4800. But this value is the distance from the x-axis and we cannot use this in our example - nor will this generalize. Since their data does not have negative values this works fine but sometimes we will have negative values. What we will need to do is find a fill value. We could use the figure height but there is a case where the total distance can be greater than the figure height strangely enough. What we’ll do is take the sum of the absolute values of the y_min and y_max values + 1. Then, if that values is larger than the height we’ll use that instead.\n\ndef building(p = None, xCoord = None, text=None, xText=None):\n\n    # Ignore this for now; we simply need it\n    y_min, y_max = p.get_ylim()\n    p.set_ylim(y_min, y_max)\n\n    yPos = y_max*.8\n\n    # Collecting height here:\n    width, height = p.figure.get_size_inches()\n    totalDist = abs(y_min) + abs(y_max) + 1\n    stackFill = height\n\n    if totalDist > height:\n        stackFill = totalDist\n\n    lenText = len(text)\n    xText = xCoord[0]  + (xCoord[1] - xCoord[0])/2 - 3 * lenText/width\n\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n\n    plt.text(x=xText, y=yPos, s=text, fontsize=12, color='red')   \n    plt.stackplot(np.arange(xCoord[0],xCoord[1]+1,1), [[stackFill]], baseline = \"sym\", color='grey', alpha=0.3) \n\n\nplt.figure(figsize=(12,6))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5), \"Solid\")\n\n# Showing you can simply run this more than once\n# to add another box highlight\n#\nbuilding(p, (20,40), \"Something Else\")\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nYou may be asking what the baseline=\"sym\" argument is since it’s not very descriptive. All that does is changes the relationship from 0 start to Symmetric around the x-axis which we needed for negative values. And, speaking let us test that we can filter for positive values and this all still works.\n\ntmp = data.query(\"Random >= 0\")\n\nplt.figure(figsize=(15,5))\np = sns.lineplot(x=range(0, len(tmp)), y=\"Random\", data=tmp)\nplt.grid()\nbuilding(p, (5, 10), \"Turtles\")\nbuilding(p, (21, 25), \"Doves\")\n\n\n\n\nExcellent. The Doves to my eyes looks a little off center but that should be good enough. Just add a little bit of error handling and we’ve got a generalized highlight box function!\n\ndef addDotBox(p = None, xCoord = None, text=None, xText=None, yText=None, fSize = 12):\n    if not p:\n        raise ValueError(\"Plot cannot be None Type.\")\n    if not text:\n        raise ValueError(\"text cannot be left blank.\")\n    \n    # Pull/reset the limits:\n    y_min, y_max = p.get_ylim()\n    p.set_ylim(y_min, y_max)\n\n    # get \n    width, height = p.figure.get_size_inches()\n    totalDist = abs(y_min) + abs(y_max) + 1\n    stackFill = height\n\n    if totalDist > height:\n        stackFill = totalDist\n    lenText = len(text)\n\n    if not xCoord:\n        raise ValueError(\"Missing xCoord argument.\")\n    if len(xCoord) != 2:\n        raise ValueError(\"There must be two values for xCoord\")\n    if not xText:\n        xText = xCoord[0]  + (xCoord[1] - xCoord[0])/2 - 3 * lenText/width\n    if not yText:\n        yText = y_max * .8\n    \n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n    plt.text(x=xText, y=y_max*.8 , s=text, fontsize=fSize, color='red')\n    plt.stackplot(np.arange(xCoord[0],xCoord[1]+1,1), [[stackFill]], baseline = \"sym\", color='grey', alpha=0.3)"
  },
  {
    "objectID": "posts/nbks/2022-10-19-ferrets-or-dragons.html",
    "href": "posts/nbks/2022-10-19-ferrets-or-dragons.html",
    "title": "Something Cute Or Something Dangerous",
    "section": "",
    "text": "Today we’re going to do something a tad simpier since the last attempt to build an image Classification model was too ambitious. Today we’re going to build an image classifier model to tell the difference between Ferrets and Dragons. These are categories I’m far more familiar with and much more comfortable telling apart. After all, one is a floofer and the other is a scaley boi.\nWe’re going to be using the same code and the same process as before. As a guideline, if you’re writing the same code more than twice then you should turn it into a function; if we’re writing the same notebook code more than twice then it’s time to build a template. By Template I mean a notebook that contains the boilerplate code which will be constaintly used over and over again. There may be a better name for this concept but looking over other words they don’t fit. And, the template concept is already something used in Web Design - such as [Jinja](https://en.wikipedia.org/wiki/Jinja_(template_engine). Although,"
  },
  {
    "objectID": "posts/nbks/2022-10-19-ferrets-or-dragons.html#the-actual-work",
    "href": "posts/nbks/2022-10-19-ferrets-or-dragons.html#the-actual-work",
    "title": "Something Cute Or Something Dangerous",
    "section": "The Actual Work",
    "text": "The Actual Work\nWe’ll start with our normal imports for working on these problems. Note that we didn’t need to import pandas and such since fastai is doing this for us.\n\nimport os\nfrom pathlib import Path\nfrom time import sleep\n\n\nfrom duckduckgo_search import ddg_images\nfrom fastdownload import download_url\n\nfrom fastcore.all import *\nfrom fastai.vision.all import *     # This is for the CNN learner.\n\nAs before, we’re going to reuse the image searching and download function used by Jeremy since it’s so useful.\n\n# This is a function from the notebook:\ndef search_images(term, max_images=200):\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\ndest = Path('..', '__data', 'example-ferret.png')\ndownload_url(urls[0], dest, show_progress=False)\n\nim = Image.open(dest)\nim.to_thumb(420)\n\n\n\n\nWhat a cute litte floofer! Now we’ll collect our data for training - and remove the failed images.\n\nsearches = 'ferret','dragon'\npath = Path('..', '__data', 'ferret_or_dragon')\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server and blocking responses\n    resize_images(path/o, max_size=400, dest=path/o)\n\n/usr/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n5\n\n\nI still need to write up a post about these Data Block objects since they’re a good idea and might be able to generalize them beyond just Machine Learning.\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=71),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=12)\n\n\n\n\nAnd, now the training!\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(7)\n\n/home/ranuse/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/home/ranuse/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.740871\n      0.012448\n      0.000000\n      00:04\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.084491\n      0.001599\n      0.000000\n      00:02\n    \n    \n      1\n      0.049812\n      0.000780\n      0.000000\n      00:02\n    \n    \n      2\n      0.032287\n      0.000196\n      0.000000\n      00:02\n    \n    \n      3\n      0.023381\n      0.000115\n      0.000000\n      00:02\n    \n    \n      4\n      0.018074\n      0.000076\n      0.000000\n      00:02\n    \n    \n      5\n      0.014552\n      0.000082\n      0.000000\n      00:02\n    \n    \n      6\n      0.012025\n      0.000100\n      0.000000\n      00:02\n    \n  \n\n\n\nThe model doing this well isn’t surprising since the categories we’ve picked are intentionally distinct. After all, the point of this post is to show how simple this can be.\nSo, now we check to see if it is a Floofer.\n\nis_ferret,_,probs = learn.predict(PILImage.create(Path('..', '__data', 'example-ferret.png')))\nprint(f\"This is a: {is_ferret}.\")\nprint(f\"Probability it's a Floofer: {probs[1]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: ferret.\nProbability it's a Floofer: 1.0000\n\n\nYou may have caught there is a single difference in the code here. I changed the probs[0] to probs[1] to pull the probability. I’m not sure how or why it’s returning like this but the probability tensor is reversed even though the category is correct. If I do the Dragon category as a example:\n\nlearn.predict(PILImage.create(Path('..', '__data', 'example-dragon.png')))\n\n\n\n\n\n\n\n\n('dragon', TensorBase(0), TensorBase([1.0000e+00, 1.3599e-08]))\n\n\n… then we see that the category is correct and the probability tensor for Dragon is now in index 0 instead of ferret. We can check the this under learn.dls.vocab and the Dragon category is in fact in index 0:\n\nlearn.dls.vocab\n\n['dragon', 'ferret']\n\n\nI would not of expected to need to check this for such a simple model but from now on I’ll need to keep in mind the order. So, let’s wright some code quick so we don’t have to think about this again. We’ll attach these together using a dictionary.\n\n_,_,probs = learn.predict(PILImage.create(Path('..', '__data', 'example-dragon.png')))\ndict(zip(learn.dls.vocab, probs))\n\n\n\n\n\n\n\n\n{'dragon': TensorBase(1.), 'ferret': TensorBase(1.3599e-08)}\n\n\nUsing this now, we can simply ask for the probability based on the category we care about. Moving this a step forward, we can turn this into a function now for future use:\n\ndef paired_categories(learner, impath):\n    _,_,probs = learner.predict(PILImage.create(impath))\n    return dict(zip(learn.dls.vocab, probs))\n\n\npCats = paired_categories(learn, Path('..', '__data', 'example-dragon.png'))\npCats\n\n\n\n\n\n\n\n\n{'dragon': TensorBase(1.), 'ferret': TensorBase(1.3599e-08)}\n\n\n\npCats['dragon']\n\nTensorBase(1.)"
  },
  {
    "objectID": "posts/qmds/2023-02-01-exploring-binomial-test.html",
    "href": "posts/qmds/2023-02-01-exploring-binomial-test.html",
    "title": "Binomial Test in R And Python",
    "section": "",
    "text": "The Binomial Test is a test between two categories and it was the results of some of Daniel Bernoulli’s Experiments. What this test does is compare the calculated result against a Binomial Distribution. That’s about as helpful as someone repeating themseves with a term you’ve never heard before - like I just did - so let’s go a bit deeper.\nA Bernoulli Trial is where you have clearly two unique and competing events: A coin flip is the canonical example. You then observe which event occurs and record that. So, you flip a coin and observe whether it lands on heads or tails and then you record what that result was. When you repeat this and collect multiple results, then this is called the Bernoulli Process because you are repeating the same Bernoulli Trial. Once all the results are collected - and since the events are independent of one another -, you get a distribution that tells you the probability that event one or event two will occur over the n flips - or trials. When you give the Binoimial Test the values, it then tells you how often you could expect to see the result and compares that to a true Binomial Distribution.\n\n\nThere is a function for this in the scipy package which we’ll use one more. And again, we’ll use the data which was used in the previous post about the Chi Squared Test.\n\n# Binomial Test in python:\nimport scipy as sci\nimport pandas as pd\n\ndata = pd.read_csv(\"https://query.data.world/s/ccu3fx6fivcq2vdyavd26fhzk5tokq\")\ndata.head()\n\n   Subject Pref\n0        1    B\n1        2    B\n2        3    B\n3        4    B\n4        5    B\n\n\nWe’re going to use the same trick with the pivot table I used before to get this into the right format for the test. You can run a Binomial Test in python using the function *scipy.stats.binom_test()`:\n\nsci.stats.binom_test( data.pivot_table( index = 'Pref', aggfunc=len ))[0]\n\n4.223704562267977e-05\n\n<string>:1: DeprecationWarning: 'binom_test' is deprecated in favour of 'binomtest' from version 1.7.0 and will be removed in Scipy 1.12.0.\n\n\nWhat I did above is the old method - which I was using and was included in my notes. I considered not including the older code but:\n\nSomeone else might run across this problem and this may be helpful.\nThis is a good time to show a bit more about how this function works.\n\nThere are some differences already between the two function’s expected inputs:\n\nfrom inspect import signature\nsignature(sci.stats.binom_test)\n\n<Signature (x, n=None, p=0.5, alternative='two-sided')>\n\n\n\nsignature(sci.stats.binomtest)\n\n<Signature (k, n, p=0.5, alternative='two-sided')>\n\n\nIf we look at the documentation page we can see that this no longer allows for passing an array or dataframe. Instead, it expects us to pass integers. So, what are the parameters Number of Successes and Number of Trials? As per my previous discussion above, those would be how often the coin was heads and how many times the coin was flipped. But, since this data was about user preferences on a website then they would be how often subjects had a preference for A and how many records there are in our data!\n\nxtab = data.pivot_table( index = 'Pref', aggfunc=len )\nrst = sci.stats.binomtest(xtab.iloc[0,0], n=len(data))\nprint(f\"The Binom Test Statistic is {round(rst.statistic, 3)} with a p-value of {rst.pvalue:.5E}\")\n\nThe Binom Test Statistic is 0.233 with a p-value of 4.22370E-05\n\n\nSo, from these results we reject the null hypothesis that these two possibilites are equally likely to occur. That was quite a bit of work and we’re using a new function so how do we know we did not make a mistake? We know we got the same answer as the old version of the function so that is a good sign. And, we’ll be verfiying it again with R next.\n\n\n\nR’s version is so much easier to use. We’ll take the code from before to get the data downloaded.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ndf = read_csv(\n    \"https://query.data.world/s/ccu3fx6fivcq2vdyavd26fhzk5tokq\",\n    col_types='if' # specify the column types are integer, factor.\n)\ndf %>% head\n\n# A tibble: 6 × 2\n  Subject Pref \n    <int> <fct>\n1       1 B    \n2       2 B    \n3       3 B    \n4       4 B    \n5       5 B    \n6       6 B    \n\n\n… and we’ll just send it right on to the function:\n\ndf %>%\n    # format it as expected\n    xtabs(~ Pref, .) %>%\n    # pipe into the test for the results.\n    binom.test\n\n\n    Exact binomial test\n\ndata:  .\nnumber of successes = 46, number of trials = 60, p-value = 4.224e-05\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.6396172 0.8661627\nsample estimates:\nprobability of success \n             0.7666667 \n\n\nThere we go! So much easier to do this in R than it is in python. You can just tell that this language was designed with Statistics in mind with how much easier it is to get results."
  },
  {
    "objectID": "posts/qmds/2023-02-03-exploring-fishers-exact-test.html",
    "href": "posts/qmds/2023-02-03-exploring-fishers-exact-test.html",
    "title": "Fisher’s exact test in Python and R",
    "section": "",
    "text": "This is another test which is excellent for testing the Goodness of Fit of data but is more reliable. This is because it is in a class called Exactness Test which means that if the Null Hypothesis is true then you can be sure that your conclusions will be a false positve at most whatever the Power is: 5% in the default case. If you’re not sure what that means then I would recommend doing some further reading on Statistical Power and Confidence Intervals.\nFor us, we’re trying to use this to see if there is an association between two features. If there is then we’d expect to reject the null hypothesis - which is what will happen with our play data.\n\n\nWe’ll get our data from another online class which also includes the Sex along with what their preference was. We need this extra information for this test since at minimum it needs to be a 2x2 cross tabulation. That means we’ll borrow the code from before with an addition:\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ndata <- read_csv(\"https://query.data.world/s/y5x2znfjt6gt5xr2gvg7vyhzqbgjr3\", col_types = 'iff')\n\ndata %>%\n    # format it as expected\n    xtabs(~ Pref + Sex, .)\n\n    Sex\nPref  F  M\n   B 29 17\n   A  2 12\n\n\nIn my previous post about the Chi-Squared, I skipped showing what happens when you push the formula notation into the xtabs function. All it really does is count the total per each category. And, now for the test:\n\ndata %>%\n    # format it as expected\n    xtabs(~ Pref + Sex, .) %>%\n    # pipe into the test for the results.\n    fisher.test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  .\np-value = 0.001877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   1.862023 101.026914\nsample estimates:\nodds ratio \n  9.844814 \n\n\nLooking at this result, we can clearly say that we’re rejecting the null hypothesis and therefore there is an association between the two features.\n\n\n\nAgain, Python is a tad finicky with this working. And again, we’re going to use the Pivot table approach which was used in the previous post about the Chi-Squared test:\n\nimport pandas as pd\nimport scipy as sci\ndata = pd.read_csv('https://query.data.world/s/y5x2znfjt6gt5xr2gvg7vyhzqbgjr3')\n\ndata['Pref'] = data.Pref.astype('category')\nsci.stats.fisher_exact( data.pivot_table(index = 'Pref', columns = 'Sex', aggfunc = len) )\n\nSignificanceResult(statistic=0.09770114942528736, pvalue=0.0018770703240777564)\n\n\nIf you look at the values you’ll see it matches what we got for R further confirming that this was done correctly."
  },
  {
    "objectID": "posts/qmds/2023-01-31-exploring-regression-trees-python-r.html",
    "href": "posts/qmds/2023-01-31-exploring-regression-trees-python-r.html",
    "title": "Regression Trees in R And Python",
    "section": "",
    "text": "Among the lesser known and less common types of models is called a Regression Tree. A Regression Tree is actaully misnomer since it does not in fact use Regression at all when being built but instead uses something called the Standard Deviation Reduction. We’ll define that in a moment because even though all this is important, the ideas of this model is built around a Decision Tree - which requires us to step back even more.\n\n\nA Decision Tree is usually discussed as a sort of flow chart based on the test data being split on a set of features. Then, there is a calculation made - usually Entropy - to see which split generates a better model. Once the split has been made then you get two subtrees which further tries the same thing until the model is built. A very simple pretend example is:\n\n\n\n\nflowchart TB\n  Income[ If Income >50K]\n  Savings[Has Savings]\n  NoSavings[No Savings]\n\n  Income ==>|True|Savings\n  Income ==>|False|NoSavings\n\n\n\n\n\n\n\n\nThe trees are usually much larger - as we’ll see in a moment. There is also a solid amount of customization which goes into a good Decision Tree - but that is not what we’re intending to work with today. Today, we’re going to look at how to make them in R and Python."
  },
  {
    "objectID": "posts/qmds/2023-01-28-exploring-chi-squared-test.html",
    "href": "posts/qmds/2023-01-28-exploring-chi-squared-test.html",
    "title": "Chi Squared - What Is It And How in Python and R",
    "section": "",
    "text": "The purpose of the Pearson Chi-Squared Test is to determine whether the observed data is different than what would be expected. You can see this quite clearly from the actual formula being used; This is one of those moments where the description is less elegant that simply looking at the math: \\[\n\\sum\\frac{(Observed - Expected)^2}{(Expected)}\n\\]\nThis is very commonly used in A/B Testing since the purpose is to test for independence of the different categories. If you’re not familiar with the term, then an A/B Test is when you’re running an experiment and you’re attempting to determine whether changing some specific interaction affects how people use a product. Common examples would be changes in appearance or behavior of buttons on websites and you’ve likely been subjected to this - whether you knew or not.\n\n\nSince I am more familiar with doing this in R than in Python we’ll start here. Thankfully, R has this included by default with little to no requirements. I’m including the tidyverse since the tooling is superior to Base R but it wont have anything to do with the actual test itself.\nWe’ll need some data and thankfully I’ve ferreted this away from an online class I’ve taken previously. It’s stored on a site called Data.World. You are welcome to use this link if you like but it might not work in the future since I’m considering moving the data off the platform and somewhere else.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ndf = read_csv(\n    \"https://query.data.world/s/ccu3fx6fivcq2vdyavd26fhzk5tokq\",\n    col_types='if' # specify the column types are integer, factor.\n)\ndf\n\n# A tibble: 60 × 2\n   Subject Pref \n     <int> <fct>\n 1       1 B    \n 2       2 B    \n 3       3 B    \n 4       4 B    \n 5       5 B    \n 6       6 B    \n 7       7 B    \n 8       8 B    \n 9       9 A    \n10      10 A    \n# … with 50 more rows\n\n\nLooking at the data collect, we can see that we would in fact expect there to be a difference in the categories; it’s pretty obvious from the graph that they’re different by quite a bit:\n\ndf %>%\n    ggplot(aes(x=Pref)) +\n    geom_bar()\n\n\n\n\nThe function chisq.test() expects the class to be of type table so we cannot quite dump anything into it. We’ll need to first format the input how it expects using the function xtabs() and the formuala notation. With the maggitr package, you can emulate the pipe operator like in bash and it makes reading R code so much nicer:\n\n# take the data\ndf %>%\n    # format it as expected\n    xtabs(~ Pref, .) %>%\n    # pipe into the test for the results.\n    chisq.test\n\n\n    Chi-squared test for given probabilities\n\ndata:  .\nX-squared = 17.067, df = 1, p-value = 3.609e-05\n\n\nA reminder that your category will need to be a factor otherwise you’re get an error here. Looking at the results, it is what we expected - namely that the two are not independent and therefore we’d reject the null hypothesis of independence. For us, this is what we want because it means that there is a difference relationship between the two categories.\n\n\n\nFor python, we’ll need the wonderful scipy package which you can get via python3 -m pip install scipy. Doing this in python turns out to be a bit tricky and not as straight forward. What we’ll need to do is make a pivot table aggregating on the length of the index. The .T in pandas just does a transpose and sometimes I do this for readability purposes:\n\nimport pandas as pd\nimport scipy as sp\n\ndf = pd.read_csv('https://query.data.world/s/rqmmkifx4fo2uaymurkmb4ibcu25p4')\nprops = df.pivot_table( index = 'Pref', aggfunc=len).T\nprops\n\nPref      A   B\nSubject  14  46\n\n\nOk, now we’ll run the test and compare this against our results with R:\n\n# Chi-Square test; don't do the pivot else it fails:\nresult = sp.stats.chisquare(\n    df.pivot_table(\n        index = 'Pref', aggfunc=len\n    )\n)\n\n# Chi Statistic, P value\nprint((\n    f\"The Chi-Squared statistic is {round(result.statistic[0],2)}\\\n    and the p-value is {result.pvalue[0]:.3E}.\"))\n\nThe Chi-Squared statistic is 17.07    and the p-value is 3.609E-05.\n\n\nExactly as expected!"
  },
  {
    "objectID": "posts/qmds/2022-12-12-using-mermaid-for-some-fun.html",
    "href": "posts/qmds/2022-12-12-using-mermaid-for-some-fun.html",
    "title": "Playing with Mermaid.js",
    "section": "",
    "text": "My friends and I are playing V Rising on Sundays and we’ve made it to the point where we’re working on Dark Silver.This is an end game resource which is a few levels deep into the game. And, it is also a few levels deep into the resource management of the game. We’ve had to build a new castle further into the game. While setting this, up we were discussing the best way to build a pipeline from basic resources to Dark Silver. I did quick sketch on paper but realized that - thanks to Quarto - I have some extra tooling to play with for this."
  },
  {
    "objectID": "posts/qmds/2022-12-12-using-mermaid-for-some-fun.html#final-graph",
    "href": "posts/qmds/2022-12-12-using-mermaid-for-some-fun.html#final-graph",
    "title": "Playing with Mermaid.js",
    "section": "Final Graph",
    "text": "Final Graph\nflowchart BT\n\n    classDef Furnace fill:#696969, color:#f9f9f9;\n    classDef Grinder fill:#b9b9b9, color:#0a0a0a;\n    classDef Resource fill:#009F4B, color:#000000;\n\n    furnace1[(Furnace)]:::Furnace\n    furnace2[(Furnace)]:::Furnace\n    furnace3[(Furnace)]:::Furnace\n    furnace4[(Furnace)]:::Furnace\n    furnace5[(Furnace)]:::Furnace\n\n    grinder1[(Grinder)]:::Grinder\n    grinder2[(Grinder)]:::Grinder\n\n    darkSilver> Dark Silver]:::Resource\n    copperOre>     Copper Ore]:::Resource\n    silverOre> Silver Ore]:::Resource\n    copper> Copper Ingots]:::Resource\n    stoneDust>  Stone Dust]:::Resource\n    graveDust> Grave Dust]:::Resource\n    quartz> Quartz]:::Resource\n    bones> Bones]:::Resource\n    scourge> Scourge Stone]:::Resource\n    whet> Whetstones]:::Resource\n    glass> Glass]:::Resource\n    stone> Stone]:::Resource\n\n    stone-.->grinder2\n    grinder2-->stoneDust\n\n    copperOre-.->furnace1\n    furnace1-->copper\n\n    quartz-.->furnace5\n    furnace5-->glass\n\n    silverOre-.->furnace4\n\n    bones-.->grinder1\n    grinder1-->graveDust\n    graveDust-->furnace3\n\n\n    copper-->furnace2\n    stoneDust-->furnace2\n    furnace2-->whet\n    whet-->furnace3\n\n\n    glass-->furnace3\n    furnace3-->scourge\n\n    scourge-->furnace4\n    furnace4-->darkSilver\n\n\n\n\nflowchart BT\n\n    classDef Furnace fill:#696969, color:#f9f9f9;\n    classDef Grinder fill:#b9b9b9, color:#0a0a0a;\n    classDef Resource fill:#009F4B, color:#000000;\n\n    furnace1[(Furnace)]:::Furnace\n    furnace2[(Furnace)]:::Furnace\n    furnace3[(Furnace)]:::Furnace\n    furnace4[(Furnace)]:::Furnace\n    furnace5[(Furnace)]:::Furnace\n\n    grinder1[(Grinder)]:::Grinder\n    grinder2[(Grinder)]:::Grinder\n\n    darkSilver> Dark Silver]:::Resource\n    copperOre>     Copper Ore]:::Resource\n    silverOre> Silver Ore]:::Resource\n    copper> Copper Ingots]:::Resource\n    stoneDust>  Stone Dust]:::Resource\n    graveDust> Grave Dust]:::Resource\n    quartz> Quartz]:::Resource\n    bones> Bones]:::Resource\n    scourge> Scourge Stone]:::Resource\n    whet> Whetstones]:::Resource\n    glass> Glass]:::Resource\n    stone> Stone]:::Resource\n\n    stone-.->grinder2\n    grinder2-->stoneDust\n\n    copperOre-.->furnace1\n    furnace1-->copper\n\n    quartz-.->furnace5\n    furnace5-->glass\n\n    silverOre-.->furnace4\n\n    bones-.->grinder1\n    grinder1-->graveDust\n    graveDust-->furnace3\n\n\n    copper-->furnace2\n    stoneDust-->furnace2\n    furnace2-->whet\n    whet-->furnace3\n\n\n    glass-->furnace3\n    furnace3-->scourge\n\n    scourge-->furnace4\n    furnace4-->darkSilver\n\n\n\n\n\n\n\n\nPast this, we could include rates of production for each furnace and start calculating how long each edge takes and naming them. This is starting to feel like Satisfactory and this is really all I needed to share with my friends. Hopefully, you find this library as useful as I am going to."
  },
  {
    "objectID": "posts/qmds/2023-03-15-deeper-dive-into-measuring-metrics.html",
    "href": "posts/qmds/2023-03-15-deeper-dive-into-measuring-metrics.html",
    "title": "Review of Specificity and Sensitivity, Etc",
    "section": "",
    "text": "Measuring Beyond Just Correct\nWorking with many of these Machine Learning Algorithms can sometimes lead to false sense of confidence. Using them has become very easy with software packages so it’s important to step back sometimes and think about how to go beyond simply fitting the model. Commonly, the metric Accuracy is used to measure the success of our models and while we may have an intuitive understanding of what Accuracy means we should have a Mathematical understanding. When we do this, we will find out that there are other metrics around Accuracy which are - in some cases - even more important than Accuracy.\n\n\nSo, Accuracy is…?\nWe will get there but before discussing it, I would like to step back into the past even further. We are going to discuss Contingency Tables since they are very often overlooked in all this. We have become so accustom to using these that we have ceased to think about them. But, a Contingency Table - or Crosstab as the R function prefers - is a table of frequencies for the observed features. With respect to Machine Learning, our observed features are what was predicted vs what was true. But, these are not limited to to these in any way. They can be any sort of observations such as hair color or categories of human temper. This is what was done with the Fisher’s Test and Pearson Chi-Squared in my previous posts. We needed to convert our observations into a Contingency Table for the functions to successfully interpret them.\n\n\nBack to Accuracy\nSo, Accuracy is mathematically calculated as: \\[\nAcc = \\frac{(TP + TN)}{(P_a + N_a)}\n\\] … where: - \\(TP\\) means True Positives  - \\(TF\\) means True Negatives - \\(P_a\\) means All Positives - \\(N_a\\) means All Negatives\n… which is our true predictions over the total amount of observations. This information is often displayed in the Cross Tab discussed above; you may know this often called a Confusion Matrix:\n\nThis is also the correct time to discuss Type I and Type II errors. When a prediction is incorrect, these fall into to possibilities: 1. Prediction was true but reality was false. 2. Prediction was false but reality was true. We call the first case Type I errors. We call the second case Type II errors.\nAnd, now that we have this then we can build out other metrics.\n\n\nSensitivity\nWhen looking at our results, knowing the accuracy is often not enough. Sometimes we’ll want to know how accurate just the positive cases are as sometimes we care more about how often the positive cases are correctly identified. We can use the pieces from the Matrix above to get what we want: \\[\nSensitivity = \\frac{TP}{TP + FN}\n\\] \n\n\nSpecificity\nAnd, sometimes we’ll want to know how accurate just our negative cases are. And, like above, we can use the matrix to filter out the negative cases: \\[\nSpecificity = \\frac{TN}{TN + FP}\n\\]\n\n\n\nConclusions\nThere are plenty of other metircs of these kind on the wikipedia page but these are the most likely to be seen. In fact, this post is mostly a review for my own sake. I often mix these too up due to their similar names. Hopefully, I remember them this time."
  },
  {
    "objectID": "posts/qmds/2023-02-06-exploring-shapiro-wilks-test.html",
    "href": "posts/qmds/2023-02-06-exploring-shapiro-wilks-test.html",
    "title": "Shaprio-Wilk’s Test for Normalicy in Python and R",
    "section": "",
    "text": "Part of the assumptions that you need to confirm before using some of these tests is Normalicy which is a way of saying that the data follows a Normal/Guassian Distribution. When this is true, it looks like the Bell Curve which is posted all over when learning about Statistics and is where most time of a student’s time is spent in Statistics classes. If you’re reading this, then you probably already know what one is but if you don’t then I’d suggest touching a Statistics 101 course; something like Khan Academy or the wikipedia page.\nThis test’s job is to look at the data and attempt to check whether it is not normally distributed.\n\n\nAgain, we’ll be borrowing some data from an online class I’ve taken in the past; this dataset is about usage based on IDE.\n\nimport pandas as pd\nimport scipy as sci\n\ndata = pd.read_csv('https://query.data.world/s/patv4rpu4qipeb4hgggtjen7qh3vdr')\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 40 entries, 0 to 39\nData columns (total 3 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   Subject  40 non-null     int64 \n 1   IDE      40 non-null     object\n 2   Time     40 non-null     int64 \ndtypes: int64(2), object(1)\nmemory usage: 1.1+ KB\n\n\n\ndata.sample(10)\n\n    Subject      IDE  Time\n31       32  Eclipse   362\n30       31  Eclipse   562\n17       18  VStudio   305\n5         6  VStudio   270\n14       15  VStudio   310\n3         4  VStudio   155\n6         7  VStudio   250\n8         9  VStudio   209\n39       40  Eclipse   244\n16       17  VStudio   376\n\n\nThis test expects a single array of numeric data so we’ll want to split on the IDE type since that is what we’re interested in:\n\n#Don't forget to split the IDE column to separate them\nide1 = data.query('IDE == \"VStudio\"')['Time']\nide2 = data.query('IDE == \"Eclipse\"')['Time']\n\nw1, p1 = sci.stats.shapiro( ide1 )\nw2, p2 = sci.stats.shapiro( ide2 )\n\nprint(f\"Visual Studio had a W Stat of {w1:.3} (P={p1:.3E})\", f\"Eclipse had a W Stat of {w2:.2} ((P={p2:.3E})\")\n\nVisual Studio had a W Stat of 0.844 (P=4.191E-03) Eclipse had a W Stat of 0.87 ((P=1.281E-02)\n\n\nWhat we’re looking for is if the W Stat is greater than 1.0 and if it is then we’ll reject the Null Hypothesis. Here, both of them are fine so we fail to reject and grant Normalicy. While this works, I’d rather be able to do this for all numeric columns we are interested in. Then, we can include our Normalicy Check right in a pipeline.\n\n# This slash causes python to ignore the return character at the end\n# then python keeps reading the next line\n# kind of hacky way to get something looking like R pipes\nswStats = data\\\n    .drop('Subject', axis=1)\\\n    .groupby('IDE')\\\n    .apply(lambda x: sci.stats.shapiro(x))\n\nfor ide,stats in swStats.items():\n    print(f\"{ide} had a W Stat of {stats[0]:.3} (P={stats[1]:.3E})\", )\n\nEclipse had a W Stat of 0.872 (P=1.281E-02)\nVStudio had a W Stat of 0.844 (P=4.191E-03)\n\n\nThat is much better!\n\n\n\nOnce again, R is a wonderful language this this is easy to work with. Here is our pipeline R version with the same results:\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ndata <- read_csv(\"https://query.data.world/s/patv4rpu4qipeb4hgggtjen7qh3vdr\")\n\nRows: 40 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): IDE\ndbl (2): Subject, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# skip to the pipeline!\nswState <- data %>%\n    select( IDE, Time) %>%\n    group_by(IDE) %>%\n    summarize(\n        stat = shapiro.test(Time)[['statistic']],\n        pvalue=shapiro.test(Time)[['p.value']])\nswState\n\n# A tibble: 2 × 3\n  IDE      stat  pvalue\n  <chr>   <dbl>   <dbl>\n1 Eclipse 0.872 0.0128 \n2 VStudio 0.844 0.00419\n\n\n\n\n\nThere is some disagreements about whether you should even bother with this test since a QQPlot does the same thing but visually. I touched on the QQ Plot in another post about Apache Spark. Really, it’s up to you and what your needs are but this can be included in a data pipeline and a QQplot cannot so there is at least one point in it’s favor."
  },
  {
    "objectID": "posts/qmds/2023-02-02-exploring-knn-algorithm.html",
    "href": "posts/qmds/2023-02-02-exploring-knn-algorithm.html",
    "title": "K-Nearest Neighbors in Python and R",
    "section": "",
    "text": "This is a supervised learning algorithm to cluster related data together to search for related groups in data. Please note that this is not K-Means and the two get commonly mixed up all the time. I’ll do K-Means at some point in the future as well as how to try to remember the difference later. But, how KNN works is that you tell it how many neighbors (k) the algorithm should consider and then attempts to put the observation into one of the classes based on those accumulated probabilities. If that sounds painful then think of it like this:\n\nTake a single point.\nCheck the [K] points around that point.\nPut it into the most commonly seen class around that point.\nReturn that class.\n\nA forewarning that getting this working in both R and Python was surprisingly annoying for different reasons which we’ll now discuss.\n\n\nAs always, I prefer doing these Machine Learning Algorithms in R since they are easier. In this case, it still was easier but there was a problem getting it converted to the Tidyverse. We’ll get our data which is going to be the Wine data we used before:\n\nlibrary(class)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(rsample)\n# library(gmodels)\n\ndata <- read_csv('https://github.com/stedy/Machine-Learning-with-R-datasets/raw/master/whitewines.csv');\n\nRows: 4898 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (12): fixed acidity, volatile acidity, citric acid, residual sugar, chlo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis algorithm really only works with numeric data and if we’re not careful about what numbers come into the it then we cannot trust our results. When working with numeric data in many of these Machine Learning models we’ll want to normalize each column so that they’re comparable. This is commonly done using this kind of function:\n\nnormalize <- function(x){\n    return (x - min(x) / (max(x) - min(x)))\n}\n\nPlease note that this is not the same thing as the Z-Score you may have seen in Statistics. Although, if the data is all normally distributed then doing that might be better than this method.\nNext, thanks to the wonders of the R programming language, we can apply this to all the numeric columns aside from our response and then attach our response back at the end:\n\ncData <- map( data %>% select( -quality ), normalize) %>% # Normalize the columns\n    as_tibble %>% # convert teh list output to a tibble\n    bind_cols(data %>% select(quality)) # add the response back\n\nThen we’ll do our split and modeling. Notice that - unlike many Machine learning algorithms - you’ll need to pass the testing into the model as well.\n\nsplit <-initial_split(cData)\n\npred <- knn(\n    train = training(split)[-12],\n    test = testing(split)[-12],\n    cl = training(split)[12] %>% as_vector, # Let's talk about this part.\n    k=5\n)\n\nAnd, there we go!\n\n\nBecuase something about how tibbles interact with the knn function causes it to read the dimensions wrong and therefore it refuses to work. If you pass the class labels as a vector then it allows it to function fine. Just keep that in mind if you’re not working in Base R like I usually do not.\n\n\n\n\nOh Python, how much I like you outside of statistics. This was the roughest machine learning algorithm to use so far. There are so many problems to work through so let’s get started.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('https://github.com/stedy/Machine-Learning-with-R-datasets/raw/master/whitewines.csv')\n\nThe first problem is that - like the R version - we’ll want to standardize the inputs. This will require a function like we have needed to before. I’m not a fan of this fit + transform API in python. So, first we’ll import and fit our data:\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(with_mean=True, with_std=True)\nscaler.fit(data);\n\nSadly, we’re not ready to do the modeling since Python is very adamant that this be encoded. And, by that I dont mean a Categorical as we have done in the past. If you try this:\n\nnData = pd.DataFrame(data = scaler.transform(data), columns=data.columns)\ny,X = nData.quality.astype('category'), nData.drop('quality', axis =1)\n\n… then it will still complain and refuse to work. This funtion must not have been updated with consideration to the category type even existing. Which means, we’ll have to use another function to encode the labels. We skipped past this in the other post about Logistic Regression but we’re not able to avoid it here:\n\n# setup to encode the response variable\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder();\n\n# normalize our data:\nnData = pd.DataFrame(data = scaler.transform(data), columns=data.columns)\n\n# encode the labels: \ny = label.fit_transform(nData.quality)\nX = nData.drop('quality', axis = 1)\n\nNow we can actually use the algorithm:\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=421)\n\nknn.fit(X_train, y_train);\nknn.score(X_test, y_test)\n\n0.5542857142857143"
  },
  {
    "objectID": "posts/qmds/2022-12-10-k-anonymity-and-data-privacy.html",
    "href": "posts/qmds/2022-12-10-k-anonymity-and-data-privacy.html",
    "title": "K-Anonymity With Python",
    "section": "",
    "text": "What is K-Anonymity?\nData Privacy is a problem which users and institutions are starting to care more about. Neither users nor companies want to reveal or be liable for some of the data and conclusions that arise from the data collected. One such popular method of trying to mitigate the problem of de-anonymizing individuals in data is K-Anonymity.\nThe idea is that given observations and features in data, knowing this information should not allow one to individually identify someone from that data. Thus, K-Anonymity is a metric which states that data is sufficiently anonymized to the k-1 threshold such that given specific columns in the data there exists no such persons where their arrangement leads to less than k individuals in the group.\nWe’ll use the data from the Wikipedia page to illustrate this idea.\n\nurl = 'https://en.wikipedia.org/wiki/K-anonymity'\nsite = pd.read_html(url)\nrawData = site[0]\nrawData\n\n\n\n\n\n  \n    \n      \n      Name\n      Age\n      Gender\n      Height\n      Weight\n      State of domicile\n      Religion\n      Disease\n    \n  \n  \n    \n      0\n      Ramsha\n      30\n      Female\n      165cm\n      72kg\n      Tamil Nadu\n      Hindu\n      Cancer\n    \n    \n      1\n      Yadu\n      24\n      Female\n      162cm\n      70kg\n      Kerala\n      Hindu\n      Viral infection\n    \n    \n      2\n      Salima\n      28\n      Female\n      170cm\n      68kg\n      Tamil Nadu\n      Muslim\n      Tuberculosis\n    \n    \n      3\n      Sunny\n      27\n      Male\n      170cm\n      75kg\n      Karnataka\n      Parsi\n      No illness\n    \n    \n      4\n      Joan\n      24\n      Female\n      165cm\n      71kg\n      Kerala\n      Christian\n      Heart-related\n    \n    \n      5\n      Bahuksana\n      23\n      Male\n      160cm\n      69kg\n      Karnataka\n      Buddhist\n      Tuberculosis\n    \n    \n      6\n      Rambha\n      19\n      Male\n      167cm\n      85kg\n      Kerala\n      Hindu\n      Cancer\n    \n    \n      7\n      Kishor\n      29\n      Male\n      180cm\n      81kg\n      Karnataka\n      Hindu\n      Heart-related\n    \n    \n      8\n      Johnson\n      17\n      Male\n      175cm\n      79kg\n      Kerala\n      Christian\n      Heart-related\n    \n    \n      9\n      John\n      19\n      Male\n      169cm\n      82kg\n      Kerala\n      Christian\n      Viral infection\n    \n  \n\n\n\n\nOk, so now that we have this information we can see there are Sensitive Attributes which should not show up in public data. So, using this data we can use one single groupby() call to quickly identify an individual user. Obviously, if we have your name then we should drop or Suppress that.\n\nrawData['Name'] = '*'\nrawData\n\n\n\n\n\n  \n    \n      \n      Name\n      Age\n      Gender\n      Height\n      Weight\n      State of domicile\n      Religion\n      Disease\n    \n  \n  \n    \n      0\n      *\n      30\n      Female\n      165cm\n      72kg\n      Tamil Nadu\n      Hindu\n      Cancer\n    \n    \n      1\n      *\n      24\n      Female\n      162cm\n      70kg\n      Kerala\n      Hindu\n      Viral infection\n    \n    \n      2\n      *\n      28\n      Female\n      170cm\n      68kg\n      Tamil Nadu\n      Muslim\n      Tuberculosis\n    \n    \n      3\n      *\n      27\n      Male\n      170cm\n      75kg\n      Karnataka\n      Parsi\n      No illness\n    \n    \n      4\n      *\n      24\n      Female\n      165cm\n      71kg\n      Kerala\n      Christian\n      Heart-related\n    \n    \n      5\n      *\n      23\n      Male\n      160cm\n      69kg\n      Karnataka\n      Buddhist\n      Tuberculosis\n    \n    \n      6\n      *\n      19\n      Male\n      167cm\n      85kg\n      Kerala\n      Hindu\n      Cancer\n    \n    \n      7\n      *\n      29\n      Male\n      180cm\n      81kg\n      Karnataka\n      Hindu\n      Heart-related\n    \n    \n      8\n      *\n      17\n      Male\n      175cm\n      79kg\n      Kerala\n      Christian\n      Heart-related\n    \n    \n      9\n      *\n      19\n      Male\n      169cm\n      82kg\n      Kerala\n      Christian\n      Viral infection\n    \n  \n\n\n\n\nBut also, if I know your age and I know your religion then I can quickly identify you.\n\nrawData.groupby(['Age','Religion']).size().reset_index(name='Count')\n\n\n\n\n\n  \n    \n      \n      Age\n      Religion\n      Count\n    \n  \n  \n    \n      0\n      17\n      Christian\n      1\n    \n    \n      1\n      19\n      Christian\n      1\n    \n    \n      2\n      19\n      Hindu\n      1\n    \n    \n      3\n      23\n      Buddhist\n      1\n    \n    \n      4\n      24\n      Christian\n      1\n    \n    \n      5\n      24\n      Hindu\n      1\n    \n    \n      6\n      27\n      Parsi\n      1\n    \n    \n      7\n      28\n      Muslim\n      1\n    \n    \n      8\n      29\n      Hindu\n      1\n    \n    \n      9\n      30\n      Hindu\n      1\n    \n  \n\n\n\n\nSince we want some sort of representation of age in our analysis, we can Generalize the Age values by binning them. Guidance on the actual bin sizes looks to be trial and error; I guess it is expected to use your own judgement in these methods. For my own guideline I’m going to take the rounded standard deviation and step down until the number of k threshold has been met.\n\nint(rawData['Age'].std())\nrawData['AgeGroup'] = pd.cut(rawData['Age'], bins=int(rawData['Age'].std()))\nrawData.groupby(['AgeGroup']).size().reset_index(name='Count')\n\n\n\n\n\n  \n    \n      \n      AgeGroup\n      Count\n    \n  \n  \n    \n      0\n      (16.987, 20.25]\n      3\n    \n    \n      1\n      (20.25, 23.5]\n      1\n    \n    \n      2\n      (23.5, 26.75]\n      2\n    \n    \n      3\n      (26.75, 30.0]\n      4\n    \n  \n\n\n\n\n…. not enough yet:\n\nrawData['AgeGroup'] = pd.cut(rawData['Age'], bins=int(rawData['Age'].std())-1)\nrawData.groupby(['AgeGroup']).size().reset_index(name='Count')\n\n\n\n\n\n  \n    \n      \n      AgeGroup\n      Count\n    \n  \n  \n    \n      0\n      (16.987, 21.333]\n      3\n    \n    \n      1\n      (21.333, 25.667]\n      3\n    \n    \n      2\n      (25.667, 30.0]\n      4\n    \n  \n\n\n\n\nThat looks good. And, if we take it in combination with another column?\n\nfor col in rawData.columns.to_list():\n    if col not in ['AgeGroup', 'Age']:\n        print(rawData.groupby(['AgeGroup', col]).size().reset_index(name='Count'))\n        print()\n\n           AgeGroup Name  Count\n0  (16.987, 21.333]    *      3\n1  (21.333, 25.667]    *      3\n2    (25.667, 30.0]    *      4\n\n           AgeGroup  Gender  Count\n0  (16.987, 21.333]  Female      0\n1  (16.987, 21.333]    Male      3\n2  (21.333, 25.667]  Female      2\n3  (21.333, 25.667]    Male      1\n4    (25.667, 30.0]  Female      2\n5    (25.667, 30.0]    Male      2\n\n            AgeGroup Height  Count\n0   (16.987, 21.333]  160cm      0\n1   (16.987, 21.333]  162cm      0\n2   (16.987, 21.333]  165cm      0\n3   (16.987, 21.333]  167cm      1\n4   (16.987, 21.333]  169cm      1\n5   (16.987, 21.333]  170cm      0\n6   (16.987, 21.333]  175cm      1\n7   (16.987, 21.333]  180cm      0\n8   (21.333, 25.667]  160cm      1\n9   (21.333, 25.667]  162cm      1\n10  (21.333, 25.667]  165cm      1\n11  (21.333, 25.667]  167cm      0\n12  (21.333, 25.667]  169cm      0\n13  (21.333, 25.667]  170cm      0\n14  (21.333, 25.667]  175cm      0\n15  (21.333, 25.667]  180cm      0\n16    (25.667, 30.0]  160cm      0\n17    (25.667, 30.0]  162cm      0\n18    (25.667, 30.0]  165cm      1\n19    (25.667, 30.0]  167cm      0\n20    (25.667, 30.0]  169cm      0\n21    (25.667, 30.0]  170cm      2\n22    (25.667, 30.0]  175cm      0\n23    (25.667, 30.0]  180cm      1\n\n            AgeGroup Weight  Count\n0   (16.987, 21.333]   68kg      0\n1   (16.987, 21.333]   69kg      0\n2   (16.987, 21.333]   70kg      0\n3   (16.987, 21.333]   71kg      0\n4   (16.987, 21.333]   72kg      0\n5   (16.987, 21.333]   75kg      0\n6   (16.987, 21.333]   79kg      1\n7   (16.987, 21.333]   81kg      0\n8   (16.987, 21.333]   82kg      1\n9   (16.987, 21.333]   85kg      1\n10  (21.333, 25.667]   68kg      0\n11  (21.333, 25.667]   69kg      1\n12  (21.333, 25.667]   70kg      1\n13  (21.333, 25.667]   71kg      1\n14  (21.333, 25.667]   72kg      0\n15  (21.333, 25.667]   75kg      0\n16  (21.333, 25.667]   79kg      0\n17  (21.333, 25.667]   81kg      0\n18  (21.333, 25.667]   82kg      0\n19  (21.333, 25.667]   85kg      0\n20    (25.667, 30.0]   68kg      1\n21    (25.667, 30.0]   69kg      0\n22    (25.667, 30.0]   70kg      0\n23    (25.667, 30.0]   71kg      0\n24    (25.667, 30.0]   72kg      1\n25    (25.667, 30.0]   75kg      1\n26    (25.667, 30.0]   79kg      0\n27    (25.667, 30.0]   81kg      1\n28    (25.667, 30.0]   82kg      0\n29    (25.667, 30.0]   85kg      0\n\n           AgeGroup State of domicile  Count\n0  (16.987, 21.333]         Karnataka      0\n1  (16.987, 21.333]            Kerala      3\n2  (16.987, 21.333]        Tamil Nadu      0\n3  (21.333, 25.667]         Karnataka      1\n4  (21.333, 25.667]            Kerala      2\n5  (21.333, 25.667]        Tamil Nadu      0\n6    (25.667, 30.0]         Karnataka      2\n7    (25.667, 30.0]            Kerala      0\n8    (25.667, 30.0]        Tamil Nadu      2\n\n            AgeGroup   Religion  Count\n0   (16.987, 21.333]   Buddhist      0\n1   (16.987, 21.333]  Christian      2\n2   (16.987, 21.333]      Hindu      1\n3   (16.987, 21.333]     Muslim      0\n4   (16.987, 21.333]      Parsi      0\n5   (21.333, 25.667]   Buddhist      1\n6   (21.333, 25.667]  Christian      1\n7   (21.333, 25.667]      Hindu      1\n8   (21.333, 25.667]     Muslim      0\n9   (21.333, 25.667]      Parsi      0\n10    (25.667, 30.0]   Buddhist      0\n11    (25.667, 30.0]  Christian      0\n12    (25.667, 30.0]      Hindu      2\n13    (25.667, 30.0]     Muslim      1\n14    (25.667, 30.0]      Parsi      1\n\n            AgeGroup          Disease  Count\n0   (16.987, 21.333]           Cancer      1\n1   (16.987, 21.333]    Heart-related      1\n2   (16.987, 21.333]       No illness      0\n3   (16.987, 21.333]     Tuberculosis      0\n4   (16.987, 21.333]  Viral infection      1\n5   (21.333, 25.667]           Cancer      0\n6   (21.333, 25.667]    Heart-related      1\n7   (21.333, 25.667]       No illness      0\n8   (21.333, 25.667]     Tuberculosis      1\n9   (21.333, 25.667]  Viral infection      1\n10    (25.667, 30.0]           Cancer      1\n11    (25.667, 30.0]    Heart-related      1\n12    (25.667, 30.0]       No illness      1\n13    (25.667, 30.0]     Tuberculosis      1\n14    (25.667, 30.0]  Viral infection      0\n\n\n\nOk, looking at these results it is clear that Religion and Disease are problems still. This is where we would need to start asking ourselves if we had enough data and what our analysis’s goals are. If we were trying to explore what features were related to these Diseases then we will need more data since no matter what we do the No illness will always be alone. In this instance we could drop that value:\n\nfilteredData = rawData.loc[ ~rawData.Disease.str.contains('illness')].copy()\nfilteredData.groupby(['AgeGroup', 'Disease']).size().reset_index(name='Count')\n\n\n\n\n\n  \n    \n      \n      AgeGroup\n      Disease\n      Count\n    \n  \n  \n    \n      0\n      (16.987, 21.333]\n      Cancer\n      1\n    \n    \n      1\n      (16.987, 21.333]\n      Heart-related\n      1\n    \n    \n      2\n      (16.987, 21.333]\n      Tuberculosis\n      0\n    \n    \n      3\n      (16.987, 21.333]\n      Viral infection\n      1\n    \n    \n      4\n      (21.333, 25.667]\n      Cancer\n      0\n    \n    \n      5\n      (21.333, 25.667]\n      Heart-related\n      1\n    \n    \n      6\n      (21.333, 25.667]\n      Tuberculosis\n      1\n    \n    \n      7\n      (21.333, 25.667]\n      Viral infection\n      1\n    \n    \n      8\n      (25.667, 30.0]\n      Cancer\n      1\n    \n    \n      9\n      (25.667, 30.0]\n      Heart-related\n      1\n    \n    \n      10\n      (25.667, 30.0]\n      Tuberculosis\n      1\n    \n    \n      11\n      (25.667, 30.0]\n      Viral infection\n      0\n    \n  \n\n\n\n\nNot quite there. So, another technique which will assist us is generalizing the Disease values. We’ll use a substitution heirarchy to replace some values in the data with slightly less accurate values.\n\nheirarchy = {\n    \"Cancer\": \"Body\",\n    'Viral infection': 'Body',\n    'Tuberculosis': 'Chest',\n    'Heart-related': 'Chest'\n}\n\nfilteredData['heirarchy'] = filteredData['Disease'].apply(lambda x: heirarchy[x] if heirarchy.get(x) else x)\nfilteredData.groupby(['AgeGroup','heirarchy']).size().reset_index(name='count')\n\n\n\n\n\n  \n    \n      \n      AgeGroup\n      heirarchy\n      count\n    \n  \n  \n    \n      0\n      (16.987, 21.333]\n      Body\n      2\n    \n    \n      1\n      (16.987, 21.333]\n      Chest\n      1\n    \n    \n      2\n      (21.333, 25.667]\n      Body\n      1\n    \n    \n      3\n      (21.333, 25.667]\n      Chest\n      2\n    \n    \n      4\n      (25.667, 30.0]\n      Body\n      1\n    \n    \n      5\n      (25.667, 30.0]\n      Chest\n      2\n    \n  \n\n\n\n\nAlmost there. Let’s re-examine our age bins and make them one more step smaller.\n\nfilteredData['AgeGroup'] = pd.cut(rawData['Age'], bins=int(rawData['Age'].std())-2)\nfilteredData.groupby(['AgeGroup']).size().reset_index(name='Count')\n\n\n\n\n\n  \n    \n      \n      AgeGroup\n      Count\n    \n  \n  \n    \n      0\n      (16.987, 23.5]\n      4\n    \n    \n      1\n      (23.5, 30.0]\n      5\n    \n  \n\n\n\n\nAnd, there we go! This would be 4-anonymity with respect to Age and Disease. Finally, it is safe to consider using in a public data set or Algorithm. This is a really small dataset but this proces would be how you would anonymize larger datasets as well."
  },
  {
    "objectID": "posts/qmds/hello-qmd/2022-11-28-hello-qmd.html",
    "href": "posts/qmds/hello-qmd/2022-11-28-hello-qmd.html",
    "title": "Adding Quarto Was A Good Decision",
    "section": "",
    "text": "Add This To Your Toolbox\nIf you’ve ever worked with the R programming language then you may have come across RMarkdown. And, if you have come across it but ended up mostly moving to Python - like I have had to - then you likely miss Knitr and this tooling. If you don’t know what I’m talking about then toda y is going to be a good day for you. We’re going to learn how to take what could of been a Jupyter Notebook and convert it to format which is similar in practice but also has excellent support for converting out of Notebooks.\n\n\nWhat is Quarto?\nAccording to the site:  Quarto® is an open-source scientific and technical publishing system built on Pandoc. \nIn simpler language, Quarto is a tool to convert prose and runnable code into more accessible technical documentation: such as PDF files. As someone who is currently working on building an automated report of their Finances, this is so much better than trying to convert a Jupyter Notebook into a PDF. The real problem with that pipeline is that the conversion between the JSON formatting of the Notebook to the PDF is terrible. If you have never done that, I would recommend openning up one of your own notebooks and exporting it to a PDF just so you can see for yourself. And, the problem is that you can no real control over the formatting of the text. This is where Quarto comes in - and we’re going to use some examples on the website to show just how much better this really is.\n\n\nWhat Really is This?\nWe’ll take the example from the site as a base to build off of. Like an RMarkdown or Markdown file, the Quarto Markdown file is a kind of structured formatting shorthand to generate a document. These documents are broken into a Header, Prose and Code Chunks:   \nSo, if we convert the Quarto and the Notebook examples to PDFs - Quarto PDF and the Notebook PDF - then you can quickly see that the Quarto version is much better formatted. And, if you look at the example code, you can see that the code and the prose are more mixed together. This could be an advantage or a disadvantage depending on how you like to work; the output improvements are just superior.\nA note that I ended up with some issues while trying to export the Notebook PDF which you might also run into. nbconvert wants you to install with running this:\nsudo apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic\n… but I found that wasn’t enough and it still could not find pandoc. Since I’m working with the debian system, a simple sudo apt install pandoc was sufficient to solve this and export the PDF file finally.\n\n\nSome highlights and Idea.\nLike I said previously, if you’ve ever wanted to write up actual reports from notebooks then this is wonderful. Something like this could allow you to write an analysis, set it to run automatically and simply email them out to people. No longer would you necessarily need to rely on a Dashboard. And, one disadvantage of a dashboard is that it is not a traditional static file. This is actually an advantage if you wanted to keep those records of tracked changes - such as finances - where you may even require them.\nAnother surprisingly simple and useful builtin for this is graphing. You can simply include flowcharts right into your documents and it just works! Here a flowchart from the site:\n\n\n\n\nflowchart LR\n  A[Get Hungry] --> B(Proceed to Kitchen)\n  B --> C{Decision}\n  C --> D[Taco Day!]\n  C --> E[Steak Day!]\n\n\n\n\n\n\n\n\nI had a fun post idea a while ago to build a graph out of the ghosts in Phasmophobia and now I can do it in a few minutes:\n\n\n\n\n\n\n\nGhosts\n\n Spoopis  \n\nBanshee\n\n Banshee   \n\nDemon\n\n Demon   \n\nJinn\n\n Jinn   \n\nMare\n\n Mare   \n\nOni\n\n Oni   \n\nPhantom\n\n Phantom   \n\nPoltergeist\n\n Poltergeist   \n\nRevenant\n\n Revenant   \n\nShade\n\n Shade   \n\nSpirit\n\n Spirit   \n\nWraith\n\n Wraith   \n\nYurei\n\n Yurei   \n\nFrezing Temperatures\n\n Frezing Temperatures   \n\nFrezing Temperatures->Banshee\n\n    \n\nFrezing Temperatures->Demon\n\n    \n\nFrezing Temperatures->Mare\n\n    \n\nFrezing Temperatures->Phantom\n\n    \n\nFrezing Temperatures->Wraith\n\n    \n\nFrezing Temperatures->Yurei\n\n    \n\nFingerprints\n\n Fingerprints   \n\nFingerprints->Banshee\n\n    \n\nFingerprints->Poltergeist\n\n    \n\nFingerprints->Revenant\n\n    \n\nFingerprints->Spirit\n\n    \n\nFingerprints->Wraith\n\n    \n\nEMF level 5\n\n EMF level 5   \n\nEMF level 5->Banshee\n\n    \n\nEMF level 5->Jinn\n\n    \n\nEMF level 5->Oni\n\n    \n\nEMF level 5->Phantom\n\n    \n\nEMF level 5->Revenant\n\n    \n\nSpirit Box\n\n Spirit Box   \n\nSpirit Box->Demon\n\n    \n\nSpirit Box->Jinn\n\n    \n\nSpirit Box->Mare\n\n    \n\nSpirit Box->Oni\n\n    \n\nSpirit Box->Poltergeist\n\n    \n\nSpirit Box->Spirit\n\n    \n\nSpirit Box->Wraith\n\n    \n\nGhost Writing\n\n Ghost Writing   \n\nGhost Writing->Demon\n\n    \n\nGhost Writing->Oni\n\n    \n\nGhost Writing->Revenant\n\n    \n\nGhost Writing->Shade\n\n    \n\nGhost Writing->Spirit\n\n    \n\nGhost Writing->Yurei\n\n    \n\nGhost Orb\n\n Ghost Orb   \n\nGhost Orb->Jinn\n\n    \n\nGhost Orb->Mare\n\n    \n\nGhost Orb->Phantom\n\n    \n\nGhost Orb->Poltergeist\n\n    \n\nGhost Orb->Shade\n\n    \n\nGhost Orb->Yurei\n\n   \n\n\n\n\n\n\n\nConclusions\nI cannot wait to start using this at work and in my personal projects. There is so much potential for this tool."
  },
  {
    "objectID": "posts/qmds/2023-01-28-exploring-linear-regression.html",
    "href": "posts/qmds/2023-01-28-exploring-linear-regression.html",
    "title": "Exploring Linear Regression in R and Python",
    "section": "",
    "text": "Linear Regression is a reliable and common method of measuring the relationship between numeric variables in data. The simplest usage is over two numeric variables and attempts to draw a line between all the available points. This is a bit of an oversimplification since you can also do categorical variables - at least for R, anyways - but this post is about the the How and not the Why.\n\n\nMany languages have packages to solve problems like this for us and Python is no exception. The pacakges for this in Python are scikit-learn and statsmodels. Mostly commonly, you will see scikit-learn so we’ll talk about that first. To install it, you’ll want to run python3 -m pip install scikit-learn.\nMost of the models you’ll want to build will come standard but you’ll need to use the documentation to find which one you’re after. We’ll be importing from linear_model to get LinearRegression:\n\n# Grab this here:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nimport seaborn as sns\n\nNext we’ll get the data from the the seaborn package since it conveniently contains common datasets - which we’ll be using now:\n\n\n# Get the data for both:\niris = sns.load_dataset('iris')\nX = iris.sepal_length\ny = iris.petal_length\n\n# Normally, you'd want to do this but I'm ignoring it for the purpose of example\n# Set the size of the split based on your needs\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.8, random_state=1111)\n\nFitting a model in scikit-learn is extremly easy. All you need to do is initialize the model object, fit the model using the cleaned data and then run predictions.\nlm = LinearRegression()\n# This is one sample so you'll need to reshape; will throw error otherwise.\nlm.fit(X.values.reshape(-1,1), y);\nFinally, you can get the slope and the intercept of the line which is often what you’ll be after; we’ll need it for comparison later.\nintercept, slope = lm.intercept_, lm.coef_[0]\nprint(f\"The Slope and Intercept of the line are: Slope is {round(slope,2)} and Intercept is {round(intercept, 2)}\")\nThe Slope and Intercept of the line are: Slope is 1.86 and Intercept is -7.1\n\n\n\nR is a language which is built around Statistics and as such much of the statistical tooling is built right into the default langauge. To build a Linear Model in R, one would use the lm() function passing using formula notation. If you’ve never seen this before, R has something called Formula Notation which allows you to specify a relationship in simple text and then gets parsed into meaningful input for the function. So, if we want to define a relationship between x and y then we’d say y ~ x. There are other choices for these as well as different functions using this notation in different ways but this is how we’re going to use it here.\nAsking for a Linear Regression Model is as simple as:\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.3.0      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nmodel = lm(Petal.Length ~ Sepal.Length, data = iris)\nprint(model)\n\n\nCall:\nlm(formula = Petal.Length ~ Sepal.Length, data = iris)\n\nCoefficients:\n (Intercept)  Sepal.Length  \n      -7.101         1.858  \n\nsummary(model)\n\n\nCall:\nlm(formula = Petal.Length ~ Sepal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.47747 -0.59072 -0.00668  0.60484  2.49512 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.10144    0.50666  -14.02   <2e-16 ***\nSepal.Length  1.85843    0.08586   21.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8678 on 148 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\nNotice that we get the same Slope and Intercept as we did in Python. When transferring across languages, it’s a good idea to use what you know to check against what is new for you.\n\n\n\nIf you’re more comfortable with the R notation - like I happen to be - then using the statsmodel package is the way to go. This allows you to use the formula notation instead of the Object Oriented way that scikit-learn does. Since we’re already covered the forumula notation then we don’t have to review that; you’ll want the API interface for formulas but otherwise it’s the same as R.\n\nimport statsmodels.formula.api as smf\nresults = smf.ols('petal_length ~ sepal_length', data=iris).fit()\nresults.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:      petal_length     R-squared:             0.760\n\n\n  Model:                   OLS         Adj. R-squared:        0.758\n\n\n  Method:             Least Squares    F-statistic:           468.6\n\n\n  Date:             Sat, 28 Jan 2023   Prob (F-statistic): 1.04e-47\n\n\n  Time:                 12:53:11       Log-Likelihood:      -190.57\n\n\n  No. Observations:         150        AIC:                   385.1\n\n\n  Df Residuals:             148        BIC:                   391.2\n\n\n  Df Model:                   1                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                  coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept       -7.1014     0.507   -14.016  0.000    -8.103    -6.100\n\n\n  sepal_length     1.8584     0.086    21.646  0.000     1.689     2.028\n\n\n\n\n  Omnibus:        0.253   Durbin-Watson:         1.204\n\n\n  Prob(Omnibus):  0.881   Jarque-Bera (JB):      0.386\n\n\n  Skew:          -0.082   Prob(JB):              0.824\n\n\n  Kurtosis:       2.812   Cond. No.               43.4\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nI really like this output and also because you can simply pull out the parts you need using dot notation:\n\nresults.params\n\nIntercept      -7.101443\nsepal_length    1.858433\ndtype: float64"
  },
  {
    "objectID": "posts/qmds/2022-12-07-using-recursion-for-pmt.html",
    "href": "posts/qmds/2022-12-07-using-recursion-for-pmt.html",
    "title": "Calculating Loan Payments - Recursively",
    "section": "",
    "text": "I was reviewing the classwork from the Financial Planner course to apply to the real world. Namely, I was using the calculation of the Loan Payments for a loan that I currently have. And, while working through this problem, I realized that I didn’t want to calculate each payment but what I wanted was how many periods I would have left if I modifed my monthly payment to the loan."
  },
  {
    "objectID": "posts/qmds/2022-12-07-using-recursion-for-pmt.html#a-word-on-python",
    "href": "posts/qmds/2022-12-07-using-recursion-for-pmt.html#a-word-on-python",
    "title": "Calculating Loan Payments - Recursively",
    "section": "A Word on Python",
    "text": "A Word on Python\nIf you try to do this in python, then you’ll find that there is quite a bit of old documentation around. Per the release notes, the financial fucntions which used to be in numpy have been moved to a different external package called numpy-financial and you can find it’s documentation here. This package has not changed in a while but these functions are pretty timeless so I’m not too worried about it. Anyways, you can install it with pip install numpy-financial or python3 -m pip install numpy-financial. Once done, you can run the example below:\n\nimport numpy_financial as npf\n\n# No, this is not my loan; this is fake data:\nperiods = 120\nAIR = .03 # Annual Interest Rate\nMIR = AIR/12\nloanAmount = 300000\n\npayment = abs(round(npf.pmt(MIR, periods, loanAmount), 2))\n\nThe documentation about the function points out that it is solving the equation: \\[\nF_{v}+P_{v}(1 + rate)^{nper}+\\frac{Pmt(1 + rate*when)}{rate((1 + rate)^{nper} -1)} == 0\n\\] Where:  - Fv is the Future Value.  - Pv is the Present Value.  - rate is the Period Interest Rate; monthly for us.  - nper is the Number of Periods  - pmt is the Payment Amount.  - when is which part of the period is the payment made. \n\n\nSince it is not made explicit, the source code for the function defaults to when = 'end'. That is a lot to take in but what we really care about is the result: $2896.82"
  },
  {
    "objectID": "posts/qmds/logisitc-regression/2023-02-02-exploring-logistic-regression.html",
    "href": "posts/qmds/logisitc-regression/2023-02-02-exploring-logistic-regression.html",
    "title": "Logistic Regression in Python and R",
    "section": "",
    "text": "Instead of predicting the actual values like Linear Regression, Logistic Regression predicts the probability that an outcome belongs to a specific category. Note that the default assumption for this is Binomial - meaning between two different classes. You can also select multiple classes but will require a different package which we wont be discussing today.\nThis is a simple and common way to solve Classification problems which we’ll be looking at next.\n\n\nLike most model problems, we’ll be falling back to using the sklearn package. We’ll initialize the LogisiticRegression() object, drop the columns we will be ignoring and then get our results:\n\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n# I got this from an online class:\ndf = pd.read_csv(\"_data/loan.csv\")\nlogit = LogisticRegression()\n\n\n# Columns we care about:\ndf = df[['loan_default', 'loan_amount', 'debt_to_income', 'annual_income']]\n# Split the data apart:\nX,y = df.drop('loan_default', axis=1), df.loan_default.astype('category')\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=421)\nlogit.fit(X_train, y_train);\n\n… and we can check how well the model fits:\n\nlogit.score(X_test, y_test)\n\n0.5963302752293578\n\n\nYou may see some posts saying you can do a One Hot Endcode using the LabelEncoder() but with the category type in Pandas you shouldn’t need this. This is because the Categorical data type is the equivalent of a Factor in R per the Documentation:\n\nAll values of categorical data are either in categories or np.nan. Order is defined by the order of categories, not lexical order of the values. Internally, the data structure consists of a categories array and an integer array of codes which point to the real value in the categories array. cf: Docs\n\nWe’ll step through it anyways just to show that this ends up the same. We’ll import, convert and then give ths score below:\n\n# generate encoder\nle = preprocessing.LabelEncoder();\n\n# \"fit\" the column to the encoder:\nle.fit(df['loan_default']);\n\n\n# convert the column for the response:\ny_2 = le.transform(df['loan_default']);\n\n# Same split as normal:\nX_train, X_test, y_train, y_test = train_test_split(X,y_2, random_state=421)\n\nlogit.fit(X_train, y_train);\n\nAnd, the score:\n\nlogit.score(X_test, y_test)\n\n0.5963302752293578\n\n\nAnd, it’s the same so skip it and just use categories.\n\n\n\nAdding to the list of reasons I like using R more, this is how simple it is.\n\n# install.packages(c('ISLR', 'caret'))\nlibrary(tidyverse)\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\ndefault = read_csv('_data/loan.csv')\n\nRows: 872 Columns: 8\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): loan_default, loan_purpose, missed_payment_2_yr\ndbl (5): loan_amount, interest_rate, installment, annual_income, debt_to_income\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary( default )\n\n loan_default       loan_purpose       missed_payment_2_yr  loan_amount   \n Length:872         Length:872         Length:872          Min.   : 1000  \n Class :character   Class :character   Class :character    1st Qu.:10000  \n Mode  :character   Mode  :character   Mode  :character    Median :15700  \n                                                           Mean   :17469  \n                                                           3rd Qu.:25000  \n                                                           Max.   :40000  \n interest_rate     installment      annual_income    debt_to_income  \n Min.   : 4.720   Min.   :  36.19   Min.   :  3120   Min.   :  0.00  \n 1st Qu.: 7.492   1st Qu.: 279.48   1st Qu.: 46000   1st Qu.: 11.37  \n Median :10.220   Median : 451.46   Median : 67000   Median : 17.80  \n Mean   :10.762   Mean   : 517.35   Mean   : 79334   Mean   : 19.59  \n 3rd Qu.:13.250   3rd Qu.: 737.44   3rd Qu.: 96000   3rd Qu.: 25.60  \n Max.   :20.000   Max.   :1566.59   Max.   :780000   Max.   :215.38  \n\ndefault %>% head\n\n# A tibble: 6 × 8\n  loan_default loan_purpose      misse…¹ loan_…² inter…³ insta…⁴ annua…⁵ debt_…⁶\n  <chr>        <chr>             <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 no           debt_consolidati… no        25000    5.47    855.   62823   39.4 \n2 yes          medical           no        10000   10.2     364.   40000   24.1 \n3 no           small_business    no        13000    6.22    442.   65000   14.0 \n4 no           small_business    no        36000    5.97   1152.  125000    8.09\n5 yes          small_business    yes       12000   11.8     308.   65000   20.1 \n6 yes          medical           no        13000   13.2     333.   87000   18.4 \n# … with abbreviated variable names ¹​missed_payment_2_yr, ²​loan_amount,\n#   ³​interest_rate, ⁴​installment, ⁵​annual_income, ⁶​debt_to_income\n\nlog_model <- default %>%\n    mutate(did_default=as.factor(loan_default)) %>%\n    glm(\n        did_default ~ loan_amount + debt_to_income + annual_income,\n        data = .,\n        family = binomial\n    )\n\nsummary( log_model )\n\n\nCall:\nglm(formula = did_default ~ loan_amount + debt_to_income + annual_income, \n    family = binomial, data = .)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.3610  -0.9659  -0.8107   1.2511   3.0721  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -1.033e+00  2.155e-01  -4.796 1.62e-06 ***\nloan_amount     2.518e-05  7.869e-06   3.200  0.00137 ** \ndebt_to_income  2.898e-02  6.915e-03   4.192 2.77e-05 ***\nannual_income  -5.783e-06  1.885e-06  -3.068  0.00215 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1163.5  on 871  degrees of freedom\nResidual deviance: 1113.9  on 868  degrees of freedom\nAIC: 1121.9\n\nNumber of Fisher Scoring iterations: 4\n\n\nThere we go! Simple and easy as always."
  },
  {
    "objectID": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html",
    "href": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html",
    "title": "Credit Card Security",
    "section": "",
    "text": "Everyone is worried about having their credit card information stolen at some point. We all trust the banks to ensure nobody can just guess our number and submit orders in our name. But, did you know math is the secret behind your security? There are four main issues with stealing a credit card: 1. Getting the Credit Card Number 2. Getting the Security Code 3. Getting the Expiration Date 4. Getting your Personal Details"
  },
  {
    "objectID": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-credit-card-number",
    "href": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-credit-card-number",
    "title": "Credit Card Security",
    "section": "Getting the Credit Card Number",
    "text": "Getting the Credit Card Number\nThere are a total of 10000000000000000 total numbers that can exist for a credit card number. The odds, therefore, to get your number is 1:10000000000000000. But, it’s worse than that because the entire collection of numbers are not used to generate credit cards. In fact, credit cards can pass or fail passed on something called Luhn’s Algorithm This algorithm is used to create what is called a subgroup inside of the total number of cards. We don’t want criminals to just guess numbers and accidentally hit anyone. This is formula is a first line of defense to ensure fake numbers cannot be submitted.\nALGORITHM_CONSTANT = 9\n\ndef confirmLuhn(number):                                             #==== Luhn's Algo ==========================\n    count = 0                                                    # Sum all numbers excluding the last number.\n    sum   = 0                                                    # For every other number, square it.\n                                                                     # Compare the last digit of the CC number\n    for digit in number:                                         # against the last digit of the sum.\n        num = int(digit)\n        if   count == 15:                                    # If final digit,\n            return (sum *ALGORITHM_CONSTANT %10 == num)  # compare final digits.\n        elif count % 2 == 1:                                 # If off number,\n            sum   += num*num                             # square number.\n            count +=1\n        else:                                                # If not off number,\n            sum   += num                                 # just add the number to the sum\n            count += 1                                   #===========================================\n\n\noutfile = open(\"CCNumbers.txt\", 'w')                          # File opening, obviously.\n                                                              # First we generate the number space that we're testing.\nfor x in range(0, 0000000000000100):                          # The real CC number space is actually [0, 9999999999999999], but that's too much.\n    xString = \"%.16d\" % x                                 # This is probably lazy design, but I want to fill out the space of numbers and keep the 0's\n    if confirmLuhn(xString): outfile.write(xString+\"n\")  # If it's one of them, then write the result to the file.\n\noutfile.close()                                               # BILLIONS OF YEARS OF WORK AND YOU FORGOT TO CLOSE THE STORAGE FILE IN THE CODE?!?\nI wrote a basic CC number finder that uses a very small space of numbers to check against as existing numbers - and, you can find it herefor those wishing to pull it and build on it."
  },
  {
    "objectID": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-security-code",
    "href": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-security-code",
    "title": "Credit Card Security",
    "section": "Getting the Security Code",
    "text": "Getting the Security Code\nThe probability of getting the 3 digit number on the back of that card is 1:1000. That might not seem like a lot, and it’s not, but without that number you can’t use the card online. And, most banks will lock your card after a few failed tries. If I can see and memorize the 16 digit Credit Card number from seeing once - this isn’t all that hard, it just takes a bit of practice - this 1:1000 probability is what keeps them from using it online without your consent.\nLet’s assume that I did memorize your number and I’m trying to figure out what that code is. Most banks I know of will lock a hard after 5 failed attempts. That means I have a 5:1000 or 1:200 chance of guessing right before the card number becomes useless. Of course, that’s completely ignoring if I don’t have the number and am just trying to guess everything. Even if I beat the 1:10000000000000000 chance of picking the right number and it passes the Luhn Algorithm then I still need to guess with a 1:1000 chance of getting the right number within each potential number. But, it gets even worse for me."
  },
  {
    "objectID": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-expiration-date",
    "href": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-expiration-date",
    "title": "Credit Card Security",
    "section": "Getting the Expiration Date",
    "text": "Getting the Expiration Date\nI still have to get past your expiration date. There really is no good way to know the actual probability since banks can use any number of years before the card expires. But, I’ll assume a larger case to overestimate since banks will make the task as hard as possible. Let’s guess that the range of expiration dates is from 2000 - 2030 since some cards are in the wild. We’re looking at a 1:360 chance - which is really tiny when you compare it to other probability spaces. But, we need to think about the entire probability space of 1:10000000000000000 * 1:200 * 1:360 just to find the right card. To select the right card data of just you is 1:720,000,000,000,000,000,000. For comparison, that’s about 6 multiples large than the estimated total of every human being ever alive.\nBut, what if I memorized all of this information?"
  },
  {
    "objectID": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-your-personal-details",
    "href": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-your-personal-details",
    "title": "Credit Card Security",
    "section": "Getting your Personal Details",
    "text": "Getting your Personal Details\nEven after selecting the right credit card with the right security code and the right expiration date you still need to know the precise living address of the person you’re trying to attack to submit orders online. No wonder credit card scammers steal them with skimmers or fake online bank pages! Math wins."
  },
  {
    "objectID": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html",
    "href": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html",
    "title": "Simple Introduction to Python Debugging",
    "section": "",
    "text": "While working on a script that I’m writing for work, I was having trouble really tracking down where my logic was failing. After looking over the code, everything looked correct but the results were showing me otherwise. So, I opened up python’s debugging system and started to dig in."
  },
  {
    "objectID": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html#what-is-debugging",
    "href": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html#what-is-debugging",
    "title": "Simple Introduction to Python Debugging",
    "section": "What is Debugging?",
    "text": "What is Debugging?\n\nIn computer programming and software development, debugging is the process of finding and resolving bugs (defects or problems that prevent correct operation) within computer programs, software, or systems. cf: https://en.wikipedia.org/wiki/Debugging\n\nThere are plenty of tools to do stuff like this but Python comes with its own: pdb — The Python Debugger. To activate it, you’d simply go into your terminal and type:\nuser@station# python3 -m pdb <script-name>\nThis with start the script from the top and immediately stop before running anything:\nuser@station:/scripts# python3 -m pdb keyIntegrityCheck.py\n> /scripts/keyIntegrityCheck.py(5)<module>()\n-> from pathlib import Path\n(Pdb)\nWhile this is useful, we need to be able to explore the file. You could keep an editor open on a different monitor and move around but sometimes that’s not going to be an option. To print all the lines in the file you’d use:\n(Pdb) ll                                                                                    \n1     #!/usr/bin/env python3                                        \n2     ## Author: Collin Mitchell                                 \n3     ## Purpose: To check the integrity of keys without user input.  \n4                                                                               \n5  -> from pathlib import Path                                                   \n6     from itertools import filterfalse                                            \n7     import subprocess as sp                                                          \n8     import json                                                                     \n9     import sys    \n# ...\n370                 integrityFunc = dd.get(str(filename).split('.')[-1])\n371                 if integrityFunc:\n372                     consistent = integrityFunc( filename )\n373                     if not consistent: print(\"{} is not consistent.\".format(filename))\n374                 else:\n375                     print(\"{} is missing a checker; please report {} so it can be added.\".format(filename,filename))\n(Pdb)\nYou can see the pointer on line five telling us where the current execution point is. We can move the pointer a single line using n:\n(Pdb) n                                                                                                                                                                   [336/1995]\n> /scripts/keyIntegrityCheck.py(6)<module>()\n-> from itertools import filterfalse                   \n(Pdb) ll                                                                                    \n  1     #!/usr/bin/env python3                                        \n  2     ## Author: Collin Mitchell                                 \n  3     ## Purpose: To check the integrity of keys without user input.  \n  4                                                                               \n  5     from pathlib import Path                                                   \n  6  -> from itertools import filterfalse                                            \n  7     import subprocess as sp                                                          \n  8     import json                                                                     \n  9     import sys     \nYou can see the next 11 lines in the console buffer using l:\n(Pdb) l\n  1     #!/usr/bin/env python3\n  2     ## Author: Collin Mitchell\n  3     ## Purpose: To check the integrity of keys without user input.\n  4  \n  5     from pathlib import Path\n  6  -> from itertools import filterfalse\n  7     import subprocess as sp\n  8     import json\n  9     import sys\n 10  \n 11     # stuff to do with phpserialize:\n(Pdb)\nNot that if you do this and run it again that you wont get the same result:\n(Pdb) l\n 12     import codecs\n 13     try:\n 14         codecs.lookup_error('surrogateescape')\n 15         default_errors = 'surrogateescape'\n 16     except LookupError:\n 17         default_errors = 'strict'\n 18     try:\n 19         xrange\n 20     except NameError:\n 21         xrange = range\n 22     try:\n(Pdb)\n… but you will get the next 11 lines instead. You can tell it which lines to list centered on a line number using l 10:\n(Pdb) l 10\n  5     from pathlib import Path\n  6  -> from itertools import filterfalse\n  7     import subprocess as sp\n  8     import json\n  9     import sys\n 10  \n 11     # stuff to do with phpserialize:\n 12     import codecs\n 13     try:\n 14         codecs.lookup_error('surrogateescape')\n 15         default_errors = 'surrogateescape'\n(Pdb)\nNow that we can move around, let’s discuss how to actually stop code execution using breakpoints. These are locations you set - sometimes with conditions - to stop the code execution and explore the current state. You can set these using ‘b’:\n(Pdb) b\n(Pdb)\nSince we don’t have any breakpoints set, then it makes sense we don’t see any listed. So, now lets set one:\n(Pdb) b 9\nBreakpoint 1 at /scripts/keyIntegrityCheck.py:9\n(Pdb)\n… and continue execution until the breakpoint using c:\n(Pdb) c\n> /scripts/keyIntegrityCheck.py(9)<module>()\n-> import sys\n(Pdb)\n… and list the active breakpoints again:\n(Pdb) b\nNum Type         Disp Enb   Where\n1   breakpoint   keep yes   at /scripts/keyIntegrityCheck.py:9\n        breakpoint already hit 1 time\n(Pdb)\nYou can clear that breakpoint using the number of the breakpoint as well:\n(Pdb) b\nNum Type         Disp Enb   Where\n1   breakpoint   keep yes   at /scripts/keyIntegrityCheck.py:9\n        breakpoint already hit 1 time\n(Pdb) cl 1\nDeleted breakpoint 1 at /scripts/keyIntegrityCheck.py:9\n(Pdb) b\n(Pdb)\nConditional breakpoints I found a bit tricky to get to work correctly because you need to place a comma after the statement:\n(Pdb) b 371, integrityFunc.__name__ == 'integrityPhp'\nBreakpoint 3 at /scripts/keyIntegrityCheck.py:371\n(Pdb)\n… and then you c until it triggers:\n> /scripts/keyIntegrityCheck.py(371)<module>()\n-> if integrityFunc:\n(Pdb) integrityFunc\n<function integrityPhp at 0x7f99291477b8>\n(Pdb)"
  },
  {
    "objectID": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html#conclusion",
    "href": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html#conclusion",
    "title": "Simple Introduction to Python Debugging",
    "section": "Conclusion",
    "text": "Conclusion\nI couldn’t find much outside of the official documentation when I needed it so hopefully you find the highlights useful. There are also tools for whichever IDE you’re using so look out for those as well."
  },
  {
    "objectID": "posts/Mds/2022-11-26-moving-to-quatro-from-fastpages.html",
    "href": "posts/Mds/2022-11-26-moving-to-quatro-from-fastpages.html",
    "title": "Migrating From Fastpages to Quarto",
    "section": "",
    "text": "For those unaware - or not following what Jeremy and his team produce -, Fastai also produced something called Fastpages which is intended to help creators in the Machine Learning Space Blog about their work. What was so useful about how this worked is that you could very easily explore a problem in a Jupyter Notebook only to clean it up and have the tooling produce a blog post: code and all. But, since it also supported other formats - such as Markdown documents, Word documents - you could really just use it as a general blogging tool as well. They made setting this all up easy so one could focus on writing and exploring instead of fighting with template engines or other tools. I have dabbled a bit with other technologies for blogging - a moment of word press, a few flask apps, and a bit of Django - but they were always about prose and not code. There was also a large amount of copying and pasting from editors to code blocks; not to mention trying to get the output from the runs showing in some meaningful way. While looking over quite a few of these I had started to consider actually writing my own just becuase there was such an annoying gap here.\nThankfully, Fastpages was born and it was exactly what I was looking for in a blogging platform. The original version of this blog was using fastpages and I was having good success slowly getting used to working through problems or ideas and then getting them online. Only to find out one day that it was being depreciated for a different tool: Quarto. Quarto’s purpose is about creating documents in general as opposed to simply blog posts. But, that’s not a loss so long as the blogging allows me to still write Jupyter Notebooks and have them converted to blog posts. Unsurprisingly, it does do this and so and to quote the disclaimer from Fastpages:\n\nEven though I created fastpages, I believe that people trying to blog with notebooks should be using Quarto. I am a believer in recommending the best tools for the job and helping people use those tools where possible. To that end, I have created this migration guide 153 that can help you migrate your fastpages projects to Quarto, along with other Quarto resources that might be helpful. I did a quick trial run to see how the migration would go: it did not go.\n\n\n\nRight from the start, this is not being built to be used across distributions. I am running Manajro linux - which is based on Arch Linux - and the install tries to install a debian package using apt. Not going to work here. Obviously. With this being as new as it is, I didn’t expect there to be support for it on the AUR but someone did in fact go through the trouble of getting it up there. But, sadly, as they note on that page:\n\ntitaniumbones: Getting a dependency error when trying to install; dependency deno<1.23.0 leads yay to install deno-bin from AUR (version 1.20), but it appears that deno-bin doesn’t actually fulfill the dependency:\n\n\ntrap000d: There is no solution for this yet, until quarto devs manage to move to latest Deno. Either downgrade deno to 22.3… or patch quarto sources.\n\nThe issue has not moved in months at this point and I need this working now. These posts are - while fun - proof that I can use these technologies and I cannot wait an indeterminate number of maybe months for this blog to continue. And, Fastpages was showing some annoying behaviors - really ugly slugs for the urls - which I also needed fixed.  Well, I can always just setup a Debian Based Virtual Machine and post from that; so, that’s what I did.\n\n\n\nIf I want to experiment and write post about more cutting edge ideas then I will need access to my Video Card in this computer. There are a few ways to work around this: 1. Just don’t do them. 2. Use a Service like Collab; Download the notebooks. 3. GPU Passthrough to the VM. 4. Build another computer just for Blogging. 5. Network sharing the same Directory.\nOption One I included for completeness but I refuse to not post about these.  Option Two is passable since a Jupyter Notebook is basically just a JSON document so we could use a Cloud Service, explore and clean it up, download it and then save it to post. This is annoying and would slow down the interation process too much. Plus, I want to use my own Video Card that I have already spent money on for this. This is worst case but at least it could work.  Option Three is viable but risky. I have never actually done GPU passthrough and both systems would need to share the Video Card which could cause all sorts of weird issues. If I kept the Virtual Machine online and was playing a Video Game with my friends then I expect to have problems. And, that means constantly spinning the VM up and down as I need it. This is quite a lot of maintenance for writing posts. Again, this might work but with this many problems to accidentily trip over it’s risky at best and not reliable at worst.  Option Four is just silly; And, with Video Card prices what they are there no way I’m building a second computer just for this blog. Nope. \nOption Five is what I went with. I can do all the work on my computer but save the actually outputs directly into the repository to commit and upload. This allows me to also keep data on my own computer and thus ignore the size constraints of data. It would also let me work on any computer I wanted - for example - my laptop is also Arch and also has a Video card so I could write posts from that as well. The VM could stay up as long as I wanted, I could even migrate it to other computers if need be and it would affect literally nothing. The only real maintenace will be the mounting remotely of the directory which is a very well solved problem.\n\n\n\nSo, I have a Debian Virtual Machine now. There are some packages that I both wanted and needed for this to work: * sshfs to share the folders. * sshd to allow access remotely. * vscodium to write posts with: locally and remotely. * xpra to host remote windows. * I just like vim.\nLuckily, sshfs and sshd are installed by default so we wont need to worry about them. I ended up needing to install curl since it’s apparently not here by default; installed with apt install curl since this is a Debian system. So, I don’t tend to use raw jupyter notebooks but instead use a derivative of Visual Studio code called vscodium. I just happen to be comformtable with using this and it supports notebooks with a few extensions. It also has a Quatro extension which is useful for some of the publication parts I could use later.\nAlways setup passwordless ssh access between systems when you setup a new system. There is a useful new way to do all of this via console and there are good guides already for this. One little caveat which bit me is I have quite a few of these now and so I name them when I make them. If you organize your .pub files - like I do - make sure you not move the .pub until after you’ve ran ssh-copy-id <username>@<ip-address>. If you move it before then the command doens’t find it; I tried to pass it manually but it just wouldn’t take.\nxpra is a useful tool which allows for Remote Window control. It allows you to start a program as a daemon on any system, connect remotely to that system to get the remote window while it is running. If you’re familiar with X11 Forwarding then this will not be a new idea. This is simply better though since you can disconnect from the remote system at any time, go do something else and then re-attach later exactly where you were before. I use this at work often to conceptually break computers apart: this one is for Deep Learning and Gaming, this one is for Work related only, this is for Server Hosting, etc. Once it’s installed, I started the editor on the host with xpra start :100 --start=codium to start the daemon and connect to the editor remotely from my desktop using xpra attach ssh:<ip-address>:100 to open the editor on my own workstation. Great! Now I can setup the new repository to host the files, open it via the editor on the remote system.\nNext I have to connect a folder locally to the VM which is easy using sshfs; this creates a fuse mount via an ssh tunnel to mimic the remote folder locally. You could also do this with samba but I find sshfs much easier and faster to setup and use. I made a directory as a mount point - Blog - and then mounted it via sshfs blog:<path-to-code> Blog so that I can work locally and read/write to repository.\n\n\n\nThe Document provided made this really easy; none of the steps failed me. One of my posts had an extension .MD instead of .md which caused the post to fail to convert but that’s about it. I updated the default them, built out a drafts folder for posts in the future and everything just kind of worked. I’ve been working through fixing the image paths on a few of my posts but otherwise this is really an improvement.  Expect more posts in the future and thanks for the new tool."
  },
  {
    "objectID": "posts/Mds/2020-09-18-Game-Review-Empyrion-Galactic-Survival.html",
    "href": "posts/Mds/2020-09-18-Game-Review-Empyrion-Galactic-Survival.html",
    "title": "Game Review: Empyrion - Galactic Survival",
    "section": "",
    "text": "Empyrion - Galactic Survival Header Image\n\n\n\nEmpyrion - Galactic Survival is a 3D open world space sandbox survival adventure. Build powerful ships, mighty space stations and vast planetary settlements to explore, conquer or exploit a variety of different planets and discover the mysteries of Empyrion!\n\nIf you’re looking for a TL:DR: game is fun but has some challenges.\nThe first time I opened the map to see all the different systems we could visit I was basically sold on the game. The first time we made our system hop, we almost got trapped in hellscape and had a blast trying to get off that planet.\nEmpyrion - Galactic Survival is - well - a Survival Game that adds the idea of different planets and systems to explore instead of simply other towns or woods or whatever that splotch over that hill is. What’s nice about it was as you explore the menus, it gives you something to plan for while trying to build up some semblance of safety where you are. As you scroll out, there are hundreds of systems to explore with their own rings of planets and moons and asteroid fields - which really is the allure of this game. I want to float around the Galaxy and do stupid things.\nOut of the gate, there are lots of geometries to play with when building bases or anything else which is a nice change from other Survival games where you’re kind of stuck with a few pre-defined structures. Sadly, the internal structure of the actual block shape wont impact where you can actually place anything since you’re still locked into a cubic construction system. This does make a good number of the more interesting shapes rather impractical to try and use on anything you build.\nThe plethora of pieces to care about makes navigating and management challenging. Finding out just how to build your first actual base was annoying until you realize that you need a core or a starter block to build off anything. Different blocks are tied to different kinds of structures which are denoted by the initials HV,CV,SV,BA which are just not explained. You’re going to be reading the descriptions of the stuff you build; you’re going to have to just about all the time because you’re going to miss important details. My mistake was not realizing that shields will require a Pentaxid to fuel them and there is no warning about this until the shields don’t work - except the last line in the text. A better idea maybe would have been that you don’t have any shields until you’ve actually fed your Vessel Pentaxid and therefore you’ll be forced to realize this. There is a good amount of finding out the hard way.\nLikewise, there are lots of small stats and management that goes into a base which make sense but just don’t feel intuitive at all. I know that I need power and to do that I need something which makes power but I don’t know anything about Solar Panel rates from looking at the device before construction; there is no real way to optimize buildings or make decisions about if I want something until I’ve made it and realized that I either don’t need it yet or it’s missing necessary components. The Solar Panels need a Capacitor else they’re really not much use once the sun is down - and the base doesn’t store anything. Explore the Control Panel as soon as you can.\nContrary to some complaints, the combat isn’t bad for a Survival game and fights in space have been pretty fun. Having my friend float around in a tuned Small Vessel hecking out resources in Zirax controlled space to avoid pulling us into a space fight with our starter Capital Vessel only to be discovered by a wandering drone led to quite the episode of us coordinating his docking while everything in the system came to murder us escaping with a queue’d up lock to a different system for just such an emergency is why this game has been fun. The complaints about being outgunned are certainly true though. The Defense Station that sits just outside the atmosphere is a space graveyard of failed attempts to kick it out of the orbit. After building a prototype to try and kill the thing, I point-blanked it with 6 pulse Lasers 49 times before it finally blew apart my small vessel and I lost. The AI for the enemies in the game is certainly questionable too. I’ve watched as three factions of different types mingle with one another as if they’re not even aware of each other until I am taking shots at them and then they’re all after me like there is some pact to murder non-NPCs in the Galaxy on sight.\nAll in all, I’m not regretting buying it and it’s probably my favorite current Survival game. I think that the criticisms are pretty fair though; the game is very ambitious and they haven’t delivered yet but the platform is there so long as they’re continuing to diligently add the pieces patch by patch I’ll keep playing it. Complicated but promising."
  },
  {
    "objectID": "posts/Mds/2021-02-04-Introduction-To-Streamlit.html",
    "href": "posts/Mds/2021-02-04-Introduction-To-Streamlit.html",
    "title": "Quick Introduction to Streamlit",
    "section": "",
    "text": "Streamlit is a Framework which is growing in popularity among Python Data Scientists. And the reason for this is rather obvious if you’ve every tried mixing building Web Applications with Data Exporation or Reporting for other people inside your company - or just in general for your projects.\nWe’re going to build a little Web App with the Iris dataset. And yes, please don’t groan; I am also sick of the Iris dataset but it’s clean and simple and accessible.\nSo, first we start with actually downloading and it’s just their namesake:\npython -m pip install streamlit\nNext we’ll import and scaffold off their introduction application:\nimport streamlit as st\n# To make things easier later, we're also importing numpy and pandas for\n# working with sample data.\nimport numpy as np\nimport pandas as pd\n\nst.text(\"Hello World\")\n… and save that to a file called irisExplore.py. Once that’s done, you’ll run it from the console using streamlit run irisExplore.py: \nAnd there we have our first Web Application. Calling the HTML elements that you want and expect is just that simple. If you want a paragraph then you’ll use st.title():\nst.title(\"Turtles Turtles Turtles\")\nst.subheader(\"But, which movie reference is it?\")\nst.text(\"Hello World\")\n And, there is even support for Latex too!:\nst.title(\"Turtles Turtles Turtles\")\nst.subheader(\"But, which movie reference is it?\")\nst.text(\"Hello World\")\nst.latex(r'''\n    a + ar + a r^2 + a r^3 + \\cdots + a r^{n-1} =\n    \\sum_{k=0}^{n-1} ar^k =\n    a \\left(\\frac{1-r^{n}}{1-r}\\right)\n    ''')\n\n\n\nHello World, Now With Latex!\n\n\n\n\nSo, now we’re going to take a quick look at how to add charts. First we’re going to collect the iris dataset. If you have R or something like Tensorflow installed then you can import it from there. I’m going to collect it from someone random on the internet. You should probably download this dataset and store it somewhere; you’ll never get away from it.\nimport pandas as pd\niris = pd.read_csv(\"https://datahub.io/machine-learning/iris/r/iris.csv\")\nAdding the chart is just as easy and you would expect:\nst.line_chart(iris)\n Not exactly what we’re looking but it’s kind of neat. This should serve as a reminder that when it comes to tools, it’s Garbage In, Garbage Out for what you feed it. Be careful what you’re doing because our tools wont know any better."
  },
  {
    "objectID": "posts/Mds/2021-12-08-Review-Of-2021-Challenges.html",
    "href": "posts/Mds/2021-12-08-Review-Of-2021-Challenges.html",
    "title": "Review of 2021 - Challenges, Thoughts",
    "section": "",
    "text": "The honest truth is that I what I wanted to get done this year was pretty fluid and I did not get many of the commits done. Looking at what I have been working on though - and what I ended up spending my time on - I didn’t do so bad this year.\nFirstly, I finally got a macro workflow that is working for pushing my projects forward along with learning new topics. One of the problems which I have been struggling with is how to both learn enough new concepts and frameworks along with actually using them. The normal process being used it to 1. Find something of interest. 2. See what’s been made in it. 3. Work on tutorials, docs for the framework/tool/information. 4. Try to apply it to pet projects. 5. Maybe use it in a real project.\nThis process has problems that I needed solved. Since I have my hands in multiple different places - Data Science, Linux, Programming, Web Design, Secret Project I can’t discuss yet, etc - then this simply doesn’t work. With my life growing in so many dimensions, I simply cannot sit down and follow this process for a single framework and then maybe add it to what I am working on. If it takes a month or two to apply and become comfortable with the framework then it’s too slow and the other aspects of my life will end up neglected. So, what do I do now?\nI have split the months of the year up into four quarters - just like a a business calendar. Then, the first month of that quarter is a Learning Month which means I review what I need to learn, allocate out a set of classes for the month and then do power through them in that month. I take notes about what I’m learning and for some parts will actually practice with a very small pet project which is meant to implement a single thing. After that month is over, the next two months are Apply Months which means that I take everything that I learned in that month, review it, and then start adding it to my projects.\nThis allows me to schedule up to four of classes covering any material I might need coming up in the next few months. This is also better because it keeps learning and application separate. When my schedule was fluid I kept finding ways to not work on projects since I constantly fell back into learning about new frameworks and tools. Now, the clear separation is allowing me to be strict about when and what tasks are supposed to be done. It is already paying out dividends – even though the first attempt at it I was sick for most of the first month of Apply.\n\n\nOf all the challenges that I wanted to complete, this is the one. And, I did worse than last year too: 81 books last year vs 31 books this year. Looking over how my life has changed since the Pandemic started, I think the biggest reason is that much of my reading was done in between. If I was waiting in line at a store then I was reading; while walking from the parking garage at work into the office I was reading. Now that we’re remote only and there are no lines when I am out running errands, I just don’t really read books as much as I used to. I do listen to them while walking around stores but often I don’t count those in the totals; I finished all of the Dragonlance Chronicles books via Audible for example and they’re not filled out on Goodreads. This means setting up time at home to read and I’m always working on projects or doing chores or watching lecture videos. That really only leaves downtime at work to read and we’ve been relatively busy lately. And, I happen to also have projects for Datto that I work on so that’s not really in the cards either. I’ll have a separate post to what the Challenges of 2022 are going to be so strategies will be discussed in that post.\n\n\n\nFor this one, I was successful. Currently, I have two streams of income: one active (Datto) and one semi-passive (Crypto) to work with. I am making money in both of those realms at this point which was more than I had set for the goal. The goal was to identify three potential streams of income that I could have for the year and there was no shortage of ideas for this one. I’ll discuss some of my favorites in the real Challenges of 2022 post but there are plenty more than three,\n\n\n\nThis is was a failed challenge. After exploring and talking with lots of people, this is going to be the hardest one. And, you’d consider that since I already have so many useful skills that it would be the easiest but it’s simply not. Realistically, I like where I work and I don’t really want to leave any time soon. All the positions that are within reach are jobs that I simply don’t want. And, I am not willing to trade in the job I have for those other jobs since they’d change something fundamental about what I want: working overnights, well limiting interacting with users, flexible time table for my own work. I’ll discuss more details in the Challenges of 2022 post when I write it.\n\n\n\nThis is another place that I did decent but did not hold myself too. The biggest factor for this was simply not setting aside the time for it. It’s lower on the list of priorities which need to get done so I simply didn’t schedule out the time. Along side that was anything I wanted to write about I didn’t feel either qualified to write about or didn’t feel what I was willing to write would be be long or interesting enough to justify the post. This is a mindset problem which I should be able to easily solve in the coming year. Especially after looking at what the average person on Medium is writing about; they’re not even trying to write quality content most of the time but instead are simply writing very short not-even-tutorial posts about how to use a framework. There is also a surprising amount of “X Ideas fro Passive Income” posts which are mostly all the same thing over and over. I can do better than that.\n\n\n\nI dropped this one entirely – and I’m dropping it for next year too. I have a list of Daily Tasks which include things like exercise, typing practice along with drawing practice and by far the task I skip the most is drawing. I need to change this so I can actually start drawing at an ok level. Right now, I am struggling with curved lines and that’s quite basic for this level of challenge. I have a few ideas – when don’t I? - but this is a long road which I need to build basic habits out before even trying some kind of real challenge like a Comic."
  },
  {
    "objectID": "posts/Mds/2022-11-16-just-a-catpost.html",
    "href": "posts/Mds/2022-11-16-just-a-catpost.html",
    "title": "Nothing To See Here",
    "section": "",
    "text": "Stable Diffusion is Hilariously Fun\nCat."
  },
  {
    "objectID": "posts/Mds/2021-09-07-game-review-ethereal-estate.html",
    "href": "posts/Mds/2021-09-07-game-review-ethereal-estate.html",
    "title": "Game Review: Ethereal Estate",
    "section": "",
    "text": "Game Header Image\n\n\n\nAs an Ethereal Estate agent, your job is to make a mess out of a mansion and its helpless inhabitants. Never before has a ghost unleashed such devastating chaos… until now!\n\nI had quite a bit of fun with this game. While working on a Data Science project to try and make predictions on “What Genre Is this Game Based on it’s Header Image?” I stumbled across this weird little game trying to solve an error. Looking through the sample set of games, Ethereal Estate pops out among all the other DLC headers and who knows what else these other images are supposed to inspire. The game is free and you play as a ghost - along with maybe your friend - to terrorize, ruin and throw everything within reach around: like a proper ghost should!\nAnyways, the trailer is simple but hilarious so good work Emergence Studio for getting my attention and pulling me in. Your ghost experience is a shopping list of mayhem courtesy of the designer where you cross all kinds of lines: lighting things on fire, throwing people through windows, ramming things with swords, stuff people in closets, poisoning food, breaking records, bringing your victoms back to murder them again, etc. The game is surprising smooth and simple - and fun. My friend and I found the list a bit unclear while roaming around throwing people through windows; my favorite activity in the whole game. I just hope there are more levels coming since there is only the one and we cleared it without too much trouble. Maybe a more traditional graveyard?"
  },
  {
    "objectID": "posts/Mds/2022-12-16-review-of-challenges-2022.html",
    "href": "posts/Mds/2022-12-16-review-of-challenges-2022.html",
    "title": "Review for the Year: 2022 Edition",
    "section": "",
    "text": "Before we start out, this was a tough year. Not only for me but also for many people. I got to watch lots of people forced to makes changes to their lives they never thought they would need to. But, from hardship we learn a lot - and I learned some lessions this year.\nThe first would have to be that nobody believes you if don’t already have something. Much of the respect that gets given to me is due to others watching me fall into problems and then come out the other side with solutions.\nBut, in my own personal life I don’t have problems. There is no drama or technical limits I run up against on a daily basis; there is no Technology Stack which I need to reach for to solve anything. I’ve managed my life so well and so ruthlessly that I could sit here and read without much of a single real problem of any kind to disturb me. This is excellent if I want to spend time fiddling around but not so great if I want to grow beyond where I am.\nAs I work to condense my old lectueres notes from previous classes, I realize how much knowledge sat around to gather dust. I have plenty of books on Apache Spark or building a Database from scratch but I have no Cluster nor Database to my name. I have used some of what I have gained with respect to my job but nothing compared to slid my fingers across. When was the last time I used an ROC curve for anything? Literally never.\nI know that I can pick up any one of these books at implement anything in them without too many problems. I have lectures and code examples for building Algorithms to setting up Machine Learning Pipelines. I can do these things even if I run into them in real world; I have already in the internal projects that I have applied some of this too. But, if I cannot point at a website or a repository or a blog post showing I have done this and can do this then it means nothing. Too many people lie about what they are capable of to get into roles they want; too many people sell well beyond what they can actually do. I need public works for the topics I am touching or I mind as well not have taken the class - or read the book - at all.\nThe second would be Leisure time is a must - even if we think otherwise. I’m a workaholic and I enjoy spinning the wheels. It doesn’t even matter much if I’m going anywhere as long as whatever is getting them spinning is novel in some way. And, that’s where you end up in burn out. Even if it is something you enjoy doing, you can still burn out. This is a really hard lesson for someone like me who likes pushing on boundaries. I sleep less than I should: purposefully. I will work even when my body literally is fighting me to do otherwise. I’ve been at work doing my job with migraines so bad I have to actaully cycle between keeping my eyes away from the screen which hurts me back to trying to solve whatever it is I’m working on. Don’t do this. Projects might be fun but they’re not freinds.\nNext would be If you haven’t actively used something recently, then sell it off to someone who will. As I work out how I’m going to move to different state, I am sorting everything. I have repeat books and motherboards which I never needed. I always kept them under the impression that I would make more servers out of them. I kept a rule in place where if the first server I built never made me money then I wouldn’t build the others; I never obviosuly made anything from the Server I had. We keep so much clutter around we don’t need that others certiainly could put to better use. I shoul of sold these motherboards long ago to someone who would actaully use them.\nAnyways, on to the review."
  },
  {
    "objectID": "posts/Mds/2022-12-16-review-of-challenges-2022.html#streaming",
    "href": "posts/Mds/2022-12-16-review-of-challenges-2022.html#streaming",
    "title": "Review for the Year: 2022 Edition",
    "section": "Streaming",
    "text": "Streaming\nI have been streaming for a while now and it’s hard. I am not surprised that people fail so often at this. There is so much against being successful in this. And, I mean just making any kind of money at all let alone living off this. I did an analysis of some channels at random around the 50-100 viewer average and they really don’t make that much. Granted, I can only judge based on the gifted subs and bits since I cannot see how many people actually subscribe. But, you just don’t make that much. And, that assumes you can even get off the floor and have people continue to show up. Getting someone to follow your content is hard; getting them to come back is even harder. This problem is not better on Youtube either. The general idea is to funnel people from different platforms around but you would need impressions on those other platforms. No wonder content creates jump to any new platform as fast as they can; if you’re the first few then you can funnel whoever is around easier.\nSplitgate is dead at this point. They are ending development on it to do a proper release of it as a new game sans the technical debt they incured while making it. The numbers for streaming it were not that great anyways which is too bad since it was fun to play. I believe there is a post around here about why I quit the game: they lied about the number of real people in my games. I may come back around to that at some point when I rediscover the code I wrote to detect this."
  },
  {
    "objectID": "posts/Mds/2022-12-16-review-of-challenges-2022.html#web-design",
    "href": "posts/Mds/2022-12-16-review-of-challenges-2022.html#web-design",
    "title": "Review for the Year: 2022 Edition",
    "section": "Web Design",
    "text": "Web Design\nI strongly dislike Web Design but also the amount of competition here is expansive. I dislike it for the same reason there are so many people doing it: it is boringly easy most of the time. The repetition is painful for me as I get bored too easily of it. Of course, this is excellent from money making perspective; you solve a problem and sell the solution with minor updates to others who need the same problem solved. I should start looking into paid APIs since that could be something interesting that could be a second income. I still want a second income."
  },
  {
    "objectID": "posts/Mds/2022-12-16-review-of-challenges-2022.html#logo-design",
    "href": "posts/Mds/2022-12-16-review-of-challenges-2022.html#logo-design",
    "title": "Review for the Year: 2022 Edition",
    "section": "Logo Design",
    "text": "Logo Design\nI did a solid amount of drawing practice but am still not very good. I would be very happy to be able to do this as a side gig of some kind. Maybe I could mix Stable Diffusion with some photoshop skills to make some cool ideas."
  },
  {
    "objectID": "posts/Mds/2022-12-16-review-of-challenges-2022.html#blog-posts-for-the-year-passed.",
    "href": "posts/Mds/2022-12-16-review-of-challenges-2022.html#blog-posts-for-the-year-passed.",
    "title": "Review for the Year: 2022 Edition",
    "section": "36 Blog Posts For The Year: Passed.",
    "text": "36 Blog Posts For The Year: Passed.\nI counted out the posts and I counted 32 at the time of writing. This will be post number 33 and I have at least 3 other posts being written now. I’m getting much better about writing these and applying what I learn. I expect this to continue as my prototyping skills and ability to plan these posts is stepping up quickly."
  },
  {
    "objectID": "posts/Mds/2020-12-30-Challenges-For-2021.html",
    "href": "posts/Mds/2020-12-30-Challenges-For-2021.html",
    "title": "Challenges for 2021",
    "section": "",
    "text": "Read 100 Books.\nThis is a challenge because I haven’t done it yet and fitting that many books into my life forces me to think about my spent time. With 52 weeks in the year, this means I will need to read 1.92 books per week. If I’m going to have a break day from everything - on Sundays - then that usually means reading about 100-134 pages of the current novel per day. Doing this along with being productive and pursuing larger goals gets it onto my list.\n\n\nCreate Three Potential Streams of Income.\nI wont be discussing what those are but if modern society and the current state of affairs has taught me anything it’s to be proactive. You don’t want to be responding to a loss after it’s happened; you want to have responded before it has happened. Always have a plan B - and maybe a C. If you need your computer to work, then you’ll want a fallback that can throw in as a substitute. With working from home, the first thing I did was check to see if I can use my phone as a substitute for if my ISP decides to do “maintenance.” It works by the way. I wont hold myself to actually generating an income yet but that’s a stretch goal for the year.\n\n\nNew Job Title\nI’m bored of being a Senior at this point and either need some larger technical challenges or just a swap in the set of responsibilities I deal with on a daily basis. There are some possibilities which I will try either to create or follow up on but I’m not staying where I am.\n\n\nA Blog Post a Month.\nI have this place where I can drop interesting project ideas or discuss problems that I’ve solved for myself to share with others. Since I don’t have a lack of problems to solve, then I should be writing about them for others to read and refer back to. Solving a problem a month is not the challenge but remembering to write about it is.\n\n\nDraw An Original Comic.\nIf you’re following me on any social media then you’ve probably seen some crude drawings. While I admit I’m doing them for my own enjoyment - and challenge - I should push towards something tangible.\n\n[A book you’ve been meaning to read.]: The Grammer of Science by Karl Pearson\n[A book about a topic that fascinates.]: Unrestricted Warfare by Qiao Liang, Wang Xiangsui\n[A book in the backlist of a favorite author.]: Legionaire by Jason Anspath, Nick Cole\n[A book recommended by someone with great taste.]: Awake in the Nightland by John C. Wright\n[Three books by the same author.]:\n\nForgotten Ruin by Jason Anspath, Nick Cole\nHit And Fade by Jason Anspath, Nick Cole\nViolence of Action by Jason Anspath, Nick Cole\n\n[A book you chose for the cover.]: JinJang by Iris Paustian\n[A book by an author who is new to you.]: The Forge of Christendom: The End of Days and the Epic Rise of the West by Tom Holland\n[A book in translation.]: The Nine Chapters on the Mathematical Art: Companion and Commentary by Shen Kangshen\n[A book outside your comfort zone.]: The Social Construction of Reality: A Treatise In the Sociology of Knowledge\n[A book published before you were born]: The Constitution of the United States of America by The Found Fathers"
  },
  {
    "objectID": "posts/Mds/2023-01-01-challenges-for-2023.html",
    "href": "posts/Mds/2023-01-01-challenges-for-2023.html",
    "title": "Challenges for 2023",
    "section": "",
    "text": "It’s the start of the new year and like most other people, I have some goals I’d like to accomplish. There are some people who claim these are useless and that most people don’t meet their goals at all. But, those people should be ignored; if you’re not meeting your goals then that means you need to review the whys and make changes to how you work towards them. Sometimes the goals are too abstract - and I’m going to be running the risk of this today - and sometimes they’re simply unrealistic.\nAccomplishments are commonly moving posts because as you learn more about your goals then you’ll change the path to get there. If you’re not changing the path then maybe you’re not actaully trying to reach them; maybe you’re trying to look like you’re working towards them. Like being moral, sometimes people don’t want what they claim but really just want to be treated like they already have it. A liar doesn’t want to tell the truth; they want to be treated like they’re telling the truth.\nAnd so, many of these are tasks I wanted last year but failed to reach. And, that’s fine because I learned more about what I’m doing wrong last year and have gained some motivation to push harder to change them. Without further ado then let’s get started.\n\n\nThere are many reasons for this - much of which cannot be discussed publicly - but part of the reason is that simply my hopes and plans did not work out. I was hoping that via some internal projects I could transition from my current role to something related to data. After all, the final goal is still to be a Data Scientist and I have not been motivated enough to produce a lot of works since I figured it would of been self evident to move me. Clearly, something happened somewhere along the way to prevent this from happening. Either I failed to convince well enough with my projects or perhaps some kind of internal political games which I was unaware of blocked this. Regardless, it did not pan out and now I’ll have to work much harder to translate what I know into visible public projects.\nThis presents a problem though as many jobs require some form of accredidation via College and I’m not going to do this. So, either my public works will need to be excellent enough to overcome this or I’ll have to take a job somewhere and then try to transition again. A final option would be to start a business around this. But, frankly, this is unlikely as I neither have the current business acument nor would most any client be willing to take on someone who did not already have experience. It is not impossible but that is definitely a last resort.\n\n\n\nOne of the problems I’m having is that all these little hobbies are so interesting to stick your fingers in. Trying to make a videwo game was fun; trying to draw can be fun; writing a book would be fun. But, doing all of these is simply not realistic. And, switching between them is hurting quite a bit. I’ve played around with and learned about each in turn. I’ve even been trying all of them to some limited extent. But, if I have a job and also have to produce data projects then that’s two hard blocks of time. Trying to fit writing a book and drawing and making a video game on and off over and over is simply stupid.\nI have to pick one and I pick drawing; this was a really hard choice. At heart, I would like to pick making a video game but drawing is such a useful skill and would allow me so much flexibility to create. Being able to draw most anything I could imagine is very alluring. And, I do really like drawing. I believe I’ve written before that I find drawing humbling. I’ve been treating it more like brushing my teeth than an actual skill; I draw some boxes or something as fast as I think I can just to say I did: just to check the box that I practiced today. This is not how you get better and it is not how you enjoy the hobby.\n\n\n\nHonestly, this is more a necessity every day. It gives some bargaining power against employers who constantly try to undervalue. It also allows one to invest money against the growing econmic failures of our current Society.\nThe real problem with these is that they either have to be very cost efficient with respect to time. Something like Door Dash and such are very bad investments and you’re more or less back to just working a part time job. In the past, I had a second job which took the other two days of the week which my present job did not. You end up making a choice about whether spending that time to improve at your current job out of office will get you more money then your side job. If working a minimum wage is worth more than taking a raise at your job in a promotion then why even bother getting the promotion? You mind as well stay where you are and work at McDonalds until you can leverage your current job against a new employer and a hiring raise.\n\n\n\nEvne when I was physically active, I was not taking care of my own body. To say I was doing harm would be an understatement. I have never been good about sleep and I’ve never spent the time to stretch or go to a gym. I’ve never really been overweight since my jobs were mostly physical and I did an enormous amount of walking. Office work has done even more harm since now I lack much of physical work I used to do; And, with the need to spend much of own time learning and reading and coding I have deglected even the walking I used to do so much of. This year is when I work to fix that; at least the physical part. I’m surely eternally doomed to a broken sleep schedule.\n\n\n\nI already wrote about how keeping track of the number of books you read is the wrong way to go. You just end up with pressure to read smaller books which are almost always barely worth reading compared to the much larger ones. So, the only book count I am doing is to read Five Important Books: 1. An Encyclopedia of World History by William M. Langer. 2. Tragedy & Hope by Carroll Quigley 3. Arthashastra by Kautilya 4. Economics by Paul A. Samuelson 5. Alogrithms by Robert Wedgewick\nThis does not mean I wont be reading other books but that my attention will be focused on finishing these. I hope this will also help remove the Completionist Drive as this pushes one to skip forwards as opposed to simply enjoy and pay attention.\n\n\n\nI realize that the Completionist Mindset mentioned above will do this harm but splitting posts apart should help some. I will need to write only three posts per month and I think this will not be that hard. I’m slowly getting better about what should and should not be turned into a post. I admit that some of the Game Review posts can probably be considered more fluff than real content yet. But, I’m quickly also learning about how to write those better as well.\nMy projects posts do have direction now though. The posts are intended really for three audiences: people looking for some guidance, people looking to confirm I know what I say I know and myself to remind myself of solutions I’ve stumbled across. There are very different expectations for each of these. For Guidance folks and myself, I’d prefer the posts to be closer to Technical Documentation with code examples to be easily copied out. I want the writing to be simple and to the point to remind me how to do something. For people confirming competence, they want something closer to a Traditional Data Story. There is a genre of book that is like this called **Narrative Non-Fiction* which is a solid comparison of building a story out of - usually - historical events. That is what I need to work towards: telling a compelling story which is true and also browsable for those looking for solutions."
  },
  {
    "objectID": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html",
    "href": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html",
    "title": "How Do You Create a Normal Map?",
    "section": "",
    "text": "Looking around the Internet, it ended up being much harder to find the information about how to make a Custom Normal Map then I though it should be. After finally figuring this out, I thought I’d formalize/share it here so that it doesn’t get lost among everything else."
  },
  {
    "objectID": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html#where-were-going",
    "href": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html#where-were-going",
    "title": "How Do You Create a Normal Map?",
    "section": "Where We’re Going",
    "text": "Where We’re Going\nWe’ll go over the how to do this with a simple example: dirt. First you’ll want to start blender up and clear the scene. Drop in a new plane and make it whatever size you like; I scaled it by 2x so if you’re trying to just copy my examples then simply do that.  Since we’re just working on making a material to import into a game, we don’t really need anything else. So, we’re moving on to making the texture: \nAgain, this is just a basic example so we’ll create something like this:  Not bad, not complicated but will show us the effect we’re after; feel free to copy the settings if you’re following along. Now add an Image Texture Node to the graph but don’t attach it to anything: \nCreate a new image and call it what your target material is about. In my case, I’m just making dirt so we’re calling it BasicDirt:  Now we’re going to do something called Baking. So, what is Baking actually? What we’re doing is saving information to the texture so that it doesn’t have to be re-calculated. Eevee - the default engine in blender - doesn’t support baking so you’ll need to switch over to Cycles for this to work. Once that is done then go ahead and click Bake and it should work!  You should now see the image show up in the Image Editor in the UI. You’ll want to save this to an acutal file for use in the Game Engine - or for other uses like making a normal map for it."
  },
  {
    "objectID": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html#the-next-stop",
    "href": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html#the-next-stop",
    "title": "How Do You Create a Normal Map?",
    "section": "The Next Stop",
    "text": "The Next Stop\nOk, now that you have the texture go ahead and pull this into your photo editing software. I’m going to do this with Gimp so to follow along you’re going to need it. Otherwise, you should just be able to look up “Making a Normal Map in Photoshop” and it should be a simple option. I spent quite a bit of time trying to make the Normal Map in Blender and I couldn’t get it working; if someone did figure this out then please let me know. But until then, you’ll want pull your image into Gimp first. Once that is done, you’ll select Filters > Generic > Normal Maps{{ site.baseurl }}. and it will present you with the ability to set how harsh the effect is:  … and that scale is not the default. Go ahead and play around with that number to get the effect you want. Depending on your Game Engine, save it with the proper nomenclature for when you pull it in. \nGet your Blender project back open and hop into the Shading Pane once more. Add a new Image Texture Node and insert the new Normal map you created. Once added the image into the Image Texture Node, you’ll want to switch over to non-color: \nAnd, that’s how you Make a Normal Map, and add it into blender. You can improve it some using other options like adding a Bump Node between it."
  },
  {
    "objectID": "posts/Mds/2022-12-01-game-review-journey.html",
    "href": "posts/Mds/2022-12-01-game-review-journey.html",
    "title": "Game Review: Journey",
    "section": "",
    "text": "Explore the ancient, mysterious world of Journey as you soar above ruins and glide across sands to discover its secrets.\n\nI am again late this one but I am so grateful I found the time to play it. Journey is a masterpiece in design. Everything is stripped down the most basic elements of play and built out again with a simple, beutiful world guided by a nameless character. You do not know who you are; you do not know what you are; you do not know where you are. What you know is that you are in a peaceful desert which leads you on towards your next destination with nothing but what is right in front of your eyes. The controls are also simple - there is but direction, communucate and eventually fly - which means you can quickly experiment to find the almost none that exist and start your own journey.\nThe world uses the environment to guide you along towards little puzzles and problems in turn revealing just a bit more than it did before. If you like exploring, then there are stone murals spread through the levels which build more about the world. And, as you move through the hidden and voiceless world around you then game adds on it’s next real surpise: other random people will join in your adventure to help you solve the puzzles. But, since there is no way to talk to one another you are left to communicate with nothing but your actions. This was a surprisingly wonderful experience as I ended up walking up the mountain with someone else I didn’t know - and actually cared that they made it up with me.\nAfter completing the game, I was sure that they were real people and not bots but with how long the game has been out I felt compelled to check that my companion was real. And, they were in fact real! The game holds a steady level of just under 100 players at any time. I cannot wait to be one of those people and come back to experience walking up the mountain with others again."
  },
  {
    "objectID": "posts/Mds/2022-11-01-game-review-powerwash-simulator.html",
    "href": "posts/Mds/2022-11-01-game-review-powerwash-simulator.html",
    "title": "Game Review: Powerwash Simulator",
    "section": "",
    "text": "Powerwash Header Image\n\n\n\nWash away your worries with the soothing sounds of high-pressure water. Fire up your power washer and blast away every speck of dirt and grime you can find, all with the simple satisfaction of power-washing to a sparkling finish.\n\nMy friends have been playing this game and I finally picked it up. After doing some cooperative levels with them, I started into the Career Mode of the game. This is the Story Mode and does what you’d expect from a Single Player experience: slowly increasing difficulty through different tools, different surfaces and different buildings with different shapes. There is a story here which is silly; reach the end or spoil it with a Let’s Play if you’d like but I laughed.\nThe beginning is mostly slow to work through since the early tools are acceptable for the tasks you’re given. If you invest the stars and money into betters tools and nozzles then you can get ahead of the curve - at least in my experience. Discussing which tools are best is somewhat moot since you’ll beat the levels faster - but do you really want even want to?\nThis brings me to my final thoughs: I’m not sure if this game is fun. After reaching the end of everything, I can say it was relaxing and I definitely accomplished something of value. If nothing else, the game lends itself well to listening to podcasts while you mindlessly clean bathrooms and boats. The core loop works but I found myself somewhat resistant to completing levels around the middle of the game. It was putting on the podcasts to distract me that ended up making the game playable."
  },
  {
    "objectID": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html",
    "href": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html",
    "title": "How to Setup V Rising on Linux",
    "section": "",
    "text": "If you’re here becuase you just want the answer then here are the exact steps that always work for me. 1. Download the V Rising Dedicted Server from steam to your account. 2. Download Proton Experimental and change to the Bleeding Edge Beta branch. 3. Run this command:\nSTEAM_COMPAT_CLIENT_INSTALL_PATH=/tmp STEAM_COMPAT_DATA_PATH=/tmp <Path/to/proton/experimental/install/location/proton> run VRisingServer.exe <args-from-offical-repo>\n\nIf this doesn’t work then proceed to the below:\n\n\nDownload Lutris and install the dependecies it asks for.\nFind and install the Epic Games laucher from Lutris.\nDo the steps it asks you to run through: including those dependencies.\nReboot.\nRun the command:\n\nSTEAM_COMPAT_CLIENT_INSTALL_PATH=/tmp STEAM_COMPAT_DATA_PATH=/tmp <Path/to/proton/experimental/install/location/proton> run VRisingServer.exe <args-from-offical-repo>"
  },
  {
    "objectID": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#how-do-we-run-something-manaully-with-proton",
    "href": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#how-do-we-run-something-manaully-with-proton",
    "title": "How to Setup V Rising on Linux",
    "section": "How Do We Run Something Manaully With Proton?",
    "text": "How Do We Run Something Manaully With Proton?\nLuckily, just like the wonder people over at the Github Page, we can find that answer online! According to this old thread from Reddit: > STEAM_COMPAT_DATA_PATH=~/.proton/ ~/.steam/steam/steamapps/common/Proton 3.7/proton run whatever.exe\n> You need to create ~/.proton (it can be any directory and can be empty)\nExcellent! Now we have something to work with. Looking at the command I built, I simply put those in /tmp since I didn’t want to think about what compatdata is; I still don’t really know what this is after a quick Google but it works like this. So, simply create a little script to be ran in bash:\n#!/usr/bin/bash\nSTEAM_COMPAT_CLIENT_INSTALL_PATH=/tmp STEAM_COMPAT_DATA_PATH=/tmp <Path/to/proton/experimental/install/location/proton> run VRisingServer.exe <args-from-offical-repo>\n… and then chmod +x scriptName.sh and run it to start the server."
  },
  {
    "objectID": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#this-didnt-work",
    "href": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#this-didnt-work",
    "title": "How to Setup V Rising on Linux",
    "section": "This Didn’t Work!",
    "text": "This Didn’t Work!\nSo, in my own troubleshooting I found that I ended up with an error Failed to create batch mode window: Success. when I ran this a second time on my server. However, after doing some testing on multiple different distributions - Ubuntu 20.10, EndevourOS, Manajaro - I found that it didn’t work on any of my systems even though it had ran once. Adding to my confusion, the dedicated server ran find on my own Desktop and without issues at all. Considering one of my servers was Manjaro just like my desktop then there must be something installed on locally which is not being installed along with the Steam Proton Dependencies. The only noticable difference in dependencies was that I have Lutris Installed on my own Destktop and it’s not on any of my other systems. So, I ran through exacty what is installed via Lutris and once I rebooted my other server it worked without issues."
  },
  {
    "objectID": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#so-what-is-missing",
    "href": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#so-what-is-missing",
    "title": "How to Setup V Rising on Linux",
    "section": "So, What Is Missing?",
    "text": "So, What Is Missing?\nThe honest answer is that I don’t know. The list of dependences that gets installed along with the process is quite a list and I simply don’t have time to isolate which ones are the correct packages. If you have the time to figure it out then please let me know so I can update this post. Otherwise, Happy Hunting!"
  },
  {
    "objectID": "posts/Mds/2023-01-01-game-review-swordship.html",
    "href": "posts/Mds/2023-01-01-game-review-swordship.html",
    "title": "Game Review: SwordShip",
    "section": "",
    "text": "Swordship is a futuristic lightning-fast dodge’em up which flips the script on the traditional arcade shooter. Turn both the tide of battle and enemies on themselves by dodging, weaving and diving your way through an onslaught of enemies hell-bent on tearing your Swordship apart.\n\n\nGenuinely Novel And Fun\nSwordship feels like playing an old Arcade Shoot ’Em Up but where you turn your enemies into weapons. It’s like if you took a comedy sketch where everyone was accidentally hurting one another but you were composer getting them to do it – and you were hyped on cocaine while doing it.\n\n\nPlaying the Game\nThe game spins the camera around but the enemies still come from the top: usually. Unlike the old games where you found power ups by killing enemies, you unlock them Rougelike style after you beat a level. You can get some very useful power ups; my favorite so far is the slomo when grabbing a crate only because I have had not so great luck with the blocking spawns power up. The enemies are well designed and easy to tell apart. The attacks are varied as you progress; I also like that after you unlock groups of enemies you can see them randomly. That helps keep the variabiliy of the game going run after run.\n\n\nAnd, That Tune.\nThe music in this game is really wonderful. I wont even pretend that I don’t keep listening to it on the trailer every time I come to the page. It really nicely keeps the mood of the game but does not distract you from the game while you’re playing it.\n\n\n.. But That Price?\nYeah, the price tag feels a little expensive for the game but so long as they don’t abandon it then I think they have a lot of room to grow into. Some different music, some different level gimmicks and some more enemy variability and it will be fine. They could even add some novel ships beyond the sword ship for all we know.\nAll in all I definitely recommend."
  },
  {
    "objectID": "posts/Mds/2020-10-28-The-Pulp-Mindset-Review.html",
    "href": "posts/Mds/2020-10-28-The-Pulp-Mindset-Review.html",
    "title": "Book Review: The Pulp Mindset; A Newpub Survival Mindset",
    "section": "",
    "text": "This book is about something worth noticing as the access to writing and publishing has broadened. There are healthy and helpful tips for writing and about writing learned from reading the Pulps of the past - and as someone that hasn’t read much of them and slowly breaking that particular seal I cannot agree more. However, reading the book feels more like what my own essays in high school felt like with repetition spread around and re-wordings of the same ideas.\nThat isn’t to say don’t purchase or read it; I think you should just because there is some good history here. The author’s knowledge about such topics as the speech Mutation or Death was quite a surprise and the relationship between Old Pub and New Pub is a meaningful distinction that Cowan does well. Personally, I think he’s overselling it as there is enough poor writing on display via Amazon Unlimited or niches such as LitRPG where the divide between practiced writers or amateurs shows up from the first page.\nAll in all, spend the money but get it in digital."
  },
  {
    "objectID": "posts/Mds/2021-02-09-Memorandum-Of-Understanding-Blurb.html",
    "href": "posts/Mds/2021-02-09-Memorandum-Of-Understanding-Blurb.html",
    "title": "What is a Memorandum of Understanding?",
    "section": "",
    "text": "A Memorandum of Understanding primarily serves as a point of negotiation between two or more parties with respect to their intents. The document serves as a way to find common ground along with set expectations about what individual parties are both responsible for and expect from the other party. Special consideration should be paid to whether the document is Legally binding in the legal context it is being used; these are normally not legally binding. It is best thought of as similar to a Gentlemen’s Agreement: “an informal and legally non-binding agreement between two or more parties. It is typically oral, but it may be written or simply understood as part of an unspoken agreement by convention or through mutually-beneficial etiquette”[2].\n\n\n\nSample from Violence Against Women Oganization\nFormSwift"
  },
  {
    "objectID": "posts/Mds/2021-02-09-Memorandum-Of-Understanding-Blurb.html#citations",
    "href": "posts/Mds/2021-02-09-Memorandum-Of-Understanding-Blurb.html#citations",
    "title": "What is a Memorandum of Understanding?",
    "section": "Citations:",
    "text": "Citations:\n\nhttps://en.wikipedia.org/wiki/Memorandum_of_understanding\nhttps://en.wikipedia.org/wiki/Gentlemen’s_agreement\nhttps://www.investopedia.com/terms/m/mou.asp\nhttps://www.investopedia.com/terms/l/letterofintent.asp"
  },
  {
    "objectID": "posts/Mds/2022-01-06-Game-Review-How-I-Lost-My-Eraser.html",
    "href": "posts/Mds/2022-01-06-Game-Review-How-I-Lost-My-Eraser.html",
    "title": "Game Review: Eraser",
    "section": "",
    "text": "Eraser Header Image\n\n\n\nThrow your stick-figure-self at cannonballs and airplanes in an upward pursuit of the eraser! Play cooperatively online to share the journey! Who’s to say whether the company will help or hinder, though…\n\nRecently, I was looking for some simple and free games to enjoy and ran across this little gem. Eraser by Ringating feels like playing a Platformer from the 90s but with the Visuals and Game Design philosophy of now. Let me explain.\nFirst off, there is no such thing as death in this game. You simply cannot die and there is no real punishment for making a mistake. The worst that happens is that you have to start all over or fall a little ways down and have to regain ground again. You might be thinking they’re the same because if you died in platformers from before then you’d also have to start all over; They all had a checkpoint system where if you had lives and you died then you could start from the checkpoint again. Games like Spyro have the faeries which record your position after all. But, this game has no hard Start Over Because You Died like those games do. Sure, you could incidentilly fall all the way down - I certainly did more times than I’d like to admit - but there is no hard lose condition. You keep playing as long as you’re willing to and sometimes you fall and have to start part of it again. Somewhere along the way, the levels make it harder to fall to to the beginning so there isn’t even really a checkpoint system as much as the design makes it harder to return to the beginning.\nThe game itself is just smooth to play. The musical theme of simple classical fits nicely with the theme of the game of being a simple stick figure in a simple world full of simple enemies and obstacles. I’d definitely recommend picking it up and playing it for a few minutes when you don’t have a lot of time to commit to something else - and it’s free on Steam!"
  },
  {
    "objectID": "posts/Mds/2020-09-30-Game-Review-Post-Void.html",
    "href": "posts/Mds/2020-09-30-Game-Review-Post-Void.html",
    "title": "Game Review: Post Void",
    "section": "",
    "text": "Post Void Header Image\n\n\nAbout a month ago I Steam was kind enough to recommend a rough looking LSD-inspired shooter. And, this little gem is called Post Void created by a little Developer group called YCJY Games. Looking at the videos, screenshots and description I was immediately sold:\n“Post Void is a hypnotic scramble of early first-person shooter design that values speed above all else. Keep your head full and reach the end; Kill what you can to see it mend; Get the high score or try again.”\nThe game delivers on all of that with the classic Arcade Shooter style that I miss everytime I spin up any modern First Person Shooter and with a price tag of $2.99, there wasn’t any risk.\nThere is a tutorial level to get you acquainted with the basic ideas of the game - which is really just: You have a slide like many modern shooters. Your health is that glass tiki skull which drains away. Then you’re dropped right in and good luck.\nThe management of your health which forces you to press on creates a fun stresser which is simple but not overwhelming to manage: kill to fill. The enemy varieties are also limited for the 11 stages you race through - which isn’t all that many. It’s not a long game and wouldn’t actually mind seeing a Post Void 2 which pulls some elements from Rouge-likes and maybe a bit of coop shuffle. Also, the aim is a bit forgiving and there were moments where I was sure I would miss but still killed an enemy.\nThe art style is a bit rough on the eyes at first but actually is the right choice to make registering enemies and threats blend into the background just enough that you can miss or run right into them if you’re not careful. I’m also partial to colorful games so I admit this visually is my kind of game.\nAll in all, I’d recommend stuffing this game into downtime between lobbies and breaks between studying to stay alert.\nAlso, the Shotgun is best. Who knows why anyone wants to use the knife: weirdos."
  },
  {
    "objectID": "posts/Mds/2020-07-09-Voices-of-the-Void.html",
    "href": "posts/Mds/2020-07-09-Voices-of-the-Void.html",
    "title": "Book Review: Voices of the Void",
    "section": "",
    "text": "The Voices of the Void by David Stewart is a pleasant short story that mixes a Science Fiction backdrop with some supernatural horror. We join the introduced protagonist - Andrew Dalatent - entering a mining colony called New Gibralter where he appears tasked with figuring out what happened to the people there. And, I say it appears because as he starts his descent you find out he’s more aware of the whys and whats of the situation than is initially let on.\nCoupled with his own supernatural ability to observe future and past versions of events around him, we get to join him as he tries to survive against humanity removed of its own free will. These sections that Stewart uses were my own personal favorite because they’re surprisingly easy to follow but also because they grow slowly out of being just a novel tactic Andrew uses to protect himself into an important aspect of the later story.\nShort read but enjoyable."
  },
  {
    "objectID": "posts/Mds/2022-09-08-Game-Review-Pawnbarion.html",
    "href": "posts/Mds/2022-09-08-Game-Review-Pawnbarion.html",
    "title": "Game Review: Pawnbarian",
    "section": "",
    "text": "A friend of mine recommended that I check this game out as he’d stumbled across it somewhere, somehow. Pawnbarian is what looks to be the first published game by j4nw and it’s a good start. There was an earlier iteration of the game on their itch.io page if you go lookikng for it to try out. Checking out the store page, the game presents itself well and is instantly recognizable since it’s a chess board with unique avatars of somekind for your piece and the enemies. The aesthetic is clean and simple which doesn’t distract while playing the game: \n\n\nThe game is simple to undertand too - even if you’ve never been good at chess. You play as an Avatar - the starter is, of course, the Pawnbarian - which comes with usually a single gimmick and a different deck of moves. The Deck of Moves is how you move around the board to kill the enemy tokens - and they’re simply cards with different chess pieces which limit how you move. For example, the pawn moves like the pawn does in chess; the rook moves like the rook and so on. Your moves for the turn are drawn from this deck and you can see both the remaining moves as well as what was discarded so you have some idea of what could happen next round as the cards get pulled. You’re not limited by time since once the all the cards are drawn then they are simply put back and you get access to all of them again. \nAs mentioned, your avatar has a gimmick and the Pawnbarians is that when he plays a card with the Cantrip - the lightning symbol - then he gets to move again. And, you can chain these the more you play them to take as many moves as possible. When the cantrip is played then it also draws another card for you to play so you’re never limited by the cards in the deck. At the end of each round, you can use the gold earned by winning the round to add more abilities to your cards. The cantrip is not the only gimmick to add to cards; you can also add diagonal attack or a horizontal attack which will not only harm the square you land on but allow you to hit multiple enemy tokens. There more as well but we’ll limit the discussion to just those as they get the point across.\nEnemies also have their own gimmicks, such as the Nimble attribute in the screenshot above, which constrains you simply stomping all over the enemy tokens. There are three dungeons with different enemies and different gimmicks to hinder you in your progress in the dungeons. I do wish there were more than the three dungeons since I stomped most of them much faster than I hoped. That isn’t to say some levels are not a challenge; the Void Grasp afflicted boss of Dungeons three was a frustrating challenge to beat but that’s why we play the game. What is not lacking is the list of different avatars you can play; there are 6 of them with their own ways to play the boards.\nIf you like Rouge-lights and Chess then you’ll end up enjoying turning this game on when you’re looking for a break from either Chess or Rouge-lights."
  },
  {
    "objectID": "posts/Mds/2021-12-08-Challenges-For-2022.html",
    "href": "posts/Mds/2021-12-08-Challenges-For-2022.html",
    "title": "Challenges for The Year: 2022 Edition",
    "section": "",
    "text": "I’m lowering the number back to 50 books even though 100 books is still the larger goal. The real reason is I need to find a way to fit reading back into my schedule. With so many projects, I will be shifting to audiobooks which is depressing since I read faster than I can process audiobooks. I do have a trick for this: listen to ebooks via the Kindle Store in a Web Browser. This is probably a good enough place to explain how this works.\nInstall an extension for your Web Browswer which does Text-To-Speech - such as Read Aloud. Honestly, finding a quaility Text-to-Speech reader and paying for the extension is a better idea than free; the lack of intonation from the extension I am using makes listening much harder. Concentrating is harder compared to a professionally done book from Audible. Go to Your Online Kindle Library and find a book to listen to. Open the book up and then hit the play button inside your Text-To-Speech extension. You will need to adjust the speed, voice, volume and location when you hit play per new book for tuning. But, this is a good way to fit more average fiction reading into your day.\nPutting this in the background of the first half of my work shift sounds correct. Since I am not usually interacting with people then it should be an easy way to get 2-3 hours of “reading” done each day. Let the challenge begin."
  },
  {
    "objectID": "posts/Mds/2021-12-08-Challenges-For-2022.html#have-three-streams-of-income",
    "href": "posts/Mds/2021-12-08-Challenges-For-2022.html#have-three-streams-of-income",
    "title": "Challenges for The Year: 2022 Edition",
    "section": "Have Three Streams of Income",
    "text": "Have Three Streams of Income\nWe’re moving from planning to application with this one.\n\nPrimary “9-5” Job.\nObviously, my job is the primary source of income - and I have no real interest in changing that. Working Technicaly Support is usually an awful experience since you’re dealing with End Users which have no real understanding of technology nor how their behaviors make little sense. However, where I work I am dealing with people that at least have some background in technology so having conversations with them is usually much less painful than End Users. Since my role is also on Overnights, this significantly limits the interactions with the kinds of people who constantly cause problems for themselves - and therefore me.\n\n\nStreaming\nI’m going to take a serious attempt at making money on Content Creation sites like Youtube, Twitch and TikTok. The competition here is enormous and this is not the kind of job I admit I’m naturally good at but it fits well into my life. I play games with my friends and coworkers consistenly enough that these can be put online without too much trouble. Getting people to watch or care about the content is a totally different problem. Doing the research that I have so far, we’re really going to have to lean on Personality quite a bit otherwise this simply wont work. Due to being on Linux, there really is nothing but Splitgate which I can play competitively and stream. And looking at the numbers, Splitgate is struggling both as a streamed game as well as keeping an audiance of players:  \nSo, maybe I could try and niche with Splitgate but I would be concerned with getting locked into playing a game struggling to grow. To start though, I will need: 1. To become Affiliate on Twitch. 2. Get better at tags on Youtube. 3. To start posting shorts to Tik Tok.\n\n\nWeb Design\nThis is the least fun option but I’m capable of this. I have found a set of frameworks and a niche that I can work in without getting too far dragged into the larger frameworks: Single Page Websites. As I expand my ability to use tools such as Blender, Inkscape then I can push into more artistic and aesthetic styles instead of the more normal Single Page Sites. Not much else to say aside from if this works out then expect a Portfolio coming soon.\n\n\nLogo Design\nBetween slowly expanding Blender skills and working on my drawing skills, this would be something fun that I could doodle designs and then try to sell to others. There is quite a bit about communicating concepts artistically along with a good amount of Color Theory to learn before I can do this."
  },
  {
    "objectID": "posts/Mds/2021-12-08-Challenges-For-2022.html#new-job-title",
    "href": "posts/Mds/2021-12-08-Challenges-For-2022.html#new-job-title",
    "title": "Challenges for The Year: 2022 Edition",
    "section": "New Job Title",
    "text": "New Job Title\nThis year is the real year that matters though for my job; I have a patent pending for the company and this should be year I get the nay/yay for whether it goes through. No matter how that goes, I’m pushing for an Analytics Job either inside or outside the company. I am familiar enough and competent enough to be doing this as a real job and I am well bored of doing Technical Support. It also is not a real future for me; there is no where to go that I want to be. So, if by six months in I don’t see any progress here then I’m going to start applying for part time Analytics jobs to build experience and push into a new career."
  },
  {
    "objectID": "posts/Mds/2021-12-08-Challenges-For-2022.html#blog-posts-for-the-year.",
    "href": "posts/Mds/2021-12-08-Challenges-For-2022.html#blog-posts-for-the-year.",
    "title": "Challenges for The Year: 2022 Edition",
    "section": "36 Blog Posts For the Year.",
    "text": "36 Blog Posts For the Year.\nI have been spending a good amount of time reading Medium posts and articles by others this year. And, my expectations are far too high and totally unnecessary. While in college, I learned about one of the books that significanly changed my expecations about writing: The Elements of Style by Strunk and White. The book emphasis is always on Omit Needless Words and I extended that to all of my writing since then: write only what needs to be said. While a good guide, this is now preventing me from writing at all. If someone else has said it then I am not writing about it; if someone hasn’t written about it then I am not confident in writing about it. I know enough and am learning enough to write content to clarify my thoughts for myself and others. So, three posts per month on anything I feel like: Game Design, Data Science, etc"
  },
  {
    "objectID": "posts/Mds/2021-12-08-Challenges-For-2022.html#make-the-secret-project-public",
    "href": "posts/Mds/2021-12-08-Challenges-For-2022.html#make-the-secret-project-public",
    "title": "Challenges for The Year: 2022 Edition",
    "section": "Make the Secret Project Public",
    "text": "Make the Secret Project Public\nBy the end of the year, there will be public posts about this project. Watch for a reveal in the future."
  },
  {
    "objectID": "posts/Mds/2021-05-18-Game-Review-The-Forest.html",
    "href": "posts/Mds/2021-05-18-Game-Review-The-Forest.html",
    "title": "Game Review: The Forest",
    "section": "",
    "text": "Game Header Image\n\n\n\nAs the lone survivor of a passenger jet crash, you find yourself in a mysterious forest battling to stay alive against a society of cannibalistic mutants. Build, explore, survive in this terrifying first person survival horror simulator.\n\nThe Forest is another game in the Survival Genre which tries to use Environmental Storytelling to push a plot. You can play with your friends or you can play alone - which definitely gives you a different experience. Alone - so I’m told - feels a lot lot a horror game where you’ll fear leaving the walls of whatever you’ve built. With friends, it was good fun but definitely doesn’t feel like a horror game: FREE BONES!\nWe’ll start with the stuff I didn’t like first. Like most survival games there is crafting for shelters, walls, defenses and cooking. Unlike most other survival games that I’ve played, crafting buildings in this one feels pretty pointless. There are about four buildings that actually matter: Drying Rack, Bonfire, Water Collector, Shelter. Anything that produces light barely matters and you have to constantly feed fuel to the fires to get the initial “overburn” to really see in the darkness - or really far at all.\nWalls are fun but we built on an island off the coast of the main island so we basically just started building random stuff for fun since our base never got attacked. Which is real unfortunate because this game has one of the best building systems I’ve ever played with. The buildings are cumulative so you can build what is basically a blueprint that your friends can collectively dump resources into. And, everyone can see them and since there is a visual blueprint you can actually lay out your base “Blueprint” in full for discussion. What a great idea!\nSince it’s Environmental Storytelling, you can basically skip all of it until the end while murdering everything along the way. This is what happened to us - we ended up running into the caves to wipe out the natives and just kind of picked up whatever we found - on our way to kill more of the natives. This isn’t really the game’s fault but more the way we played so beware of missing stuff until the end when you have no choice but to address the story that you don’t really understand.\nThe combat is surprisingly fun and one the better Combat systems in a Survival Game. Ranged weapons are not broken and do not nullify enemies into a shooting range and melee feels really good with trading blows with the Cannibals. This isn’t really due to the system being complicated but much more about how well done the animations and the interactions with the Natives themselves: they flee, they try to kind of spook and intimidate you, they call their friends. Enemy and animal variety was pretty good and you get introduced to newer enemies at the right times - at least for us. Learn how to block; you’re going to need it.\nI mentioned the Night above but it’s really a positive: when it’s dark it is fucking dark. Wandering around at night can get you lost really fast. Everything moving around you - leaves, trees, animals - will making you constantly on edge thinking about if you’re about to get attacked in the dark. Following enemies in the dark is just hard so you wont know how many there are nor where they ran of to. Running around in the Forest really does feel like moving around in the woods stumbling onto deer and opposing murderhobos while you try and find a stable water supply.\nA few annoyances around multiplayer we ran into where one time we died and my friend had to afk to do something else. And, I got stuck in the plane behind him for a full hour - not game time, real life time - since it was impossible to actually get around him to play the game.\nAnother is that the server doesn’t actually save the state. You must save your own character at shelters otherwise you lose progress. This makes absolutely no sense at all when we’re playing on a server being hosted by someone else. You will lose progress if you don’t personally constantly save.\nAnother was that since we were all doing our own tasks and split up often, some of us didn’t end up having all the tools necessary to get to the end of the game. As a five-man group, one of us got stuck behind because of this while we basically tried to finish the game. If you do this then the bad news is that you all have to vote at the end otherwise you can’t finish the game. We ended up just watching the endings on Youtube after doing all the work to get to the end.\nAlso, game works in Linux completely in Proton 6.3-4."
  }
]
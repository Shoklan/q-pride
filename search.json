[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Orgulo.us",
    "section": "",
    "text": "Does The Faker Package Pass Luhn?\n\n\n\n\n\nHow good is the fake generation?\n\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nNothing To See Here\n\n\n\n\n\nJust some Catposting on the Internet\n\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Introduction to Memcached\n\n\n\n\n\nAnd Maybe a Real World Use\n\n\n\n\n\n\nNov 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Planner Review - Discounting Cash Flows\n\n\n\n\n\nNot Sure I Trust This\n\n\n\n\n\n\nNov 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Cycle Frontier Jobs Are Worth Doing? - Part Four.\n\n\n\n\n\nTime To Deal With the Missing Values\n\n\n\n\n\n\nNov 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSimple Function for Highlight Box Effect\n\n\n\n\n\nBuilding on A Pretty Plot.\n\n\n\n\n\n\nNov 2, 2022\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Powerwash Simulator\n\n\n\n\n\nI wouldn’t do this in real life; Why am I doing this?\n\n\n\n\n\n\nNov 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollecting External Data for Python.\n\n\n\n\n\nOr, Delete the Intermediate When Collecting Data.\n\n\n\n\n\n\nOct 26, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSomething Cute Or Something Dangerous\n\n\n\n\n\nFerrets Vs Dragons Image Classification\n\n\n\n\n\n\nOct 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Cycle Frontier Jobs Are Worth Doing? - Part Three.\n\n\n\n\n\nLet’s do the calculations!\n\n\n\n\n\n\nOct 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Cycle Frontier Jobs Are Worth Doing? - Part Two.\n\n\n\n\n\nSecond We’ll Clean the Tasks.\n\n\n\n\n\n\nOct 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDownloading and Exploring Loot in Cycle Frontier\n\n\n\n\n\nLet’s Check out the Loot!\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Test of Ipywidget Interact Function\n\n\n\n\n\nDoes this Work For the Website? No.\n\n\n\n\n\n\nSep 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Cycle Frontier Jobs Are Worth Doing? - Part One.\n\n\n\n\n\nFirst We’ll Clean the Jobs.\n\n\n\n\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Data Exploration of The Cycle - Frontier Weapons\n\n\n\n\n\nWhat Is The Most Solid Single Category of Guns?\n\n\n\n\n\n\nSep 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Lastest Lectures For Fastai are Online.\n\n\n\n\n\nLet’s Make Something Simple!\n\n\n\n\n\n\nSep 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Pawnbarian\n\n\n\n\n\nSolid Puzzle Game\n\n\n\n\n\n\nSep 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Setup V Rising on Linux\n\n\n\n\n\nYour Milage May Vary.\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Do You Create a Normal Map?\n\n\n\n\n\nFeaturing Blender and Gimp!\n\n\n\n\n\n\nFeb 2, 2022\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Eraser\n\n\n\n\n\nI’m in Pain But Nothing Looks Broken\n\n\n\n\n\n\nJan 6, 2022\n\n\n\n\n\n\n  \n\n\n\n\nChallenges for The Year: 2022 Edition\n\n\n\n\n\nPublic Record For Accountability\n\n\n\n\n\n\nDec 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview of 2021 - Challenges, Thoughts\n\n\n\n\n\nDid I accomplish what I wanted and what needs to change?\n\n\n\n\n\n\nDec 8, 2021\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Ethereal Estate\n\n\n\n\n\nMake a Mess With a Fried.\n\n\n\n\n\n\nSep 7, 2021\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: The Forest\n\n\n\n\n\nHow to Eat Your Neighbors.\n\n\n\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit Card Security\n\n\n\n\n\nWhat is Luhn’s Algorithm?\n\n\n\n\n\n\nFeb 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is a Memorandum of Understanding?\n\n\n\n\n\nCybersecurity+ Series.\n\n\n\n\n\n\nFeb 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\nQuick Introduction to Streamlit\n\n\n\n\n\nYou should use this framework too.\n\n\n\n\n\n\nFeb 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinimal Spark Cluster\n\n\n\n\n\nSetting Up Without Hadoop Cluster\n\n\n\n\n\n\nJan 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenges for 2021\n\n\n\n\n\nPublic Record For Accountability\n\n\n\n\n\n\nDec 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBook Review: The Pulp Mindset; A Newpub Survival Mindset\n\n\n\n\n\nUseful But Repetitive\n\n\n\n\n\n\nOct 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Introduction to Python Debugging\n\n\n\n\n\nSome Examples I learned From Debugging My Own Stuff\n\n\n\n\n\n\nOct 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\nGame Review: Post Void\n\n\n\n\n\nNostalgia with a Side of Adrenaline.\n\n\n\n\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGame Review: Empyrion - Galactic Survival\n\n\n\n\n\nComplexity is the Name of the Game.\n\n\n\n\n\n\nSep 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFastbooks Incidentally Supports R\n\n\n\n\n\nR and ggplot!\n\n\n\n\n\n\nSep 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\nA Post Exploring Altair\n\n\n\n\n\nA tutorial of fastpages for Jupyter notebooks.\n\n\n\n\n\n\nSep 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBook Review: Voices of the Void\n\n\n\n\n\nIs Andrew Dalatent Insane?\n\n\n\n\n\n\nJul 9, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nbks/2022-11-16-does-faker-cc-pass-luhns-algo.html",
    "href": "posts/nbks/2022-11-16-does-faker-cc-pass-luhns-algo.html",
    "title": "Does The Faker Package Pass Luhn?",
    "section": "",
    "text": "While taking a class about anonymizing data for public release, Rebeca Gonzalez discussed using the Faker python package to substitute real values for fake values to protect users. One of the functions listed is to generate fake credit card numbers for data.If you’re not aware, there is a patented algorithm which must pass for a generated Credit Card number written by Hans Peter Luhn from IBM. The Algorithm is not complicated and we’ll work through it now.\nLuhn’s Algorithm: 1. The algorithm first should split the number into two parts: the digits and the check value. 2. For the digits and moving left to right: 1. If Odd position then multiply by 2 and return that number; if bigger than 10 return a modulo of it. 2. If Even then simply return that number. 3. Sum the digits after the above computation. 4. Take the modulo 10 of the sum and subtract it from 10. 5. Compare the last digit against the excluded digit and they should match.\nWe will skip the first step since it will be an easy split for now. We’ll want a function which deals with step 2.\n\n# Apply to each Digit\ndef luhnSingleDigit( digit, even = True):\n    if even:\n       # if even, modify and return\n        n =  int(digit) * 2\n        if n == 10: return 0\n        if n >= 10: return 1 + n % 10\n        return n\n    else:\n        # If position is odd, return itself\n        return int(digit)\n\nluhnSingleDigit(8), luhnSingleDigit(8, even = False)\n\n(7, 8)\n\n\nNow we’ll need to iterate through each digits and so we’ll need a fake credit card number. I will reverse the order of the digits since it will be easier than trying to count backwards when we iterate through the digits.\n\nfrom faker import Faker\nfake = Faker()\ncc = fake.credit_card_number()\n\n# reverse the digits\ndigits, parity = cc[:-1:][::-1], cc[-1]\ndigits, parity \n\n('56086526834343', '5')\n\n\nWe will iterate through the digits via list comprehension in python - which is how we’re going to do next. You can have an if-else included in a list comprehension which we’ll need to tell the code which flagged version to use.\n\n# Test the filter and make sure it works\n[0 if i % 2 == 1 else x for i, x in enumerate(range(20))]\n\n[0, 0, 2, 0, 4, 0, 6, 0, 8, 0, 10, 0, 12, 0, 14, 0, 16, 0, 18, 0]\n\n\nNow we’ll throw the function in there to test it.\n\n[ luhnSingleDigit(x, even = False) if i % 2 == 1 else luhnSingleDigit(x) for i, x in enumerate(digits) ]\n\n[0, 6, 0, 8, 3, 5, 4, 6, 7, 3, 8, 3, 8, 3]\n\n\nNext we have to sum them all together and take the last digit.\n\ntheSum = sum([ luhnSingleDigit(x, even = False) if i % 2 == 1 else luhnSingleDigit(x) for i, x in enumerate(digits) ])\ntheSum\n\n64\n\n\nLastly, we do the modulo 10, subtract 10 from the computed sum and compare it to the parity digit\n\ntheSum, 10 - theSum % 10, 10 - theSum % 10 == parity\n\n(64, 6, False)\n\n\nIt was not valid credit card number!  Let’s get those steps into a function and time it to see how long it takes. And, let’s get about 100 of these numbers to check just in case it was a fluke.\n\ndef confirmLuhn(number):\n    digits, parity = cc[:-1:][::-1], cc[-1]\n    theSum = sum([\n        luhnSingleDigit(x, even = False) if i % 2 == 1\n        else luhnSingleDigit(x)\n        for i, x in enumerate(digits) ]\n    )\n\n    return 10 - theSum % 10 == parity\n\nOne thing I noticed from the output in the lectures was that there were different length fake card numbers; is that the case here too?\n\nimport numpy as np\nnumbers = [ fake.credit_card_number() for _ in range(100)]\nnp.mean([len(n) for n in numbers])\n\n15.47\n\n\nThat’s correct. Some of them are simply too short to be a valid credit card number. I’m not sure why this is but that’s beyond the scope of this post so we will simply filter them and move on.\n\nfilteredNumbers = list(filter( lambda x: len(x) == 16, numbers))\nlen(filteredNumbers)\n\n51\n\n\nOuch. That’s a lot misses. Ok, on to the main event!\n\n%timeit sum([confirmLuhn(c) for c in filteredNumbers])\n\n179 µs ± 820 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\nsum([confirmLuhn(c) for c in filteredNumbers])\n\n0\n\n\nSo, it looks like there were no valid credit card numbers in the list but at least it was fast. Good to know that the numbers being generated are not constricted to valid numbers."
  },
  {
    "objectID": "posts/nbks/2020-09-06-exploring-altair-visualization.html",
    "href": "posts/nbks/2020-09-06-exploring-altair-visualization.html",
    "title": "A Post Exploring Altair",
    "section": "",
    "text": "This Is The Tool We Have\nI’m trying out the fastpages in hopes that I wont have to spend the time building out my own website toolset. I’ve been slowly building something out of Wagtail which is a really just Djnago with some bells. The real allure though is going to be the Notebook conversions - specifically the Data Visualizations. The library for interactive version is Altair and we’re going to explore some data!\n\n\nLets Explore!\nSo, the tutorial for Scatter Plot uses the car data but I figured we mind as well do the classic Iris dataset. We’ll start by importing the dataset from vega_datasets - which is the Javascript library that Altair is built on top of - using iris = data.iris().\n\niris = data.iris()\niris\n\n\n\n\n\n  \n    \n      \n      sepalLength\n      sepalWidth\n      petalLength\n      petalWidth\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      145\n      6.7\n      3.0\n      5.2\n      2.3\n      virginica\n    \n    \n      146\n      6.3\n      2.5\n      5.0\n      1.9\n      virginica\n    \n    \n      147\n      6.5\n      3.0\n      5.2\n      2.0\n      virginica\n    \n    \n      148\n      6.2\n      3.4\n      5.4\n      2.3\n      virginica\n    \n    \n      149\n      5.9\n      3.0\n      5.1\n      1.8\n      virginica\n    \n  \n\n150 rows × 5 columns\n\n\n\nFirst lets see the graph - and then we’ll discuss the functions\n\nalt\\\n    .Chart(iris)\\\n    .mark_point()\\\n    .encode(\n        x='sepalLength',\n        y='sepalWidth',\n        color='species',\n        tooltip = ['species', 'petalLength', 'petalWidth']\n    )\\\n    .interactive()\n\n\n\n\n\n\nIt is interesting to note that - per the Docs - : > Create a basic Altair/Vega-Lite chart. > > Although it is possible to set all Chart properties as constructor attributes, > it is more idiomatic to use methods such as mark_point(), encode(), > transform_filter(), properties(), etc.\n.. which means that it’s found a way to do something similar to the R Programming Languages pipe operator. For reference, it would looks something like this:\nmtcars %>% \n    ggplot(aes(wt, mpg)) +\n    geom_point(aes(colour = factor(cyl)))\n\n\n\nExample R Graph\n\n\nFirst we tell altair to make a chart using the dataset we’re using:\n.Chart(iris)\n… which is then followed by the kind of graph that we’re after - in this case we’re after a scatter plot:\n.mark_point()\n… and then we tell it where everything belongs.\n.encode(\n    x='sepalLength',\n    y='sepalWidth',\n    color='species',\n    tooltip = ['species', 'petalLength', 'petalWidth']\n)\nOf interest is that you can add data from the other columns easily using the tooltip without having to add anything extra. Layering information which is relevant but lacks a meaningful graphic representation was a nice touch.\nThen of course, you allow users to interact with it via:\n.interactive()\nLets see what this post looks like on the blog!"
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "",
    "text": "Welcome back to the fourth post in the series. Last time, we were having issues with the data since there were missing values - including the Job that prompted the initial investigation. We’ll jump right into it picking up with the mismatch between naming in tables.\nWhile working through the list of problems I found reviewing the data, there is clearly an error in the Regex built before. The symptom of this was the Autoloader which has a hanging number when the group is pulled out.\n\ntasksSubset = siteJobs[1][[\"Name\", \"Description\", \"Tasks\"]].copy()\ntasksSubset = tasksSubset[ ~tasksSubset.Tasks.isna()]\ntasksSubset = tasksSubset[ ~tasksSubset.Tasks.str.contains(\"Kill\")]\n\nregex = r\"(\\d+\\s[\\w]+\\s[\\w]+)\"\ntmp = tasksSubset.Tasks.str.extractall(regex)\ntmp[tmp.reset_index()[0].str.contains('Autoloader').values]\n\n\n\n\n\n  \n    \n      \n      \n      0\n    \n    \n      \n      match\n      \n    \n  \n  \n    \n      8\n      0\n      1 Autoloader 5\n    \n    \n      32\n      0\n      2 Autoloader 8\n    \n  \n\n\n\n\nAfter some work, this was because the hanging [\\w]+ was too greedy and had to be toned down.\n\nnewRegex = r\"(\\d+\\s[\\w]+\\s?[a-zA-Z]+)\"\ntmp = tasksSubset.Tasks.str.extractall(newRegex)\ntmp[tmp.reset_index()[0].str.contains('Autoloader').values]\n\n\n\n\n\n  \n    \n      \n      \n      0\n    \n    \n      \n      match\n      \n    \n  \n  \n    \n      8\n      0\n      1 Autoloader\n    \n    \n      32\n      0\n      2 Autoloader\n    \n  \n\n\n\n\nThis didn’t end up being the only correction to the Regex; in fact, the Co-Tool Multitool was being broken apart since it has a - in the name. So, we had to update this to check and include the - on the split.\n\nnewRegex = r\"((\\d+\\s[\\w\\-]+\\s?[a-zA-Z]+))\"\n\nNow we’ll have our updated version of the breakLoot function we write before.\n\ndef breakLoot(taskString, index=0):\n    parts = taskString.split(' ', maxsplit=1)\n    if index == 0:\n        return int(parts[index])\n    elif index == 1:\n        return parts[index]\n    else:\n        # This shouldn't be called.\n        return None\n\ntasks = []\n\nfor index in range(0,3):\n    tasksSubset = siteJobs[index][[\"Name\", \"Description\", \"Tasks\"]].copy()\n    tasksSubset = tasksSubset[ ~tasksSubset.Tasks.isna()]\n    tasksSubset = tasksSubset[ ~tasksSubset.Tasks.str.contains(\"Kill\")]\n\n    newRegex = r\"((\\d+\\s[\\w\\-]+\\s?[a-zA-Z]+))\"\n    tmp = tasksSubset.Tasks.str.extractall(newRegex)\n\n    count = tmp.reset_index()[0].apply(breakLoot).values\n    aLoot = tmp.reset_index()[0].apply(breakLoot, index=1).values\n\n    tmp = tmp.assign(\n        count = count,\n        loot = aLoot\n    )\n\n    nameDescriptSlice = tasksSubset.loc[tmp.reset_index()[\"level_0\"], ['Name', 'Description']]\n\n    tmp = tmp.assign(\n        name = nameDescriptSlice.Name.values,\n        description = nameDescriptSlice.Description.values\n    )\n\n    taskSlice = tmp.reset_index().drop([\n        'level_0',\n        'match',\n        0\n    ], axis =1 )\n\n    taskSlice = taskSlice[['name', 'count', 'loot', 'description']]\n    tasks.append(taskSlice)\ntasks = pd.concat([*tasks])"
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#fix-item-names",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#fix-item-names",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Fix Item Names",
    "text": "Fix Item Names\nNow we can continue and move to fixing the name mismatches like intended. There was quite a few of these so I’ll only show the process for a few of them - and then the fix for all of them. We’ll pull the code from the previous post so we can start by using the 0 values and work backwards.\n\nallJobs = pd.concat([korolevRewards, icaRewards, osirisRewards])\nlootKMarks = loot.query('Name == \"K-Marks\"')\ntasks = tasks.copy()\ntaskLoot = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='outer')\ntaskLoot['Cost'] = taskLoot['count'] * taskLoot['Unit']\nresults = allJobs.query('Rewards == \"K-Marks\"').merge(\n    taskLoot[['name', 'Cost']].groupby('name').sum(), left_on=\"Job\", right_on=\"name\", how='left')\nresults['Balance'] = results['Units'] - results['Cost']\n\n# How many are there?\nlen(results.query(\"Cost == 0\"))\n\n30\n\n\nThere are 31 rows with a Cost of 0 which we’ll need to investigate. The first one which I had noticed and corrected in the previous post was the CPU’s so we’ll address this one first. We’ll need to find the values in the tasks table and the lootKMarks table so we can figure out where the disconnect is.\n\ntasks.loc[tasks.loot.str.contains(\"Master|Zero\")]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      3\n      Mining Bot\n      2\n      Zero Systems\n      Our engineers have designed an autonomous mini...\n    \n    \n      6\n      Insufficient Processing Power\n      1\n      Master Unit\n      Prospector! The Zero Systems CPU you brought u...\n    \n    \n      12\n      Automated Security\n      5\n      Zero Systems\n      We will have to build new turrets to help prot...\n    \n    \n      16\n      Classified I\n      2\n      Master Unit\n      Prospector! We need Derelict Explosives, Maste...\n    \n    \n      47\n      Upgrades\n      1\n      Master Unit\n      We want to take over an ICA Data Center. Stash...\n    \n    \n      9\n      Sabotage\n      4\n      Zero Systems\n      Prospector! Those Korolev simpletons are drill...\n    \n    \n      34\n      Spare Parts\n      4\n      Zero Systems\n      One of the Servers at Starport has been failin...\n    \n    \n      40\n      NEW-Hard-ICA-Gather-6\n      1\n      Zero Systems\n      DESCRIPTION MISSING\n    \n    \n      11\n      Data Center Upgrades\n      2\n      Master Unit\n      Turns out our data center was not powerful eno...\n    \n  \n\n\n\n\n\nlootKMarks.loc[lootKMarks.Loot.str.contains(\"CPU\")]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Personal Quarters\n      Campaigns\n      Jobs\n      Printing\n      Loot\n    \n  \n  \n    \n      131\n      3845.0\n      K-Marks\n      Epic\n      Yes x3\n      Yes x3\n      Yes\n      No\n      Master Unit CPU\n    \n    \n      321\n      506.0\n      K-Marks\n      Rare\n      Yes x37\n      Yes x6\n      Yes\n      No\n      Zero Systems CPU\n    \n  \n\n\n\n\nThe tasks table has a shortened version of its name so the easiest way to fix this would be to append CPU to the loot name in the tasks table.\n\ntasks.loc[ tasks.loot == \"Master Unit\", 'loot'] = 'Master Unit CPU'\ntasks.loc[ tasks.loot == \"Master Unit CPU\", 'loot']\n\n6     Master Unit CPU\n16    Master Unit CPU\n47    Master Unit CPU\n11    Master Unit CPU\nName: loot, dtype: object\n\n\nAnother was the Pure Focus Crystals which has the same problem of having the name truncated; this is a trend among all the affected materials with higher tiered materials. Those will all be included in the final correction and will skip showing the process; it’s literally just the same and tedious.\n\n# Missing the Crystal part\ntasks.loc[tasks.loot.str.contains(\"Pure Focus\")]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      25\n      Geologist\n      1\n      Pure Focus\n      You got time for a job, Prospector? The sample...\n    \n    \n      26\n      Industry Secret\n      3\n      Pure Focus\n      Hm. Interesting. The Pure Focus Crystals you b...\n    \n    \n      28\n      Laser Rifles\n      3\n      Pure Focus\n      We are prototyping a new laser rifle that can ...\n    \n    \n      16\n      Arms Race\n      4\n      Pure Focus\n      Prospector. It seems like Korolev is working o...\n    \n  \n\n\n\n\n\nlootKMarks.loc[lootKMarks.Loot.str.contains(\"Crystal\")]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Personal Quarters\n      Campaigns\n      Jobs\n      Printing\n      Loot\n    \n  \n  \n    \n      56\n      961.0\n      K-Marks\n      Rare\n      Yes x19\n      Yes x10\n      Yes\n      No\n      Focus Crystal\n    \n    \n      61\n      2883.0\n      K-Marks\n      Epic\n      Yes x6\n      Yes x9\n      Yes\n      No\n      Pure Focus Crystal\n    \n    \n      66\n      38924.0\n      K-Marks\n      Legendary\n      Yes x6\n      Yes x9\n      Yes\n      No\n      Polirium Crystal\n    \n    \n      81\n      1709.0\n      K-Marks\n      Epic\n      Yes x36\n      Yes x9\n      Yes\n      No\n      Teratomorphic Crystal Core\n    \n  \n\n\n\n\nThis one was a surprise since truncating it doesn’t make any sense and gains nothing but the Magnetic Field Stabilizer is being truncated as well.\n\n# Missing the Stabilizer:\ntasks.loc[tasks.loot.str.contains(\"Magnetic\")]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      42\n      Stability is Key\n      1\n      Magnetic Field\n      Our Comms are jammed. We need you to stash a M...\n    \n    \n      43\n      Stability is Key\n      1\n      Magnetic Field\n      Our Comms are jammed. We need you to stash a M...\n    \n    \n      44\n      Stability is Key\n      1\n      Magnetic Field\n      Our Comms are jammed. We need you to stash a M...\n    \n    \n      28\n      Storm Interference\n      2\n      Magnetic Field\n      Prospector, the Storm has been distorting all ...\n    \n    \n      2\n      Surveillance Center\n      2\n      Magnetic Field\n      We want to expand our surveillance operations ...\n    \n  \n\n\n\n\n\nlootKMarks.loc[lootKMarks.Loot.str.contains(\"Magnetic\")]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Personal Quarters\n      Campaigns\n      Jobs\n      Printing\n      Loot\n    \n  \n  \n    \n      171\n      338.0\n      K-Marks\n      Uncommon\n      Yes x71\n      Yes x25\n      Yes\n      No\n      Magnetic Field Stabilizer\n    \n  \n\n\n\n\nLastly, we’ll show the NiC Oil which was a problem from the previous post which is missing the Cannister at the end again.\n\ntasks.loc[tasks.loot.str.contains(\"Oil\")]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      41\n      Striking Big\n      10\n      NiC Oil\n      Damn it! We ran out of Fuel for our Radiation ...\n    \n  \n\n\n\n\n\nlootKMarks.loc[lootKMarks.Loot.str.contains(\"Oil\")]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Personal Quarters\n      Campaigns\n      Jobs\n      Printing\n      Loot\n    \n  \n  \n    \n      121\n      20183.0\n      K-Marks\n      Epic\n      Yes x103\n      Yes x10\n      Yes\n      No\n      NiC Oil Cannister\n    \n  \n\n\n\n\nThere we go! Now, we’ll update the data again and run the merge once more.\n\n# Most corrections:\ntasks.loc[ tasks.loot == \"Master Unit\", 'loot'] = 'Master Unit CPU'\ntasks.loc[ tasks.loot == \"Zero Systems\", 'loot'] = 'Zero Systems CPU'\ntasks.loc[ tasks.loot == \"Pure Focus\", 'loot'] = 'Pure Focus Crystal'\ntasks.loc[ tasks.loot == \"Heavy Mining\", 'loot'] = 'Heavy Mining Tool'\ntasks.loc[ tasks.loot == \"Magnetic Field\", 'loot'] = 'Magnetic Field Stabilizer'\ntasks.loc[ tasks.loot == \"Brittle Titan\", 'loot'] = 'Brittle Titan Ore'\ntasks.loc[ tasks.loot == \"NiC Oil\", 'loot'] = 'NiC Oil Cannister'\ntasks.loc[ tasks.loot == \"Charged Spinal\", 'loot'] = 'Charged Spinal Base'\ntasks.loc[ tasks.loot == \"Hardened Bone\", 'loot'] = 'Hardened Bone Plates'\ntasks.loc[ tasks.loot == \"Pale Ivy\", 'loot'] = 'Pale Ivy Blossom'\ntasks.loc[ tasks.loot == \"Glowy Brightcap\", 'loot'] = 'Glowy Brightcap Mushroom'\ntasks.loc[ tasks.loot == \"Blue Runner\", 'loot'] = 'Blue Runner Egg'\ntasks.loc[ tasks.loot == \"Magic\", 'loot'] = 'Magic-GROW Fertilizer'\ntasks.loc[ tasks.loot == \"Letium\", 'loot'] = 'Letium Clot'\ntasks.loc[ tasks.loot == \"Azure Tree\", 'loot'] = 'Azure Tree Bark'\n\n\ntasks = tasks.copy()\ntaskLoot = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='outer')\ntaskLoot['Cost'] = taskLoot['count'] * taskLoot['Unit']\nresults = allJobs.query('Rewards == \"K-Marks\"').merge(\n    taskLoot[['name', 'Cost']].groupby('name').sum(), left_on=\"Job\", right_on=\"name\", how='left')\nresults['Balance'] = results['Units'] - results['Cost']\n\nlen(results.query(\"Cost == 0\"))\n\n13\n\n\nWe’ve made some good progress so far but we’ve still got quite a few jobs to update. Let’s check the list of missing valuse once more and see if there is a new pattern here.\n\ntmp = tasks.merge(results[['Job', 'Cost']], left_on=\"name\", right_on=\"Job\", how='left').query(\"Cost == 0\")\ntmp\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Job\n      Cost\n    \n  \n  \n    \n      40\n      Excavation Gear\n      1\n      Heavy Mining Tool\n      For excavations, we need you to stash a Heavy ...\n      Excavation Gear\n      0.0\n    \n    \n      46\n      And two smoking Barrels\n      1\n      PKR Maelstrom\n      Prospector. Get down there and stash a PKR Mae...\n      And two smoking Barrels\n      0.0\n    \n    \n      47\n      And two smoking Barrels\n      200\n      Shotgun Ammo\n      Prospector. Get down there and stash a PKR Mae...\n      And two smoking Barrels\n      0.0\n    \n    \n      74\n      Data Drive II\n      1\n      Rare Data\n      The Data you brought us was helpful, but we ne...\n      Data Drive II\n      0.0\n    \n    \n      75\n      Data Drive III\n      2\n      Rare Data\n      Good work last time, Prospector. The Data was ...\n      Data Drive III\n      0.0\n    \n    \n      76\n      Data Drive IV\n      1\n      Epic Data\n      In order to be able to predict Storm Behaviour...\n      Data Drive IV\n      0.0\n    \n    \n      77\n      Data Drive V\n      1\n      Legendary Data\n      Yes! The more precise Data you brought us was ...\n      Data Drive V\n      0.0\n    \n    \n      78\n      Data Drive VI\n      3\n      Legendary Data\n      We're finding more than just Storm data now......\n      Data Drive VI\n      0.0\n    \n    \n      82\n      Grenadier\n      1\n      Frag Grenade\n      Prospector. You have heard of Badum's Dead Dro...\n      Grenadier\n      0.0\n    \n    \n      86\n      Ammo Supplies\n      1000\n      Medium Ammo\n      Our Field Agents need more Ammo if they are to...\n      Ammo Supplies\n      0.0\n    \n    \n      87\n      Data Drop\n      2\n      Rare Data\n      Prospector. One of our Scientists is convinced...\n      Data Drop\n      0.0\n    \n    \n      90\n      Provide an Advocate\n      1\n      Advocate at\n      Our field agent requested better gear to take ...\n      Provide an Advocate\n      0.0\n    \n    \n      129\n      Loadout Drop\n      1\n      Rare Shield\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      130\n      Loadout Drop\n      1\n      Rare Helmet\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      131\n      Loadout Drop\n      1\n      Rare Backpack\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      138\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NEW-Hard-Osiris-EliteCrusher-1\n      0.0\n    \n  \n\n\n\n\nThe biggest standout problem here now is all those data drive quests so lets resolve those next."
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#data-drives",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#data-drives",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Data Drives:",
    "text": "Data Drives:\nWe have another problem though since none of the tables we have actually contains the data we’re after. So, we’re back to the wiki to do some more scraping work. If we also look at the names of the data drives we’re going to have another problem soon - which you’ll see when we download the data.\n\n# Missing the Data Drive\ntasks.loc[tasks.loot.str.contains(\"Data\")]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      19\n      Data Drive II\n      1\n      Rare Data\n      The Data you brought us was helpful, but we ne...\n    \n    \n      20\n      Data Drive III\n      2\n      Rare Data\n      Good work last time, Prospector. The Data was ...\n    \n    \n      21\n      Data Drive IV\n      1\n      Epic Data\n      In order to be able to predict Storm Behaviour...\n    \n    \n      22\n      Data Drive V\n      1\n      Legendary Data\n      Yes! The more precise Data you brought us was ...\n    \n    \n      23\n      Data Drive VI\n      3\n      Legendary Data\n      We're finding more than just Storm data now......\n    \n    \n      32\n      Data Drop\n      2\n      Rare Data\n      Prospector. One of our Scientists is convinced...\n    \n  \n\n\n\n\nLike the previous posts, scraping is a tedious process of matching keywords and pulling the right tables so that’s getting skipped; it’s just mostly trial and error.\n\nurlDataDrives = 'https://thecyclefrontier.wiki/wiki/Utilities#Data_Drives-0'\nsiteDrive = pd.read_html(urlDataDrives, attrs={\"class\":\"zebra\"})[2]\n\nWe’ll use a modified version of the code we wrote before for parsing the loot table for this.\n\ndriveSubset = siteDrive[\n    ['Image', 'Name', 'Rarity', 'Weight']\n    ].copy()\n\ndriveSubset = driveSubset.assign(\n    Loot = np.NaN\n)\n\nindex = range( 0, len(driveSubset) - 2, 3)\noffset = np.array([1, 2])\n\nfor i in index:\n    \n    aLoot = driveSubset.iloc[i, 1]\n    indexes = i + offset\n    driveSubset.iloc[indexes, 4] = aLoot\n\n\ntmp = driveSubset.iloc[:, 1:4]\ntmp = tmp.fillna(method=\"ffill\")\ndriveSubset.iloc[:, 1:4] = tmp\n\ncutNA = driveSubset.Loot.isna()\ndriveData = driveSubset[ ~cutNA ]\ndriveData = driveData.rename(columns={'Image':'Unit', 'Name':'Reward'})\ndriveData['Rarity'] = pd.Categorical(\n    driveData.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']\n)\ndriveData['Label'] = driveData['Rarity'].astype('str') + ' Data'\ndrives = driveData\n\ndrives\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Weight\n      Loot\n      Label\n    \n  \n  \n    \n      1\n      30.0\n      K-Marks\n      Common\n      15.0\n      Data Drive Tier 1\n      Common Data\n    \n    \n      2\n      0.0\n      Reputation\n      Common\n      15.0\n      Data Drive Tier 1\n      Common Data\n    \n    \n      4\n      1013.0\n      K-Marks\n      Uncommon\n      15.0\n      Data Drive Tier 2\n      Uncommon Data\n    \n    \n      5\n      1.0\n      Reputation\n      Uncommon\n      15.0\n      Data Drive Tier 2\n      Uncommon Data\n    \n    \n      7\n      2531.0\n      K-Marks\n      Rare\n      15.0\n      Data Drive Tier 3\n      Rare Data\n    \n    \n      8\n      3.0\n      Reputation\n      Rare\n      15.0\n      Data Drive Tier 3\n      Rare Data\n    \n    \n      10\n      6075.0\n      K-Marks\n      Epic\n      15.0\n      Data Drive Tier 4\n      Epic Data\n    \n    \n      11\n      6.0\n      Reputation\n      Epic\n      15.0\n      Data Drive Tier 4\n      Epic Data\n    \n    \n      13\n      10252.0\n      K-Marks\n      Legendary\n      15.0\n      Data Drive Tier 5\n      Legendary Data\n    \n    \n      14\n      10.0\n      Reputation\n      Legendary\n      15.0\n      Data Drive Tier 5\n      Legendary Data\n    \n  \n\n\n\n\nSince this is the second time that we’ve needed this - and we’re going to need this again - we should write a function to wrap this whole process.\n\n# this is the function, where:\n## siteData: the table from the scraped site\n## columns: the columns you want to keep from the scraped data\n## adjust: the count from the bottom containing an index\n## step: how many rows between values we care about\n## offset: how many rewards are there?\ndef extractSite(siteData, columns, adjust, step,  offset):\n    if not isinstance(columns, list):\n        print(\"Columns argument must be a list.\")\n        return None\n    siteSubset = siteData[columns].copy()\n\n    siteSubset = siteSubset.assign(\n        Loot = np.NaN\n    )\n\n    # Some extra error handling \n    if not isinstance(adjust, int):\n        print(\"adjust argument must be an int.\")\n        return None\n    if not isinstance(step, int):\n        print(\"step argument must be an int.\")\n        return None\n    if not isinstance(offset, list):\n        print(\"offset argument must be a list.\")\n        return None\n    \n    index = range( 0, len(siteSubset) - adjust, step)\n    offset = np.array(offset)\n\n    for i in index:\n        aLoot = siteSubset.iloc[i, 1]\n        indexes = i + offset\n        siteSubset.iloc[indexes, len(siteSubset.columns)-1] = aLoot\n\n    tmp = siteSubset.iloc[:, 1:len(siteSubset.columns)]\n    tmp = tmp.fillna(method=\"ffill\")\n    siteSubset.iloc[:, 1:len(siteSubset.columns)-1] = tmp\n\n    cutNA = siteSubset.Loot.isna()\n    returnData = siteSubset[ ~cutNA ]\n    returnData = returnData.rename(columns={'Image':'Unit', 'Name':'Reward'})\n\n    return returnData\n\nNow we’ll sanity check this to make sure it works.\n\ndrives = extractSite(siteDrive, ['Image', 'Name', 'Rarity', 'Weight'], 2, 3, [1, 2])\ndrives['Rarity'] = pd.Categorical(\n        drives.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']\n    )\ndrives\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Weight\n      Loot\n    \n  \n  \n    \n      1\n      30.0\n      K-Marks\n      Common\n      15.0\n      Data Drive Tier 1\n    \n    \n      2\n      0.0\n      Reputation\n      Common\n      15.0\n      Data Drive Tier 1\n    \n    \n      4\n      1013.0\n      K-Marks\n      Uncommon\n      15.0\n      Data Drive Tier 2\n    \n    \n      5\n      1.0\n      Reputation\n      Uncommon\n      15.0\n      Data Drive Tier 2\n    \n    \n      7\n      2531.0\n      K-Marks\n      Rare\n      15.0\n      Data Drive Tier 3\n    \n    \n      8\n      3.0\n      Reputation\n      Rare\n      15.0\n      Data Drive Tier 3\n    \n    \n      10\n      6075.0\n      K-Marks\n      Epic\n      15.0\n      Data Drive Tier 4\n    \n    \n      11\n      6.0\n      Reputation\n      Epic\n      15.0\n      Data Drive Tier 4\n    \n    \n      13\n      10252.0\n      K-Marks\n      Legendary\n      15.0\n      Data Drive Tier 5\n    \n    \n      14\n      10.0\n      Reputation\n      Legendary\n      15.0\n      Data Drive Tier 5\n    \n  \n\n\n\n\nPerfect! Now we just add this to our list of adjustments right? Sadly no. If you look at the names of the drives you’ll find that we’re not quite there. The names of the drives were renamed in Season 2 but the values in our tasks were not updated from their previous values. This is not too hard to update since the old drive names were just the Rarity + Data and we have that so we’ll just need a new column.\n\ndrives['Loot'] = drives['Rarity'].astype('str') + ' Data'\ndrives\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Weight\n      Loot\n    \n  \n  \n    \n      1\n      30.0\n      K-Marks\n      Common\n      15.0\n      Common Data\n    \n    \n      2\n      0.0\n      Reputation\n      Common\n      15.0\n      Common Data\n    \n    \n      4\n      1013.0\n      K-Marks\n      Uncommon\n      15.0\n      Uncommon Data\n    \n    \n      5\n      1.0\n      Reputation\n      Uncommon\n      15.0\n      Uncommon Data\n    \n    \n      7\n      2531.0\n      K-Marks\n      Rare\n      15.0\n      Rare Data\n    \n    \n      8\n      3.0\n      Reputation\n      Rare\n      15.0\n      Rare Data\n    \n    \n      10\n      6075.0\n      K-Marks\n      Epic\n      15.0\n      Epic Data\n    \n    \n      11\n      6.0\n      Reputation\n      Epic\n      15.0\n      Epic Data\n    \n    \n      13\n      10252.0\n      K-Marks\n      Legendary\n      15.0\n      Legendary Data\n    \n    \n      14\n      10.0\n      Reputation\n      Legendary\n      15.0\n      Legendary Data\n    \n  \n\n\n\n\n\n# drives\npd.concat(\n    [lootKMarks.rename({\"Name\":\"Reward\"}, axis=1)[['Unit', 'Reward', 'Rarity', 'Loot']],\n    drives.query('Reward == \"K-Marks\"').drop(\"Weight\", axis=1)])\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      1\n      150.0\n      K-Marks\n      Common\n      Flawed Veltecite\n    \n    \n      6\n      570.0\n      K-Marks\n      Uncommon\n      Cloudy Veltecite\n    \n    \n      11\n      854.0\n      K-Marks\n      Rare\n      Clear Veltecite\n    \n    \n      16\n      1922.0\n      K-Marks\n      Epic\n      Pure Veltecite\n    \n    \n      21\n      6487.0\n      K-Marks\n      Legendary\n      Veltecite Heart\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1\n      30.0\n      K-Marks\n      Common\n      Common Data\n    \n    \n      4\n      1013.0\n      K-Marks\n      Uncommon\n      Uncommon Data\n    \n    \n      7\n      2531.0\n      K-Marks\n      Rare\n      Rare Data\n    \n    \n      10\n      6075.0\n      K-Marks\n      Epic\n      Epic Data\n    \n    \n      13\n      10252.0\n      K-Marks\n      Legendary\n      Legendary Data\n    \n  \n\n99 rows × 4 columns\n\n\n\nAdd that to our pipeline and let’s see where we are now\n\nallJobs = pd.concat([korolevRewards, icaRewards, osirisRewards])\nlootKMarks = loot.query('Name == \"K-Marks\"')\n\n# Most corrections:\ntasks.loc[ tasks.loot == \"Master Unit\", 'loot'] = 'Master Unit CPU'\ntasks.loc[ tasks.loot == \"Zero Systems\", 'loot'] = 'Zero Systems CPU'\ntasks.loc[ tasks.loot == \"Pure Focus\", 'loot'] = 'Pure Focus Crystal'\ntasks.loc[ tasks.loot == \"Heavy Mining\", 'loot'] = 'Heavy Mining Tool'\ntasks.loc[ tasks.loot == \"Magnetic Field\", 'loot'] = 'Magnetic Field Stabilizer'\ntasks.loc[ tasks.loot == \"Brittle Titan\", 'loot'] = 'Brittle Titan Ore'\ntasks.loc[ tasks.loot == \"NiC Oil\", 'loot'] = 'NiC Oil Cannister'\ntasks.loc[ tasks.loot == \"Charged Spinal\", 'loot'] = 'Charged Spinal Base'\ntasks.loc[ tasks.loot == \"Hardened Bone\", 'loot'] = 'Hardened Bone Plates'\ntasks.loc[ tasks.loot == \"Pale Ivy\", 'loot'] = 'Pale Ivy Blossom'\ntasks.loc[ tasks.loot == \"Glowy Brightcap\", 'loot'] = 'Glowy Brightcap Mushroom'\ntasks.loc[ tasks.loot == \"Blue Runner\", 'loot'] = 'Blue Runner Egg'\ntasks.loc[ tasks.loot == \"Magic\", 'loot'] = 'Magic-GROW Fertilizer'\ntasks.loc[ tasks.loot == \"Letium\", 'loot'] = 'Letium Clot'\ntasks.loc[ tasks.loot == \"Azure Tree\", 'loot'] = 'Azure Tree Bark'\ntasks = tasks.copy()\n\n# Extract Stuff here for now:\ndrives = extractSite(siteDrive, ['Image', 'Name', 'Rarity', 'Weight'], 2, 3, [1, 2])\ndrives['Rarity'] = pd.Categorical(\n        drives.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']\n)\ndrives['Loot'] = drives['Rarity'].astype('str') + ' Data'\n\nlootKMarks = pd.concat(\n    [lootKMarks.rename({\"Name\":\"Reward\"}, axis=1)[['Unit', 'Reward', 'Rarity', 'Loot']],\n    drives.query('Reward == \"K-Marks\"').drop(\"Weight\", axis=1)])\n\ntaskLoot = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='outer')\ntaskLoot['Cost'] = taskLoot['count'] * taskLoot['Unit']\nresults = allJobs.query('Rewards == \"K-Marks\"').merge(\n    taskLoot[['name', 'Cost']].groupby('name').sum(), left_on=\"Job\", right_on=\"name\", how='left')\nresults['Balance'] = results['Units'] - results['Cost']\n\nlen(results.query(\"Cost == 0\"))\n\n7\n\n\nThat’s much better! So, what’s left to do?\n\ntmp = tasks.merge(results[['Job', 'Cost']], left_on=\"name\", right_on=\"Job\", how='left').query(\"Cost == 0\")\ntmp\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Job\n      Cost\n    \n  \n  \n    \n      40\n      Excavation Gear\n      1\n      Heavy Mining Tool\n      For excavations, we need you to stash a Heavy ...\n      Excavation Gear\n      0.0\n    \n    \n      46\n      And two smoking Barrels\n      1\n      PKR Maelstrom\n      Prospector. Get down there and stash a PKR Mae...\n      And two smoking Barrels\n      0.0\n    \n    \n      47\n      And two smoking Barrels\n      200\n      Shotgun Ammo\n      Prospector. Get down there and stash a PKR Mae...\n      And two smoking Barrels\n      0.0\n    \n    \n      82\n      Grenadier\n      1\n      Frag Grenade\n      Prospector. You have heard of Badum's Dead Dro...\n      Grenadier\n      0.0\n    \n    \n      86\n      Ammo Supplies\n      1000\n      Medium Ammo\n      Our Field Agents need more Ammo if they are to...\n      Ammo Supplies\n      0.0\n    \n    \n      90\n      Provide an Advocate\n      1\n      Advocate at\n      Our field agent requested better gear to take ...\n      Provide an Advocate\n      0.0\n    \n    \n      129\n      Loadout Drop\n      1\n      Rare Shield\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      130\n      Loadout Drop\n      1\n      Rare Helmet\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      131\n      Loadout Drop\n      1\n      Rare Backpack\n      One of our more... lethal assets on Fortuna re...\n      Loadout Drop\n      0.0\n    \n    \n      138\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NEW-Hard-Osiris-EliteCrusher-1\n      0.0\n    \n  \n\n\n\n\n\nAdd the Guns\nWe’re going to move to getting the gun data included since we already actualy have it. This was part of another post which was done - not included in the series.\n\nurl = \"https://thecyclefrontier.wiki/wiki/Weapons\"\nsiteGun = pd.read_html(url, attrs={\"class\":\"zebra\"})[0]\n\ngunData = siteGun[~siteGun.Type.isna()]\nindx = gunData['Proj. Speed'] == 'Hitscan'\ngunData.loc[indx, 'Proj. Speed'] = np.NaN\n\ngunData = gunData.assign(\n    Unit = gunData['Sell Value'].str.replace(' K-Marks', '').astype('float'),\n    Reward = \"K-Marks\",\n    Loot = gunData['Name']\n)\n\n# # This removes the legendary weapons\n# data = data.query('Faction != \"Printing\"')\n\nguns = gunData\nguns[['Unit', 'Reward', 'Rarity', 'Loot']].head(15)\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      0\n      17429.0\n      K-Marks\n      Epic\n      Advocate\n    \n    \n      3\n      524.0\n      K-Marks\n      Common\n      AR-55 Autorifle\n    \n    \n      6\n      12341.0\n      K-Marks\n      Epic\n      Asp Flechette Gun\n    \n    \n      9\n      371.0\n      K-Marks\n      Common\n      B9 Trenchgun\n    \n    \n      12\n      63080.0\n      K-Marks\n      Exotic\n      Basilisk\n    \n    \n      15\n      1918.0\n      K-Marks\n      Uncommon\n      Bulldog\n    \n    \n      18\n      2052.0\n      K-Marks\n      Common\n      C-32 Bolt Action\n    \n    \n      21\n      17429.0\n      K-Marks\n      Epic\n      Gorgon\n    \n    \n      24\n      16805.0\n      K-Marks\n      Exotic\n      Hammer\n    \n    \n      27\n      7143.0\n      K-Marks\n      Rare\n      ICA Guarantee\n    \n    \n      30\n      228.0\n      K-Marks\n      Common\n      K-28\n    \n    \n      33\n      325513.0\n      K-Marks\n      Legendary\n      KARMA-1\n    \n    \n      36\n      22781.0\n      K-Marks\n      Epic\n      KBR Longshot\n    \n    \n      39\n      94459.0\n      K-Marks\n      Exotic\n      Kinetic Arbiter\n    \n    \n      42\n      1918.0\n      K-Marks\n      Uncommon\n      KM-9 'Scrapper'\n    \n  \n\n\n\n\nNow we have the gun data to be added to the loot table but if you were paying attention you may have noticed something is wrong with one of our values.\n\n# Not sure what this is from.\ntasks.loc[tasks.loot.str.contains(' at')]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      35\n      Provide an Advocate\n      1\n      Advocate at\n      Our field agent requested better gear to take ...\n    \n  \n\n\n\n\nThe Advocate in this row is labeled as Advocate at which is another problem. In this instance, instead of trying to deal with the regex we’re just going to update that single value.\n\ntasks.loc[tasks.loot.str.contains('Advocate at'), 'loot'] = 'Advocate'\ntasks.loc[tasks.loot.str.contains('Advocate')]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      35\n      Provide an Advocate\n      1\n      Advocate\n      Our field agent requested better gear to take ...\n    \n  \n\n\n\n\nAnd, make sure that the append works as intended:\n\npd.concat(\n    [lootKMarks.rename({\"Name\":\"Reward\"}, axis=1)[['Unit', 'Reward', 'Rarity', 'Loot']],\n    drives.query('Reward == \"K-Marks\"').drop(\"Weight\", axis=1),\n    guns[['Unit', 'Reward', 'Rarity', 'Loot']]]\n)\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      1\n      150.0\n      K-Marks\n      Common\n      Flawed Veltecite\n    \n    \n      6\n      570.0\n      K-Marks\n      Uncommon\n      Cloudy Veltecite\n    \n    \n      11\n      854.0\n      K-Marks\n      Rare\n      Clear Veltecite\n    \n    \n      16\n      1922.0\n      K-Marks\n      Epic\n      Pure Veltecite\n    \n    \n      21\n      6487.0\n      K-Marks\n      Legendary\n      Veltecite Heart\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      63\n      371.0\n      K-Marks\n      Common\n      S-576 PDW\n    \n    \n      66\n      1179.0\n      K-Marks\n      Uncommon\n      Scarab\n    \n    \n      69\n      12341.0\n      K-Marks\n      Epic\n      Shattergun\n    \n    \n      72\n      34172.0\n      K-Marks\n      Exotic\n      Voltaic Brute\n    \n    \n      75\n      270540.0\n      K-Marks\n      Legendary\n      Zeus Beam\n    \n  \n\n125 rows × 4 columns"
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-backpacks",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-backpacks",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Deal With Backpacks",
    "text": "Deal With Backpacks\nWe’re going to try to fix the Backpacks, the Shields and the Helmets toegther since they’re all on the same page of the wiki. Again, this data was not part of the previous tables that we had so we’ll need to go get it.\n\ngearUrl = 'https://thecyclefrontier.wiki/wiki/Gear'\nsite = pd.read_html(gearUrl)\n\nThis was tricky to collect - which is only why I’m pointing out how it was done. While working out how to collect the data from the website, there was no good strategy to collect just the data that I wanted. What you will see is that I use hard coded values to pull out where each table is.\n\nsiteBackPacks = site[0]\nsiteHelmet = site[10]\nsiteShield = site[22]\n\nWhat I did was enumerate throught all the tables collected to find which results had outlier sizes. I then pulled those to make sure they were what I was after like this:\n\n# 10\n# 22\nlist(enumerate(map(len, site)))\n\n[(0, 22),\n (1, 1),\n (2, 1),\n (3, 3),\n (4, 1),\n (5, 4),\n (6, 1),\n (7, 4),\n (8, 1),\n (9, 1),\n (10, 46),\n (11, 1),\n (12, 1),\n (13, 3),\n (14, 4),\n (15, 4),\n (16, 3),\n (17, 4),\n (18, 4),\n (19, 3),\n (20, 4),\n (21, 4),\n (22, 43),\n (23, 1),\n (24, 1),\n (25, 3),\n (26, 4),\n (27, 4),\n (28, 4),\n (29, 4),\n (30, 4),\n (31, 4),\n (32, 4)]\n\n\nIt is not pretty but web scraping rarely is. And, it works. Next we’ll start with the backup packs; with some tweaking of the values to the extractSite function which was defined earlier, this becomes really easy.\n\nbackpacks = extractSite(\n    siteData = siteBackPacks.loc[siteBackPacks.Name.str.contains(\"Backpack|K-Marks\")],\n    columns = ['Image', 'Name', 'Rarity', 'Space', 'Sale Price'],\n    adjust = 2, \n    step = 3,\n    offset = [1, 2])\n\nbackpacks\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Space\n      Sale Price\n      Loot\n    \n  \n  \n    \n      1\n      600.0\n      K-Marks\n      Common\n      200.0\n      180  K-Marks\n      Small Backpack\n    \n    \n      2\n      180.0\n      K-Marks\n      Common\n      200.0\n      180  K-Marks\n      Small Backpack\n    \n    \n      4\n      2700.0\n      K-Marks\n      Uncommon\n      250.0\n      810  K-Marks\n      Medium Backpack\n    \n    \n      7\n      810.0\n      K-Marks\n      Uncommon\n      250.0\n      810  K-Marks\n      Medium Backpack\n    \n    \n      9\n      6100.0\n      K-Marks\n      Rare\n      300.0\n      1,830  K-Marks\n      Large Backpack\n    \n    \n      13\n      1830.0\n      K-Marks\n      Rare\n      300.0\n      1,830  K-Marks\n      Large Backpack\n    \n    \n      15\n      12000.0\n      K-Marks\n      Epic\n      350.0\n      4,000  K-Marks\n      Heavy Duty Backpack\n    \n    \n      19\n      4000.0\n      K-Marks\n      Epic\n      350.0\n      4,000  K-Marks\n      Heavy Duty Backpack\n    \n  \n\n\n\n\nLooking at these results though, what we’re really after is the Sale Price and so we’ll need to move some stuff around.\n\nbackpacks = backpacks.assign(\n    Unit = backpacks['Sale Price'].str.replace(' K-Marks', '').str.replace(',', '').astype('float')\n)\n\nbackpacks = backpacks.drop([\"Sale Price\", \"Space\"], axis=1)\nbackpacks\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      1\n      180.0\n      K-Marks\n      Common\n      Small Backpack\n    \n    \n      2\n      180.0\n      K-Marks\n      Common\n      Small Backpack\n    \n    \n      4\n      810.0\n      K-Marks\n      Uncommon\n      Medium Backpack\n    \n    \n      7\n      810.0\n      K-Marks\n      Uncommon\n      Medium Backpack\n    \n    \n      9\n      1830.0\n      K-Marks\n      Rare\n      Large Backpack\n    \n    \n      13\n      1830.0\n      K-Marks\n      Rare\n      Large Backpack\n    \n    \n      15\n      4000.0\n      K-Marks\n      Epic\n      Heavy Duty Backpack\n    \n    \n      19\n      4000.0\n      K-Marks\n      Epic\n      Heavy Duty Backpack\n    \n  \n\n\n\n\nWe have the same problem which we had for the Data Drives: the the old name Rare Backpack is in the tasks table but is not how they are named here. Again, we’re just going to steal and modify the solution we had before and apply it here.\n\nbackpacks['Loot'] = backpacks['Rarity'].astype('str') + ' Backpack'\n\nTime to add them to the pipeline.\n\npd.concat([\n    lootKMarks.rename({\"Name\":\"Reward\"}, axis=1)[['Unit', 'Reward', 'Rarity', 'Loot']],\n    drives.query('Reward == \"K-Marks\"').drop(\"Weight\", axis=1),\n    guns[['Unit', 'Reward', 'Rarity', 'Loot']],\n    backpacks\n])\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      1\n      150.0\n      K-Marks\n      Common\n      Flawed Veltecite\n    \n    \n      6\n      570.0\n      K-Marks\n      Uncommon\n      Cloudy Veltecite\n    \n    \n      11\n      854.0\n      K-Marks\n      Rare\n      Clear Veltecite\n    \n    \n      16\n      1922.0\n      K-Marks\n      Epic\n      Pure Veltecite\n    \n    \n      21\n      6487.0\n      K-Marks\n      Legendary\n      Veltecite Heart\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      7\n      810.0\n      K-Marks\n      Uncommon\n      Uncommon Backpack\n    \n    \n      9\n      1830.0\n      K-Marks\n      Rare\n      Rare Backpack\n    \n    \n      13\n      1830.0\n      K-Marks\n      Rare\n      Rare Backpack\n    \n    \n      15\n      4000.0\n      K-Marks\n      Epic\n      Epic Backpack\n    \n    \n      19\n      4000.0\n      K-Marks\n      Epic\n      Epic Backpack\n    \n  \n\n133 rows × 4 columns\n\n\n\nThis is much better. So, how many are missing now?\n\nlen(results.query(\"Cost == 0\"))\n\n4\n\n\nMuch better. But, there is actually a new problem here: masking missing values. If we look back at our data from before we’ll see that there was Helment and Shield information missing and now it’s gone! Since we’ve fixed some of the values for those jobs the Balance is no longer 0 and thefore we’ve lost track of it! Let’s step back and look at the merged result and look for mistakes. And, upon doing this we run into our first masked problem.\n\ntmp = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='outer')\ntmp\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Loot\n      Unit\n    \n  \n  \n    \n      0\n      New Mining Tools\n      2.0\n      Hydraulic Piston\n      We are producing new Mining Tools for new Pros...\n      Hydraulic Piston\n      338.0\n    \n    \n      1\n      Excavator Improvements\n      3.0\n      Hydraulic Piston\n      The suspension on our mining excavators need i...\n      Hydraulic Piston\n      338.0\n    \n    \n      2\n      New Mining Tools\n      10.0\n      Hardened Metals\n      We are producing new Mining Tools for new Pros...\n      Hardened Metals\n      150.0\n    \n    \n      3\n      Automated Security\n      16.0\n      Hardened Metals\n      We will have to build new turrets to help prot...\n      Hardened Metals\n      150.0\n    \n    \n      4\n      Air Lock Upgrades\n      12.0\n      Hardened Metals\n      Our engineers designed a safer airlock system ...\n      Hardened Metals\n      150.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      211\n      NaN\n      NaN\n      NaN\n      NaN\n      Common Backpack\n      180.0\n    \n    \n      212\n      NaN\n      NaN\n      NaN\n      NaN\n      Uncommon Backpack\n      810.0\n    \n    \n      213\n      NaN\n      NaN\n      NaN\n      NaN\n      Uncommon Backpack\n      810.0\n    \n    \n      214\n      NaN\n      NaN\n      NaN\n      NaN\n      Epic Backpack\n      4000.0\n    \n    \n      215\n      NaN\n      NaN\n      NaN\n      NaN\n      Epic Backpack\n      4000.0\n    \n  \n\n216 rows × 6 columns\n\n\n\nThere are all these NaN values at the bottom which were attached and included due to the outer including all the rows. Can we simply change this over to a left?\n\ntmp = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='left')\ntmp.loc[tmp.Loot.isna()]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Loot\n      Unit\n    \n  \n  \n    \n      39\n      Excavation Gear\n      1\n      Heavy Mining Tool\n      For excavations, we need you to stash a Heavy ...\n      NaN\n      NaN\n    \n    \n      46\n      And two smoking Barrels\n      200\n      Shotgun Ammo\n      Prospector. Get down there and stash a PKR Mae...\n      NaN\n      NaN\n    \n    \n      79\n      Grenadier\n      1\n      Frag Grenade\n      Prospector. You have heard of Badum's Dead Dro...\n      NaN\n      NaN\n    \n    \n      83\n      Ammo Supplies\n      1000\n      Medium Ammo\n      Our Field Agents need more Ammo if they are to...\n      NaN\n      NaN\n    \n    \n      126\n      Loadout Drop\n      1\n      Rare Shield\n      One of our more... lethal assets on Fortuna re...\n      NaN\n      NaN\n    \n    \n      127\n      Loadout Drop\n      1\n      Rare Helmet\n      One of our more... lethal assets on Fortuna re...\n      NaN\n      NaN\n    \n    \n      136\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NaN\n      NaN\n    \n  \n\n\n\n\nThis is more in line what I’d have expected. We’ll temporarily keep this - but mostly for discussion purposes. The problem here is that when you look through the wiki there are no sell values for the Shields and Helmets. We cannot get these from the wiki which means we cannot automate it. We could do this manually but then I’d have to constantly update this value when it changes - which I’m trying to avoid. This looks to be a case where we’re going to need to remove this job for now.\n\n# Adding this to the pipeline:\nidx = tmp.name.str.contains(\"Loadout Drop\")\ntmp = tmp.loc[-idx]\ntmp\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Loot\n      Unit\n    \n  \n  \n    \n      0\n      New Mining Tools\n      2\n      Hydraulic Piston\n      We are producing new Mining Tools for new Pros...\n      Hydraulic Piston\n      338.0\n    \n    \n      1\n      New Mining Tools\n      10\n      Hardened Metals\n      We are producing new Mining Tools for new Pros...\n      Hardened Metals\n      150.0\n    \n    \n      2\n      Explosive Excavation\n      4\n      Derelict Explosives\n      One of our mines collapsed with valuable equip...\n      Derelict Explosives\n      1709.0\n    \n    \n      3\n      Mining Bot\n      2\n      Zero Systems CPU\n      Our engineers have designed an autonomous mini...\n      Zero Systems CPU\n      506.0\n    \n    \n      4\n      Mining Bot\n      3\n      Ball Bearings\n      Our engineers have designed an autonomous mini...\n      Ball Bearings\n      338.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      136\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NaN\n      NaN\n    \n    \n      137\n      Flexible Sealant\n      8\n      Resin Gun\n      A number of our weather balloons took damage f...\n      Resin Gun\n      759.0\n    \n    \n      138\n      Indigenous Fruit\n      4\n      Indigenous Fruit\n      Ah, Prospector, have you come across any Indig...\n      Indigenous Fruit\n      759.0\n    \n    \n      139\n      Indigenous Fruit\n      4\n      Biological Sampler\n      Ah, Prospector, have you come across any Indig...\n      Biological Sampler\n      1139.0\n    \n    \n      140\n      Don't get crushed\n      3\n      Crusher Hide\n      Gear up, Prospector! We need Crusher Skins for...\n      Crusher Hide\n      11533.0\n    \n  \n\n137 rows × 6 columns\n\n\n\nOk, so it looks like next will be solving the ammo listings.\n\ntmp.loc[tmp.Loot.isna()]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Loot\n      Unit\n    \n  \n  \n    \n      39\n      Excavation Gear\n      1\n      Heavy Mining Tool\n      For excavations, we need you to stash a Heavy ...\n      NaN\n      NaN\n    \n    \n      46\n      And two smoking Barrels\n      200\n      Shotgun Ammo\n      Prospector. Get down there and stash a PKR Mae...\n      NaN\n      NaN\n    \n    \n      79\n      Grenadier\n      1\n      Frag Grenade\n      Prospector. You have heard of Badum's Dead Dro...\n      NaN\n      NaN\n    \n    \n      83\n      Ammo Supplies\n      1000\n      Medium Ammo\n      Our Field Agents need more Ammo if they are to...\n      NaN\n      NaN\n    \n    \n      136\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NaN\n      NaN"
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-ammo",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-ammo",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Deal with Ammo",
    "text": "Deal with Ammo\n\nammoUrl = \"https://thecyclefrontier.wiki/wiki/Ammo\"\nammo = pd.read_html(ammoUrl)[0]\n\nWe’ve already done this before so this is simply here to show it was done. And, you add it to the pipeline just the same.\n\nammo = ammo.rename(\n    {\"Item Name\":\"Loot\", \"Sell Value\":\"Unit\"}, axis=1\n    ).assign(\n        Reward = \"K-Marks\",\n        Rarity = pd.Categorical(\n            ammo.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary'])\n    )[['Unit', 'Reward', 'Rarity', 'Loot']]"
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-the-heavy-mining-tool",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#deal-with-the-heavy-mining-tool",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Deal with The Heavy Mining Tool",
    "text": "Deal with The Heavy Mining Tool\nSadly, this tool is on its own page so we’ll need something custom again. There is a table here we can pull but it’s oriented incorrectly for our use.\n\nsite = pd.read_html(\"https://thecyclefrontier.wiki/wiki/Heavy_Mining_Tool\")\nminerData = site[0]\nminerData\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      Description\n      Allows faster mining of materials.\n    \n    \n      1\n      Rarity\n      Common\n    \n    \n      2\n      Weight\n      30\n    \n    \n      3\n      Buy Value\n      600\n    \n    \n      4\n      Sell Value\n      180\n    \n    \n      5\n      Faction Points\n      2\n    \n  \n\n\n\n\nThankfully, a data frame has the Transpose function that a matrix does. A simple explanation of this function is that it swaps the rows to columns and columns to rows.\n\nminerData[0].T\n\n0       Description\n1            Rarity\n2            Weight\n3         Buy Value\n4        Sell Value\n5    Faction Points\nName: 0, dtype: object\n\n\nWe’re going to extract the rows values to create a new dataframe object. Since the Heaving Mining Tool designation is missing from the data then we’ll need to add it ourself.\n\nrow = minerData[1].T.to_list() + ['Heavy Mining Tool']\nrow\n\n['Allows faster mining of materials.',\n 'Common',\n '30',\n '600',\n '180',\n '2',\n 'Heavy Mining Tool']\n\n\n\ncolumns = minerData[0].to_list() + ['Loot']\ncolumns\n\n['Description',\n 'Rarity',\n 'Weight',\n 'Buy Value',\n 'Sell Value',\n 'Faction Points',\n 'Loot']\n\n\nAnd, join them together.\n\nmineTool = pd.DataFrame(columns = columns)\nmineTool.loc[0] = row\nmineTool = mineTool.assign(\n    Reward = \"K-Marks\",\n    Rarity = pd.Categorical(\n        mineTool.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']\n    ),\n    Unit = mineTool['Sell Value'].astype(int),\n)[['Unit', 'Reward', 'Rarity', 'Loot']]\nmineTool\n\n\n\n\n\n  \n    \n      \n      Unit\n      Reward\n      Rarity\n      Loot\n    \n  \n  \n    \n      0\n      180\n      K-Marks\n      Common\n      Heavy Mining Tool\n    \n  \n\n\n\n\nAnd, add it to the pipeline.\n\nlootKMarks = pd.concat([\n    lootKMarks.rename({\"Name\":\"Reward\"}, axis=1)[['Unit', 'Reward', 'Rarity', 'Loot']],\n    drives.query('Reward == \"K-Marks\"').drop(\"Weight\", axis=1),\n    guns[['Unit', 'Reward', 'Rarity', 'Loot']],\n    backpacks,\n    ammo,\n    mineTool\n])\n\n# Drop this quest\nidx = tasks.name.str.contains(\"Loadout Drop\")\ntasks = tasks.loc[-idx]\n\ntaskLoot = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='outer')\ntaskLoot['Cost'] = taskLoot['count'] * taskLoot['Unit']\nresults = allJobs.query('Rewards == \"K-Marks\"').merge(\n    taskLoot[['name', 'Cost']].groupby('name').sum(), left_on=\"Job\", right_on=\"name\", how='left')\nresults['Balance'] = results['Units'] - results['Cost']\n\n\nresults.query(\"Cost == 0\")\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Cost\n      Balance\n    \n  \n  \n    \n      77\n      2400\n      K-Marks\n      Grenadier\n      0.0\n      2400.0\n    \n    \n      138\n      155000\n      K-Marks\n      NEW-Hard-Osiris-EliteCrusher-1\n      0.0\n      155000.0\n    \n  \n\n\n\n\n\ntmp = tasks.merge(lootKMarks[['Loot', 'Unit']], left_on='loot', right_on='Loot', how='left')\ntmp.loc[tmp.Loot.isna()]\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n      Loot\n      Unit\n    \n  \n  \n    \n      93\n      Grenadier\n      1\n      Frag Grenade\n      Prospector. You have heard of Badum's Dead Dro...\n      NaN\n      NaN\n    \n    \n      151\n      NEW-Hard-Osiris-EliteCrusher-1\n      1\n      Alpha Crusher\n      DESCRIPTION MISSING\n      NaN\n      NaN"
  },
  {
    "objectID": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#conclusions",
    "href": "posts/nbks/2022-11-04-cycle-jobs-part-four.html#conclusions",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
    "section": "Conclusions",
    "text": "Conclusions\nThis is the best we’ll get I suppose. Now, back to our check: How bad is our good old job And two smoking Barrels?\n\nresults.loc[results.Job.str.contains(\"Barrel\")]\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Cost\n      Balance\n    \n  \n  \n    \n      39\n      19000\n      K-Marks\n      And two smoking Barrels\n      40716.0\n      -21716.0\n    \n  \n\n\n\n\nAnd, there we go! So, how many jobs have a negative balance?\n\nresults.query(\"Balance < 0 \")\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Cost\n      Balance\n    \n  \n  \n    \n      39\n      19000\n      K-Marks\n      And two smoking Barrels\n      40716.0\n      -21716.0\n    \n    \n      70\n      9500\n      K-Marks\n      Data Drive III\n      10124.0\n      -624.0\n    \n    \n      73\n      56000\n      K-Marks\n      Data Drive VI\n      61512.0\n      -5512.0\n    \n    \n      81\n      17000\n      K-Marks\n      Ammo Supplies\n      69000.0\n      -52000.0\n    \n  \n\n\n\n\nThat’s not as bad as I expected. What is the average job balance?\n\nresults.Balance.mean(), results.Balance.median()\n\n(13300.702127659575, 11256.0)\n\n\nSo, what jobs are above the mean?\n\nresults.query(f\"Balance >= {results.Balance.mean()}\")\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Cost\n      Balance\n    \n  \n  \n    \n      5\n      25000\n      K-Marks\n      Excavator Improvements\n      4306.0\n      20694.0\n    \n    \n      6\n      37000\n      K-Marks\n      A new type of Alloy\n      15468.0\n      21532.0\n    \n    \n      7\n      27000\n      K-Marks\n      Automated Security\n      4930.0\n      22070.0\n    \n    \n      9\n      52000\n      K-Marks\n      Classified I\n      27030.0\n      24970.0\n    \n    \n      15\n      27000\n      K-Marks\n      Geologist\n      6727.0\n      20273.0\n    \n    \n      16\n      29000\n      K-Marks\n      Industry Secret\n      8649.0\n      20351.0\n    \n    \n      17\n      34000\n      K-Marks\n      Veltecite Hearts\n      12974.0\n      21026.0\n    \n    \n      18\n      40000\n      K-Marks\n      Laser Rifles\n      18259.0\n      21741.0\n    \n    \n      19\n      47000\n      K-Marks\n      Classified II\n      23922.0\n      23078.0\n    \n    \n      20\n      41000\n      K-Marks\n      Unlimited Power\n      17377.0\n      23623.0\n    \n    \n      32\n      27000\n      K-Marks\n      Meteor Core\n      7594.0\n      19406.0\n    \n    \n      33\n      30000\n      K-Marks\n      Shards of pure Power\n      8104.0\n      21896.0\n    \n    \n      34\n      48000\n      K-Marks\n      Battery Day\n      22782.0\n      25218.0\n    \n    \n      41\n      28000\n      K-Marks\n      Upgrades\n      3845.0\n      24155.0\n    \n    \n      43\n      140000\n      K-Marks\n      Exclusive Drilling Rights\n      115330.0\n      24670.0\n    \n    \n      44\n      32000\n      K-Marks\n      Stocking Up\n      9120.0\n      22880.0\n    \n    \n      45\n      26000\n      K-Marks\n      Shock and Awe\n      11385.0\n      14615.0\n    \n    \n      50\n      37000\n      K-Marks\n      Sabotage\n      15696.0\n      21304.0\n    \n    \n      51\n      28000\n      K-Marks\n      Old Currency\n      6000.0\n      22000.0\n    \n    \n      52\n      26000\n      K-Marks\n      Recoil Compensation\n      9390.0\n      16610.0\n    \n    \n      54\n      45000\n      K-Marks\n      New 3D Printer\n      21362.0\n      23638.0\n    \n    \n      55\n      33000\n      K-Marks\n      Arms Race\n      11532.0\n      21468.0\n    \n    \n      56\n      35000\n      K-Marks\n      Energy Crisis\n      18984.0\n      16016.0\n    \n    \n      71\n      26000\n      K-Marks\n      Data Drive IV\n      12150.0\n      13850.0\n    \n    \n      76\n      33000\n      K-Marks\n      A Solution\n      10125.0\n      22875.0\n    \n    \n      82\n      25000\n      K-Marks\n      Data Drop\n      10124.0\n      14876.0\n    \n    \n      83\n      26000\n      K-Marks\n      Field Supplies\n      3000.0\n      23000.0\n    \n    \n      84\n      25000\n      K-Marks\n      Spare Parts\n      2024.0\n      22976.0\n    \n    \n      90\n      29000\n      K-Marks\n      NEW-Hard-ICA-Gather-6\n      9142.0\n      19858.0\n    \n    \n      91\n      227000\n      K-Marks\n      Striking Big\n      201830.0\n      25170.0\n    \n    \n      93\n      25000\n      K-Marks\n      A Craving\n      4048.0\n      20952.0\n    \n    \n      97\n      26000\n      K-Marks\n      Cretins\n      6300.0\n      19700.0\n    \n    \n      98\n      27000\n      K-Marks\n      Sensor Array Repairs\n      6908.0\n      20092.0\n    \n    \n      99\n      32000\n      K-Marks\n      A new Energy Source\n      11289.0\n      20711.0\n    \n    \n      100\n      30000\n      K-Marks\n      Data Center Upgrades\n      7690.0\n      22310.0\n    \n    \n      101\n      30000\n      K-Marks\n      Keep the Experiment running\n      7200.0\n      22800.0\n    \n    \n      102\n      61000\n      K-Marks\n      Safety Measures\n      36309.0\n      24691.0\n    \n    \n      122\n      31000\n      K-Marks\n      Harvest Day\n      6583.0\n      24417.0\n    \n    \n      125\n      37000\n      K-Marks\n      Core Values\n      15188.0\n      21812.0\n    \n    \n      126\n      45000\n      K-Marks\n      Promising Results\n      21266.0\n      23734.0\n    \n    \n      130\n      24000\n      K-Marks\n      Stormy Samples\n      2700.0\n      21300.0\n    \n    \n      131\n      25000\n      K-Marks\n      Stormy Samples II\n      2024.0\n      22976.0\n    \n    \n      132\n      33000\n      K-Marks\n      Loadout Drop\n      7320.0\n      25680.0\n    \n    \n      136\n      33000\n      K-Marks\n      NEW-Hard-Osiris-Gather-7\n      12816.0\n      20184.0\n    \n    \n      138\n      155000\n      K-Marks\n      NEW-Hard-Osiris-EliteCrusher-1\n      0.0\n      155000.0\n    \n  \n\n\n\n\nPerfect! Next we’ll loop back around to automating the pipeline and uploading the data to Kaggle."
  },
  {
    "objectID": "posts/nbks/2022-11-02-simple-hightlight-over-exiting-plot.html",
    "href": "posts/nbks/2022-11-02-simple-hightlight-over-exiting-plot.html",
    "title": "Simple Function for Highlight Box Effect",
    "section": "",
    "text": "While browsing around for inspiration, I found this wonderful post by someone named SonyMonthonaRK on Medium; the post is here since it contains more than what I’ll be talking about. The graph is simple aside from the red highlighted box around relevant information: seen below: \nI’ve seen this effect before but never actually seen the code for it. Thankfully, he provided the a link to it on their Github page since I wanted to learn this trick. The Good news is that the effect is not complicated; the bad news is that it’s implementation here is conditional and ugly. We’re going to copy it here just for reference:\nplt.figure(figsize=(15,10)) # mengatur ukuran figure\nsns.lineplot(x='month', y='average_num_booking', hue='hotel_type', color= 'gray',\n             size=\"hotel_type\", sizes=(2.5, 2.5), data=df1_gr) # plot awal menggunakan lineplot dari library seaborn\n\nplt.tick_params(axis='both', which='major', labelsize=14) # memperbesar ukuran x-y axis label\nplt.grid() # menambahkan gridline\nplt.legend(title='Hotel Type', title_fontsize=15, prop={'size':13}) # mengatur judul dan ukuran font pada legenda\n\nplt.xlabel('Arrival Month', fontsize=15) # mengatur title pada x-axis \nplt.ylabel('Average Number of Booking', fontsize=15) # mengatur title pada y-axis\nplt.ylim(0, 4800) # membatasi y axis\n\nplt.axvline(4, ls='--', color='red') # membuat garis vertikal untuk menghighlight insight\nplt.axvline(6, ls='--', color='red') # membuat garis vertikal untuk menghighlight insight\nplt.text(x=4.5, y=4400, s='Holiday \\nSeason', fontsize=16, color='red') # menambahkan teks keterangan\nplt.stackplot(np.arange(4,7,1), [[4800]], color='grey', alpha=0.3) # memberikan blok warna pada area yang dihighlight 2 garis vertikal\n\nplt.axvline(9, ls='--', color='red') # membuat garis vertikal untuk menghighlight insight\nplt.axvline(11, ls='--', color='magenta') # membuat garis vertikal untuk menghighlight insight\nplt.text(x=9.5, y=4400, s='Holiday \\nSeason', fontsize=16, color='red') # menambahkan teks keterangan\nplt.stackplot(np.arange(9,12,1), [[4800]], color='grey', alpha=0.3) # memberikan blok warna pada\n\nplt.text(x=-0.5, y=5200, s=\"Both Hotels Have More Guests During Holiday Season\", \n         fontsize=20, fontweight='bold') # memberikan judul yang informatif\nplt.text(x=-0.5, y=4850, s=\"City Hotel has the decreased guests in August and September, while both hotels have less \\ncustomer during not holiday season (Jan-Mar)\", \n         fontsize=18) # memberikan keterangan tambahan atas judul\n\nplt.tight_layout() # mengatur layout dari visualisasi agar tidak terpotong\nThis is a lot of - specialized and inflexible - code to add an affect to a graph. If I wanted to add this to my own graphs then I would have to do quite a bit of work and math and customization. I want this on any graph at any point without doing all this work. So, let us take it apart and make a function we can all use. We’re not going to use their data since we want to apply it to any random set of data so we’re using random data.\n\n# our pretend dataset\ndata = np.random.randn(50)\ndata = pd.DataFrame(data, columns=['Random'])\n\nMuch of that is boilerplate and repetition so the summary of a single application would be:\n# Initialize the lineplot\n\n# Add the left dotted line\n\n# Add the right dotted line.\n\n# Add the notated text\n\n# Add a stackplot to fill the space between the lines\nWe’ll cover the plot first since it’s the easiest and can go outside of the function we’re trying to build. We’ll use the seaborn lineplot the same. We’ll need use an index similar to the example post while we build this - so a simple range:\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nWe’ll want to save the graph reference as we’ll need it for later.\nLooking at the docs for the horizintal line function, we need: 1. The x tick value. 2. The line display type. 3. The color\nWe’re going to also modify the line width in our example and fix it; you are welcome to modify this function to accept a thicker value but I like it at 1 and have no current reason to change it. We will need the left and right line points passed to the function as a starting point; we’ll hold the other values constant for now.\n\ndef building(p = None, xCoord = None):\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')    \n\n\nplt.figure(figsize=(12,5))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5))\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nGood start. The next problem will be adding the text inside the box and how to position the text automatically. We could brute force - and in some instances we might have no other choice - but that is not convenient while we’re automating the function. How I solved this is to think about what we’re doing in mathematical terms. 1. Find the center of the x values range. 2. Translate the text to the left based off the figure size and the length of the text.\nSo, first we find the middle point by taking the difference of both sides, dividing it by two and then adding that to the left side.\n\ndef building(p = None, xCoord = None, text=None, xText=None):\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n\n    # Ignore this for now; we simply need it\n    y_min, y_max = p.get_ylim()\n    yPos = y_max*.8\n\n    xText = xCoord[0] + (xCoord[1] - xCoord[0])/2\n    plt.text(x=xText, y=yPos, s=text, fontsize=12, color='red')    \n\n\nplt.figure(figsize=(12,5))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5), \"Solid\")\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nNext, we’ll need to get the figure size as well as the length of the text. Thankfully, matplotlib allows you to get the dimensions of the figure using .figure.get_size_inches().\n\ndef building(p = None, xCoord = None, text=None, xText=None):\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n\n    # Ignore this for now; we simply need it\n    y_min, y_max = p.get_ylim()\n    yPos = y_max*.8\n\n    width, _ = p.figure.get_size_inches()\n    lenText = len(text)\n    \n    xText = xCoord[0] + (xCoord[1] - xCoord[0])/2\n    plt.text(x=xText, y=yPos, s=text, fontsize=12, color='red')    \n\n\nplt.figure(figsize=(12,5))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5), \"Solid\")\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nNow that we have these values, we can think about how to move them. The answer is this formula: lenText * 3/width.\n\ndef building(p = None, xCoord = None, text=None, xText=None):\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n\n    # Ignore this for now; we simply need it\n    y_min, y_max = p.get_ylim()\n    yPos = y_max*.8\n\n    width, _ = p.figure.get_size_inches()\n    lenText = len(text)\n\n    xText = xCoord[0]  + (xCoord[1] - xCoord[0])/2 - 3 * lenText/width\n    plt.text(x=xText, y=yPos, s=text, fontsize=12, color='red')    \n\n\nplt.figure(figsize=(10,5))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5), \"Solid\")\n\n# Showing you can simply run this more than once\n# to add another box highlight\nbuilding(p, (20,40), \"Something Else\")\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nSo, why does this work? Frankily, I’m not sure quite sure where the three comes from here. Normalizing the number of characters by the figures width make sense but that alone is not enough; you can see this in the below example.\n\ndef building(p = None, xCoord = None, text=None, xText=None):\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n\n    # Ignore this for now; we simply need it\n    y_min, y_max = p.get_ylim()\n    yPos = y_max*.8\n\n    width, _ = p.figure.get_size_inches()\n    lenText = len(text)\n\n    xText = xCoord[0]  + (xCoord[1] - xCoord[0])/2 - lenText/width\n    plt.text(x=xText, y=yPos, s=text, fontsize=12, color='red')    \n\n\nplt.figure(figsize=(10,5))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5), \"Solid\")\n\n# Showing you can simply run this more than once\n# to add another box highlight\nbuilding(p, (20,40), \"Something Else\")\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nAnyways, we’ll leave what works for now and move to the last problem which is much easier to solve: the stackplot. If you’re not familiar with these graphs then it’s more than likely you’re unfamiliar with the name and not the plot itself. These are used to show percentage change of different categories - usually over time. A good example would be the follow image from the website Geeksforgeeks: \nExcept what we want is one box full of a single color: grey. Maybe there is a better way to do this but we’re going to base our work off the post and it will work anyways. For a stackplot we need: 1. The coordinates to fill. 2. The values.\nIn the example, the height is filled with a known value: 4800. But this value is the distance from the x-axis and we cannot use this in our example - nor will this generalize. Since their data does not have negative values this works fine but sometimes we will have negative values. What we will need to do is find a fill value. We could use the figure height but there is a case where the total distance can be greater than the figure height strangely enough. What we’ll do is take the sum of the absolute values of the y_min and y_max values + 1. Then, if that values is larger than the height we’ll use that instead.\n\ndef building(p = None, xCoord = None, text=None, xText=None):\n\n    # Ignore this for now; we simply need it\n    y_min, y_max = p.get_ylim()\n    p.set_ylim(y_min, y_max)\n\n    yPos = y_max*.8\n\n    # Collecting height here:\n    width, height = p.figure.get_size_inches()\n    totalDist = abs(y_min) + abs(y_max) + 1\n    stackFill = height\n\n    if totalDist > height:\n        stackFill = totalDist\n\n    lenText = len(text)\n    xText = xCoord[0]  + (xCoord[1] - xCoord[0])/2 - 3 * lenText/width\n\n    # Add the lines:\n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n\n    plt.text(x=xText, y=yPos, s=text, fontsize=12, color='red')   \n    plt.stackplot(np.arange(xCoord[0],xCoord[1]+1,1), [[stackFill]], baseline = \"sym\", color='grey', alpha=0.3) \n\n\nplt.figure(figsize=(12,6))\np = sns.lineplot(x=range(0, len(data)), y=\"Random\", data=data)\nplt.grid()\nbuilding(p, (1,5), \"Solid\")\n\n# Showing you can simply run this more than once\n# to add another box highlight\n#\nbuilding(p, (20,40), \"Something Else\")\nplt.title(\"Working Through Example\")\n\nText(0.5, 1.0, 'Working Through Example')\n\n\n\n\n\nYou may be asking what the baseline=\"sym\" argument is since it’s not very descriptive. All that does is changes the relationship from 0 start to Symmetric around the x-axis which we needed for negative values. And, speaking let us test that we can filter for positive values and this all still works.\n\ntmp = data.query(\"Random >= 0\")\n\nplt.figure(figsize=(15,5))\np = sns.lineplot(x=range(0, len(tmp)), y=\"Random\", data=tmp)\nplt.grid()\nbuilding(p, (5, 10), \"Turtles\")\nbuilding(p, (21, 25), \"Doves\")\n\n\n\n\nExcellent. The Doves to my eyes looks a little off center but that should be good enough. Just add a little bit of error handling and we’ve got a generalized highlight box function!\n\ndef addDotBox(p = None, xCoord = None, text=None, xText=None, yText=None, fSize = 12):\n    if not p:\n        raise ValueError(\"Plot cannot be None Type.\")\n    if not text:\n        raise ValueError(\"text cannot be left blank.\")\n    \n    # Pull/reset the limits:\n    y_min, y_max = p.get_ylim()\n    p.set_ylim(y_min, y_max)\n\n    # get \n    width, height = p.figure.get_size_inches()\n    totalDist = abs(y_min) + abs(y_max) + 1\n    stackFill = height\n\n    if totalDist > height:\n        stackFill = totalDist\n    lenText = len(text)\n\n    if not xCoord:\n        raise ValueError(\"Missing xCoord argument.\")\n    if len(xCoord) != 2:\n        raise ValueError(\"There must be two values for xCoord\")\n    if not xText:\n        xText = xCoord[0]  + (xCoord[1] - xCoord[0])/2 - 3 * lenText/width\n    if not yText:\n        yText = y_max * .8\n    \n    plt.axvline(xCoord[0], ls='--', lw=1, color='red')\n    plt.axvline(xCoord[1], ls='--', lw=1, color='red')\n    plt.text(x=xText, y=y_max*.8 , s=text, fontsize=fSize, color='red')\n    plt.stackplot(np.arange(xCoord[0],xCoord[1]+1,1), [[stackFill]], baseline = \"sym\", color='grey', alpha=0.3)"
  },
  {
    "objectID": "posts/nbks/2022-10-07-cycle-jobs-part-two.html",
    "href": "posts/nbks/2022-10-07-cycle-jobs-part-two.html",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Two.",
    "section": "",
    "text": "This is Part Two of a Series towards scraping, cleaning and analyzing the Jobs for The Cycle: Frontier. If you haven’t read Part One then I’d suggest start there. We’re picking up now with cleaning the tasks to complete for the job instead. So, let’s get started! We’ll pull our normal libraries for working on projects like this.\n\nimport pandas as pd             # for the data.\nimport numpy as np              # for a NaN type\nimport matplotlib.pyplot as plt # For plotting, and some customization of plots.\nimport seaborn as sns           # For pretty plots.\n\n# Fix the size of the graphs\nsns.set(rc={\"figure.figsize\":(11, 8)})\n\nWe’re actually going to be using the same data table as before from the Official Wiki and the Jobs Page. Like before, wer’e going to use the same read_html() call targetting the name class.\n\nurl = \"https://thecyclefrontier.wiki/wiki/Jobs\"\nsite = pd.read_html(url, match=\"Name\",\n    converters = {\n        \"Name\": str,\n        \"Description\": str, \n        \"Unlocked\": int, \n        \"Tasks\": str,\n        \"Rewards\": str})\n\nAnd here is the data in the weird rows and columns like before:\n\n# Weird Problem: Data Looks funny, can still use this:\nsite[0].head(8)\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Unlock Level\n      Difficulty\n      Tasks\n      Rewards\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      4.0\n      Easy\n      Collect: 2 Hydraulic Piston 10 Hardened Metals\n      3800  K-Marks  1  Korolev Scrip  15  Korolev R...\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      7.0\n      Medium\n      Collect: 4 Derelict Explosives\n      11000  K-Marks  8  Korolev Scrip  52  Korolev ...\n    \n    \n      5\n      11000\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6\n      8\n      Korolev Scrip\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7\n      52\n      Korolev Reputation\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nWe’re going to make a copy of the specific columns we want to avoid any strange insert/update issues.\n\ntasksSubset = site[0][[\"Name\", \"Description\", \"Tasks\"]].copy()\ntasksSubset\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Tasks\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      Collect: 2 Hydraulic Piston 10 Hardened Metals\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      Collect: 4 Derelict Explosives\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      183\n      470\n      Korolev Reputation\n      NaN\n    \n    \n      184\n      No Expiry Date\n      There you are, finally! There's been an accide...\n      Collect: 10 Old Medicine\n    \n    \n      185\n      6300\n      K-Marks\n      NaN\n    \n    \n      186\n      9\n      Korolev Scrip\n      NaN\n    \n    \n      187\n      62\n      Korolev Reputation\n      NaN\n    \n  \n\n188 rows × 3 columns\n\n\n\n\n# get rid of the middle stuff we don't need.\ntasksSubset = tasksSubset[ ~tasksSubset.Tasks.isna()]\ntasksSubset.head(15)\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Tasks\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      Collect: 2 Hydraulic Piston 10 Hardened Metals\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      Collect: 4 Derelict Explosives\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n      Collect: 2 Zero Systems CPU 3 Ball Bearings\n    \n    \n      12\n      None of your Business\n      Prospector. We need Toxic Glands. Don't ask qu...\n      Collect: 2 Toxic Glands\n    \n    \n      16\n      Insufficient Processing Power\n      Prospector! The Zero Systems CPU you brought u...\n      Collect: 1 Master Unit CPU\n    \n    \n      20\n      Excavator Improvements\n      The suspension on our mining excavators need i...\n      Collect: 2 Co-TEC MultiTool 3 Ball Bearings 3 ...\n    \n    \n      24\n      A new type of Alloy\n      Our scientists are confident they can create a...\n      Collect: 4 Hardened Bone Plates 12 Compound Sh...\n    \n    \n      28\n      Automated Security\n      We will have to build new turrets to help prot...\n      Collect: 5 Zero Systems CPU 16 Hardened Metals\n    \n    \n      32\n      Energy Crisis\n      Veltecite supplies are low, but we need energy...\n      Collect: 4 Miniature Reactor\n    \n    \n      36\n      Classified I\n      Prospector! We need Derelict Explosives, Maste...\n      Collect: 10 Derelict Explosives 2 Master Unit ...\n    \n    \n      40\n      Clear Veltecite\n      The Veltecite you brought us the other day is ...\n      Collect: 2 Clear Veltecite\n    \n    \n      44\n      Time to Focus\n      One of our miners searching the Jungle for Foc...\n      Kill 6 Creatures at Jungle Collect: 4 Focus Cr...\n    \n    \n      48\n      Pure Veltecite\n      The Clear Veltecite was an improvement, we gai...\n      Collect: 2 Pure Veltecite\n    \n    \n      52\n      Titans of Industry\n      Scouts have found Titan Ore deposits on Fortun...\n      Collect: 2 Titan Ore 6 Altered Nickel\n    \n    \n      56\n      Crystal Frenzy\n      We're working on a new type of laser for our l...\n      Collect: 2 Clear Veltecite 8 Focus Crystal\n    \n  \n\n\n\n\nAs I discussed in the previous post, there are three kinds of jobs: Collect, Deposit and Kill. The Collect and Deposit jobs are fine since they involved something easily quantifiable: loot. However, the Kill quests present a very real problem since there is no simple way to address quanitfying them. For killing creatures, maybe we could take the sum of their expected drops and their rate of drop and include that as part of the rewards? Of course, the player may simply choose not to pick any of that up.\nAnother problem is killing players; how much is a player kill actually worth? And, the difficulty of killing players is connected to the skill level of each player - which we also cannot know. Therefore, I’ve elected to remove the Kill Jobs from the analysis.\n\n# anything with kill just remove until I can think of a better way to deal with this.\ntasksSubset = tasksSubset[ ~tasksSubset.Tasks.str.contains(\"Kill\")]\ntasksSubset.head(15)\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Tasks\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      Collect: 2 Hydraulic Piston 10 Hardened Metals\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      Collect: 4 Derelict Explosives\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n      Collect: 2 Zero Systems CPU 3 Ball Bearings\n    \n    \n      12\n      None of your Business\n      Prospector. We need Toxic Glands. Don't ask qu...\n      Collect: 2 Toxic Glands\n    \n    \n      16\n      Insufficient Processing Power\n      Prospector! The Zero Systems CPU you brought u...\n      Collect: 1 Master Unit CPU\n    \n    \n      20\n      Excavator Improvements\n      The suspension on our mining excavators need i...\n      Collect: 2 Co-TEC MultiTool 3 Ball Bearings 3 ...\n    \n    \n      24\n      A new type of Alloy\n      Our scientists are confident they can create a...\n      Collect: 4 Hardened Bone Plates 12 Compound Sh...\n    \n    \n      28\n      Automated Security\n      We will have to build new turrets to help prot...\n      Collect: 5 Zero Systems CPU 16 Hardened Metals\n    \n    \n      32\n      Energy Crisis\n      Veltecite supplies are low, but we need energy...\n      Collect: 4 Miniature Reactor\n    \n    \n      36\n      Classified I\n      Prospector! We need Derelict Explosives, Maste...\n      Collect: 10 Derelict Explosives 2 Master Unit ...\n    \n    \n      40\n      Clear Veltecite\n      The Veltecite you brought us the other day is ...\n      Collect: 2 Clear Veltecite\n    \n    \n      48\n      Pure Veltecite\n      The Clear Veltecite was an improvement, we gai...\n      Collect: 2 Pure Veltecite\n    \n    \n      52\n      Titans of Industry\n      Scouts have found Titan Ore deposits on Fortun...\n      Collect: 2 Titan Ore 6 Altered Nickel\n    \n    \n      56\n      Crystal Frenzy\n      We're working on a new type of laser for our l...\n      Collect: 2 Clear Veltecite 8 Focus Crystal\n    \n    \n      60\n      Geologist\n      You got time for a job, Prospector? The sample...\n      Collect: 2 Pure Veltecite 1 Pure Focus Crystal\n    \n  \n\n\n\n\nSo, each Job can request you collect multiple loots as well as more than one type of loot. Here is a good example of what I mean by this:\n\ntasksSubset.loc[28].Tasks \n\n'Collect: 5 Zero Systems CPU 16 Hardened Metals'\n\n\nAs you can see, this task requires you to collect both Zero System CPUs as well as Hardened Metals - and a good number of them. What we want is to not only extract each type of loot independently of each other but also to keep the count paired with the loot type.\nI would like to take this moment to thank the developers of Pandas. I spent a bit of time thinking about how I would solve this and they had already included a solution to this problem: extractall(). What this does is allows you to pass Regular Expressions and it will then pull out anything in the string which matches. It even puts them into their own separate rows! Again, thank you!\nFor Regular Expressions, this is something you will have to learn on your own. I used a website to test and build mine from an example row; there is plenty of documentaiton about how to use these.\n\nregex = r\"(\\d+\\s[\\w]+\\s[\\w]+)\"\ntmp = tasksSubset.Tasks.str.extractall(regex)\ntmp.head(15)\n\n\n\n\n\n  \n    \n      \n      \n      0\n    \n    \n      \n      match\n      \n    \n  \n  \n    \n      0\n      0\n      2 Hydraulic Piston\n    \n    \n      1\n      10 Hardened Metals\n    \n    \n      4\n      0\n      4 Derelict Explosives\n    \n    \n      8\n      0\n      2 Zero Systems\n    \n    \n      1\n      3 Ball Bearings\n    \n    \n      12\n      0\n      2 Toxic Glands\n    \n    \n      16\n      0\n      1 Master Unit\n    \n    \n      20\n      0\n      3 Ball Bearings\n    \n    \n      1\n      3 Hydraulic Piston\n    \n    \n      24\n      0\n      4 Hardened Bone\n    \n    \n      1\n      12 Compound Sheets\n    \n    \n      28\n      0\n      5 Zero Systems\n    \n    \n      1\n      16 Hardened Metals\n    \n    \n      32\n      0\n      4 Miniature Reactor\n    \n    \n      36\n      0\n      10 Derelict Explosives\n    \n  \n\n\n\n\nPerfect! Now we have all the different loot and we got to keep the row’s index for later when we’ll attach the job name and description. Before that though, we’ll need to do some work to separate the count and the loot type into their own columns. While I’m sure there is a better way to do this, I could not think of one so we’re going to write a function to break the loot and count apart and then return them.\nThere is a solid function for this already called .split() and we’re going to use it to split on spaces but we since some of the loot is multiple words we need to force it to only split once.\n\nexample = tmp.reset_index()[0][1]\nparts = example.split(' ', maxsplit=1)\nnumber, loot = int(parts[0]), parts[1]\nnumber, loot\n\n(10, 'Hardened Metals')\n\n\nNow we’ll create the function. What we can do here though is return either the count value or the loot value depending on an index passed: 0 for count and 1 for loot.\n\ndef breakLoot(taskString, index=0):\n    parts = taskString.split(' ', maxsplit=1)\n    if index == 0:\n        return int(parts[index])\n    elif index == 1:\n        return parts[index]\n    else:\n        # This shouldn't be called.\n        return None\n\nNow we just run two .apply() calls to get the values out:\n\ncount = tmp.reset_index()[0].apply(breakLoot).values\naLoot = tmp.reset_index()[0].apply(breakLoot, index=1).values\n\nAnd, then assign them to our brand new columns for them.\n\ntmp = tmp.assign(\n    count = count,\n    loot = aLoot\n)\n\ntmp.head(15)\n\n\n\n\n\n  \n    \n      \n      \n      0\n      count\n      loot\n    \n    \n      \n      match\n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      2 Hydraulic Piston\n      2\n      Hydraulic Piston\n    \n    \n      1\n      10 Hardened Metals\n      10\n      Hardened Metals\n    \n    \n      4\n      0\n      4 Derelict Explosives\n      4\n      Derelict Explosives\n    \n    \n      8\n      0\n      2 Zero Systems\n      2\n      Zero Systems\n    \n    \n      1\n      3 Ball Bearings\n      3\n      Ball Bearings\n    \n    \n      12\n      0\n      2 Toxic Glands\n      2\n      Toxic Glands\n    \n    \n      16\n      0\n      1 Master Unit\n      1\n      Master Unit\n    \n    \n      20\n      0\n      3 Ball Bearings\n      3\n      Ball Bearings\n    \n    \n      1\n      3 Hydraulic Piston\n      3\n      Hydraulic Piston\n    \n    \n      24\n      0\n      4 Hardened Bone\n      4\n      Hardened Bone\n    \n    \n      1\n      12 Compound Sheets\n      12\n      Compound Sheets\n    \n    \n      28\n      0\n      5 Zero Systems\n      5\n      Zero Systems\n    \n    \n      1\n      16 Hardened Metals\n      16\n      Hardened Metals\n    \n    \n      32\n      0\n      4 Miniature Reactor\n      4\n      Miniature Reactor\n    \n    \n      36\n      0\n      10 Derelict Explosives\n      10\n      Derelict Explosives\n    \n  \n\n\n\n\nAnd, there we go! We have our columns how we want them. Now we just need to work on getting the Name and Description values attached to our new data frame. One way to do this would be to do some sort of merge or join based on the index we’ve saved. Or, we can do something even easier!\nIf we look at the rows when we do an index reset:\n\ntmp.reset_index().head(8)\n\n\n\n\n\n  \n    \n      \n      level_0\n      match\n      0\n      count\n      loot\n    \n  \n  \n    \n      0\n      0\n      0\n      2 Hydraulic Piston\n      2\n      Hydraulic Piston\n    \n    \n      1\n      0\n      1\n      10 Hardened Metals\n      10\n      Hardened Metals\n    \n    \n      2\n      4\n      0\n      4 Derelict Explosives\n      4\n      Derelict Explosives\n    \n    \n      3\n      8\n      0\n      2 Zero Systems\n      2\n      Zero Systems\n    \n    \n      4\n      8\n      1\n      3 Ball Bearings\n      3\n      Ball Bearings\n    \n    \n      5\n      12\n      0\n      2 Toxic Glands\n      2\n      Toxic Glands\n    \n    \n      6\n      16\n      0\n      1 Master Unit\n      1\n      Master Unit\n    \n    \n      7\n      20\n      0\n      3 Ball Bearings\n      3\n      Ball Bearings\n    \n  \n\n\n\n\n… we can see that the column level_0 actaully contains duplicate indexes from our matches. So, the values Hydraulic Piston and Hardened Metals both are associated with Task with index 0. As long as we can use that index to get duplicate values then we can just pull all the Name and Descriptions in their matching order.\n\ntasksSubset.loc[tmp.reset_index().loc[:5, \"level_0\"], ['Name', 'Description']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n    \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n    \n    \n      12\n      None of your Business\n      Prospector. We need Toxic Glands. Don't ask qu...\n    \n  \n\n\n\n\n… which is exactly what we get! Duplicates! Time to slice it out and then assign the values.\n\nnameDescriptSlice = tasksSubset.loc[tmp.reset_index()[\"level_0\"], ['Name', 'Description']]\n\ntmp = tmp.assign(\n    name = nameDescriptSlice.Name.values,\n    description = nameDescriptSlice.Description.values\n)\n\ntmp.head(15)\n\n\n\n\n\n  \n    \n      \n      \n      0\n      count\n      loot\n      name\n      description\n    \n    \n      \n      match\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      2 Hydraulic Piston\n      2\n      Hydraulic Piston\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n    \n    \n      1\n      10 Hardened Metals\n      10\n      Hardened Metals\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n    \n    \n      4\n      0\n      4 Derelict Explosives\n      4\n      Derelict Explosives\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n    \n    \n      8\n      0\n      2 Zero Systems\n      2\n      Zero Systems\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n    \n    \n      1\n      3 Ball Bearings\n      3\n      Ball Bearings\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n    \n    \n      12\n      0\n      2 Toxic Glands\n      2\n      Toxic Glands\n      None of your Business\n      Prospector. We need Toxic Glands. Don't ask qu...\n    \n    \n      16\n      0\n      1 Master Unit\n      1\n      Master Unit\n      Insufficient Processing Power\n      Prospector! The Zero Systems CPU you brought u...\n    \n    \n      20\n      0\n      3 Ball Bearings\n      3\n      Ball Bearings\n      Excavator Improvements\n      The suspension on our mining excavators need i...\n    \n    \n      1\n      3 Hydraulic Piston\n      3\n      Hydraulic Piston\n      Excavator Improvements\n      The suspension on our mining excavators need i...\n    \n    \n      24\n      0\n      4 Hardened Bone\n      4\n      Hardened Bone\n      A new type of Alloy\n      Our scientists are confident they can create a...\n    \n    \n      1\n      12 Compound Sheets\n      12\n      Compound Sheets\n      A new type of Alloy\n      Our scientists are confident they can create a...\n    \n    \n      28\n      0\n      5 Zero Systems\n      5\n      Zero Systems\n      Automated Security\n      We will have to build new turrets to help prot...\n    \n    \n      1\n      16 Hardened Metals\n      16\n      Hardened Metals\n      Automated Security\n      We will have to build new turrets to help prot...\n    \n    \n      32\n      0\n      4 Miniature Reactor\n      4\n      Miniature Reactor\n      Energy Crisis\n      Veltecite supplies are low, but we need energy...\n    \n    \n      36\n      0\n      10 Derelict Explosives\n      10\n      Derelict Explosives\n      Classified I\n      Prospector! We need Derelict Explosives, Maste...\n    \n  \n\n\n\n\nFinally, we’ll drop all those extra columns we don’t need.\n\ntasks = tmp.reset_index().drop([\n    'level_0',\n    'match',\n    0\n], axis =1 )\n\ntasks = tasks[['name', 'count', 'loot', 'description']]\ntasks.head(15)\n\n\n\n\n\n  \n    \n      \n      name\n      count\n      loot\n      description\n    \n  \n  \n    \n      0\n      New Mining Tools\n      2\n      Hydraulic Piston\n      We are producing new Mining Tools for new Pros...\n    \n    \n      1\n      New Mining Tools\n      10\n      Hardened Metals\n      We are producing new Mining Tools for new Pros...\n    \n    \n      2\n      Explosive Excavation\n      4\n      Derelict Explosives\n      One of our mines collapsed with valuable equip...\n    \n    \n      3\n      Mining Bot\n      2\n      Zero Systems\n      Our engineers have designed an autonomous mini...\n    \n    \n      4\n      Mining Bot\n      3\n      Ball Bearings\n      Our engineers have designed an autonomous mini...\n    \n    \n      5\n      None of your Business\n      2\n      Toxic Glands\n      Prospector. We need Toxic Glands. Don't ask qu...\n    \n    \n      6\n      Insufficient Processing Power\n      1\n      Master Unit\n      Prospector! The Zero Systems CPU you brought u...\n    \n    \n      7\n      Excavator Improvements\n      3\n      Ball Bearings\n      The suspension on our mining excavators need i...\n    \n    \n      8\n      Excavator Improvements\n      3\n      Hydraulic Piston\n      The suspension on our mining excavators need i...\n    \n    \n      9\n      A new type of Alloy\n      4\n      Hardened Bone\n      Our scientists are confident they can create a...\n    \n    \n      10\n      A new type of Alloy\n      12\n      Compound Sheets\n      Our scientists are confident they can create a...\n    \n    \n      11\n      Automated Security\n      5\n      Zero Systems\n      We will have to build new turrets to help prot...\n    \n    \n      12\n      Automated Security\n      16\n      Hardened Metals\n      We will have to build new turrets to help prot...\n    \n    \n      13\n      Energy Crisis\n      4\n      Miniature Reactor\n      Veltecite supplies are low, but we need energy...\n    \n    \n      14\n      Classified I\n      10\n      Derelict Explosives\n      Prospector! We need Derelict Explosives, Maste...\n    \n  \n\n\n\n\n\nConclusions\nAnd, there we have it! Another piece to the puzzle solved. Next we’re going to work to combine all the faction rewards, the job requirements and the loot tables together to finally calculate which jobs you should definitly avoid doing."
  },
  {
    "objectID": "posts/nbks/2022-09-30-testing-if-interact-works-online.html",
    "href": "posts/nbks/2022-09-30-testing-if-interact-works-online.html",
    "title": "Quick Test of Ipywidget Interact Function",
    "section": "",
    "text": "While watching the Lectures from Fastai this time around, Jeremy used a Python Decorator called @interact which creates a function with interactable variables. This is really useful feature when you want to experiment with specific values - like Jeremy did in the lecture. However, there is a warning in the Lecture notes: > Reminder: If the sliders above aren’t working for you, that’s because the interactive features of this notebook don’t work in Kaggle’s Reader mode. They only work in Edit mode. Please click “Copy & Edit” in the top right of this window, then in the menu click Run and then Run all. Then you’ll be able to use all the interactive sliders in this notebook.\nLet’s step that back a little bit for an explanation since some of this might not be familiar. Python Decorators are function annotations which modify the behavior of their function. There are excellent articles and descriptions about them but we’ll use this one from the Python Docs as an example: @cache from the functools library.\n\nfrom functools import cache\n@cache\ndef factorial(n):\n    return n * factorial(n-1) if n else 1\n\n\nfactorial(10)      # no previously cached result, makes 11 recursive calls\n\n3628800\n\n\n\nfactorial(5)       # just looks up cached value result\n\n120\n\n\n\nfactorial(12)      # makes two new recursive calls, the other 10 are cached\n\n479001600\n\n\nThe Decorator @cache will modify the behavior of the function to record previous computations. In this way, you can save time using the modified function we’ve written. In the instance with Jeremy, the @interact call modifies the function written to allow us to modify the values in real time without ending execution nor updating the values in the code block. What we’re really after here though is Does this work with Fastpages?\nConsidering the warning given by Jeremy, I wouldn’t expect this to work but I’ve gotten R working in these Jupyter Notebooks and uploaded them so let’s try it. First, we’ll need the imports to do this and then we’ll simply use the example from the Fastai Lecture Notebook and then push it to the site.\n\nfrom ipywidgets import interact\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom functools import partial\n\nplt.rc('figure', dpi=90) # This modifies the size of the graphs\n\n\n# This is just from the notebook\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)\n\n# Define a quadratic function:\ndef f(x): return 3*x**2 + 2*x + 1\n\n# define generic quadratic\ndef quad(a, b, c, x): return a*x**2 + b*x + c\ndef mk_quad(a,b,c): return partial(quad, a,b,c)\n\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n\n\nplot_function(f, \"$3x^2 + 2x + 1$\")\n\n\n\n\n\nf2 = mk_quad(3,2,1)\nplot_function(f2)\n\n\n\n\n\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.15, 1.5)\n\n\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    plt.scatter(x,y)\n    plot_function(mk_quad(a,b,c), ylim=(-3,13))\n\n\n\n\n\nConclusion\nAfter pushing the page, it doesn’t work. I assumed as much but was hopeful it would work."
  },
  {
    "objectID": "posts/nbks/2022-10-26-how-to-quickly-use-other-formats-in-python.html",
    "href": "posts/nbks/2022-10-26-how-to-quickly-use-other-formats-in-python.html",
    "title": "Collecting External Data for Python.",
    "section": "",
    "text": "While preparing for some upcoming blog posts taking material from Design and Analysis of Experiments With R by John Lawson, I wanted to convert the problems and solutions from R code to Python code. Diong this will require using the real data and - luckily - the data from the book is online on Github. Due to how these packages are, the data is uploaded and kept as binary data which we can use. Unfortunately, the data is in the .rda format which doesn’t convert easily into python.\nThere is a package for this to convert the data: pyreadr. Which we’re doing to use to convert the data into a dataframe Python understands. Sadly, this package doesn’t handle urls so we’ll need to download the data first. We could clone out the whole repository to collect the data but then we’d have to start manually managing the data - which I don’t want to do.\nAfter a bit of working around, we can use the tempfile builtin package from Python to create a temporary file to dump the data into. This is useful since these will be deleted after it’s .close() is called on the file. But, we’ll want a Named version since we want this accessible to the file system: > This function operates exactly as TemporaryFile() does, except that the file is guaranteed to have a visible name in the file system (on Unix, the directory entry is not unlinked). That name can be retrieved from the name attribute of the returned file-like object. Whether the name can be used to open the file a second time, while the named temporary file is still open, varies across platforms (it can be so used on Unix; it cannot on Windows).\nSource\nWe’ll use the requests library to pull the data from the internet since it’s builtin and easy to use.\n\n# !pip install pyreadr\nimport pyreadr as pyr\nimport tempfile as tmp\nimport requests as r\n\nOne caveat here is that you’ll need to rewind the read location in the file to read the temporary file otherwise you’ll get an LibrdataError: Unable to read from file.\n\nwith tmp.NamedTemporaryFile() as f:\n    something = r.get(\"https://github.com/cran/daewr/raw/master/data/Apo.rda\").content\n    f.write(something)\n    f.seek(0)\n    data = pyr.read_r(f.name)['Apo']\n\ndata.head(15).T\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n    \n  \n  \n    \n      lab\n      A\n      A\n      A\n      A\n      A\n      A\n      A\n      B\n      B\n      B\n      B\n      B\n      B\n      B\n      B\n    \n    \n      conc\n      1.195\n      1.144\n      1.167\n      1.249\n      1.177\n      1.217\n      1.187\n      1.155\n      1.173\n      1.171\n      1.175\n      1.153\n      1.139\n      1.185\n      1.144\n    \n  \n\n\n\n\nThere we go! You can use this as a simple way to collect data from the internet and feed it into a package which doesn’t support urls to read in data. You can expect its usage in the near future while I work through the textbook."
  },
  {
    "objectID": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html",
    "href": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html",
    "title": "Minimal Spark Cluster",
    "section": "",
    "text": "If you’d like to setup Apache Spark to experiment with but you don’t want to use a premade ISO or setup your own then I’m going to show you how. This configuration will be a minimal one using Linux Operating Sytems; I’m going to use Ubuntu so change the install based on your package mananger. I’m going to assume that you’ve setup the hosts, their networking and have some way to configure and deploy them. There are options like Puppet or Salt but I’ll be avoiding those and leave them up to you."
  },
  {
    "objectID": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#datastore",
    "href": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#datastore",
    "title": "Minimal Spark Cluster",
    "section": "Datastore",
    "text": "Datastore\nNow we’re going to work around not having a Hadoop cluster. How this works, is that we’re going to create a shared folder on all of the hosts which references the Master as the Source of Truth. First, create a folder in your spark home to hold the data:\nmdkir $SPARK_HOME/Data\nGo ahead and create a file in here for future usage: touch turtles\nNext you’ll go ahead and install a package called sshfs which is used to remotely mount a folder from one host and another:\nsudo apt install sshfs\nRepeat this for all the hosts in your cluster. Once that is done, you’ll connect the slaves to the master using:\nsshfs <username>@<master-address>:/opt/spark-2.4.6-bin-hadoop2.7/Data /opt/spark-2.4.6-bin-hadoop2.7/Data\nNow you should be able to see the turtles file we created earlier if you list the files in the Data directory\nls Data\nIf you see the file then feel free to move on! If not, then double back and troubleshoot the connection between those two computers. Could also be permissions or something like that as well!"
  },
  {
    "objectID": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#connect-the-dots-start-the-services",
    "href": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#connect-the-dots-start-the-services",
    "title": "Minimal Spark Cluster",
    "section": "Connect the Dots, Start the Services",
    "text": "Connect the Dots, Start the Services\nNow that we’ve got it all connected together, go ahead and run the appropriate commands on the masters and servers to start them all up:\n# master:\n$SPARK_HOME/sbin/start-master.sh\n\n# slaves:\n$SPARK_HOME/sbin/start-slave.sh spark://<master-Addr>:7077"
  },
  {
    "objectID": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#success",
    "href": "posts/nbks/2021-01-18-minimal-spark-cluster-without-hadoop.html#success",
    "title": "Minimal Spark Cluster",
    "section": "Success!",
    "text": "Success!\nNow try and run it on the master:\nusername@HOST:~# $SPARK_HOME/bin/pyspark \nPython 2.7.12 (default, Apr 15 2020, 17:07:12) \n[GCC 5.4.0 20160609] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n20/10/13 05:19:50 WARN Utils: Your hostname, HOST.localdomain resolves to a loopback address: 127.0.0.1; using <address> instead (on interface eth0)\n20/10/13 05:19:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n20/10/13 05:19:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.6\n      /_/\n\nUsing Python version 2.7.12 (default, Apr 15 2020 17:07:12)\nSparkSession available as 'spark'.\n>>> \nThat should give you the above.\nNow you can transfer data into that directory and read from it using the spark.read.* function that you need. Note that copying Big Data into that directory is not a good idea. If you’re looking at TeraBytes or Petabytes worth of data then you’ll definitely need a real Cluster. But, I’ve already made some interesting observations in this limited environment."
  },
  {
    "objectID": "posts/nbks/2020-09-09-fastbooks-incidentally-supports-r.html",
    "href": "posts/nbks/2020-09-09-fastbooks-incidentally-supports-r.html",
    "title": "Fastbooks Incidentally Supports R",
    "section": "",
    "text": "Pleasant Surprise\nWhile trying to test the boundaries of what fastpages actually supports, I figured I’d try out installing and setting up an R Notebook as well. Luckily enough, it does indeed support compiling and building R kernels as well.\nThe first step will be to install an R kernel for the notebook which can be done using:\nconda install -c r r-essentials\nThis can be ran either from inside a notebook by prepending a ! in a cell such as !conda install -c r r-essentials or simply run it at the console if you’re in linux and in the project directory.\nThis is mostly an exibition post about how this can be done so we’re just going to show off some R stuff.\n\n# I miss this selecting over Python's Pandas:\nmtcars[order(mtcars$gear, mtcars$mpg), ]\n\n\n\nmpgcyldisphpdratwtqsecvsamgearcarb\n\n    Cadillac Fleetwood10.4 8    472.0205  2.93 5.25017.980    0    3    4    \n    Lincoln Continental10.4 8    460.0215  3.00 5.42417.820    0    3    4    \n    Camaro Z2813.3 8    350.0245  3.73 3.84015.410    0    3    4    \n    Duster 36014.3 8    360.0245  3.21 3.57015.840    0    3    4    \n    Chrysler Imperial14.7 8    440.0230  3.23 5.34517.420    0    3    4    \n    Merc 450SLC15.2 8    275.8180  3.07 3.78018.000    0    3    3    \n    AMC Javelin15.2 8    304.0150  3.15 3.43517.300    0    3    2    \n    Dodge Challenger15.5 8    318.0150  2.76 3.52016.870    0    3    2    \n    Merc 450SE16.4 8    275.8180  3.07 4.07017.400    0    3    3    \n    Merc 450SL17.3 8    275.8180  3.07 3.73017.600    0    3    3    \n    Valiant18.1 6    225.0105  2.76 3.46020.221    0    3    1    \n    Hornet Sportabout18.7 8    360.0175  3.15 3.44017.020    0    3    2    \n    Pontiac Firebird19.2 8    400.0175  3.08 3.84517.050    0    3    2    \n    Hornet 4 Drive21.4 6    258.0110  3.08 3.21519.441    0    3    1    \n    Toyota Corona21.5 4    120.1 97  3.70 2.46520.011    0    3    1    \n    Merc 280C17.8 6    167.6123  3.92 3.44018.901    0    4    4    \n    Merc 28019.2 6    167.6123  3.92 3.44018.301    0    4    4    \n    Mazda RX421.0 6    160.0110  3.90 2.62016.460    1    4    4    \n    Mazda RX4 Wag21.0 6    160.0110  3.90 2.87517.020    1    4    4    \n    Volvo 142E21.4 4    121.0109  4.11 2.78018.601    1    4    2    \n    Datsun 71022.8 4    108.0 93  3.85 2.32018.611    1    4    1    \n    Merc 23022.8 4    140.8 95  3.92 3.15022.901    0    4    2    \n    Merc 240D24.4 4    146.7 62  3.69 3.19020.001    0    4    2    \n    Fiat X1-927.3 4     79.0 66  4.08 1.93518.901    1    4    1    \n    Honda Civic30.4 4     75.7 52  4.93 1.61518.521    1    4    2    \n    Fiat 12832.4 4     78.7 66  4.08 2.20019.471    1    4    1    \n    Toyota Corolla33.9 4     71.1 65  4.22 1.83519.901    1    4    1    \n    Maserati Bora15.0 8    301.0335  3.54 3.57014.600    1    5    8    \n    Ford Pantera L15.8 8    351.0264  4.22 3.17014.500    1    5    4    \n    Ferrari Dino19.7 6    145.0175  3.62 2.77015.500    1    5    6    \n    Porsche 914-226.0 4    120.3 91  4.43 2.14016.700    1    5    2    \n    Lotus Europa30.4 4     95.1113  3.77 1.51316.901    1    5    2    \n\n\n\n\n\nmtcars[order(mtcars$gear, mtcars$mpg), ] %>%\n    ggplot(aes(disp, hp, colour = cyl)) + \n    geom_point()\n\n\n\n\n\n\n\n\ncrimes <- data.frame(state = tolower(rownames(USArrests)), USArrests)\n\n# Equivalent to crimes %>% tidyr::pivot_longer(Murder:Rape)\n vars <- lapply(names(crimes)[-1], function(j) {\ndata.frame(state = crimes$state, variable = j, value = crimes[[j]])\n})\ncrimes_long <- do.call(\"rbind\", vars)\n\nstates_map <- map_data(\"state\")\nggplot(crimes_long, aes(map_id = state)) +\n    geom_map(aes(fill = value), map = states_map) +\n    expand_limits(x = states_map$long, y = states_map$lat) +\n    facet_wrap( ~ variable)\n\n\n\n\n\n\n\nI did also try to use ggvis as well but it just wont display properly so that’s unfortunately out."
  },
  {
    "objectID": "posts/nbks/2022-11-10-simple-idea-using-memcached.html",
    "href": "posts/nbks/2022-11-10-simple-idea-using-memcached.html",
    "title": "Simple Introduction to Memcached",
    "section": "",
    "text": "While taking a class to discover what is the currently most popular NoSQL databases for different use cases, I was informed that there is this technology called Memcached. From the Arch Wiki: > Memcached (pronunciation: mem-cashed, mem-cash-dee) is a general-purpose distributed memory caching system. It is often used to speed up dynamic database-driven websites by caching data and objects in RAM to reduce the number of times an external data source (such as a database or API) must be read.  Source\nThis was a technology developed at Live Journal to help with - well - caching commonly used values. This is interesting but why bring any of this up? Well, this is because NoSQL Database use a Key - Value pair to look up the matching values and that happens to be how this works as well. However, there are some hard limitations with this and especially related to the size of what is allowed to be cached; Looking around the default looks to be 1MB and you can configure it up to 1GB but that’s it.\nI was thinking about how you could apply this to Data Science and it’s pretty limited. For one, the only useful stuff to share across sessions would be either the data you are using or the actual trained model itself. Since the data to be used would assuredly be larger than the configured limit that is not of much use - at least for most interesting problems. And, as there is a glut of tooling for hosting applications online you are very unlikely to need to setup a cache for the model. The tooling online does this really for you with instances and such.\nBut, I did have an interesting idea about what I could use this for. After you’ve worked on problems, you’re bound to have functions that have been written to solve common problems. Keeping these means finding that code, then copying it into your project and finally using it for what you want. You could build a python package just for yourself but that seems overkill unless it’s a general topic to share with others.\nWhat if these simply functions could simply be a network share library? For example, date formats are something that I tend to need to convert with Python data frames. And, sadly there are no nice date format functions like there is in R; I do miss the R lubridate functions which has functions to convert a date into commonly needed formats: such as ymd(date) would convert the date into a Year-Month-Day format for display. I wrote a few lambda functions in python to do this for me and I would want them accessible while I do data exploration.\nSo, how would we go about doing this? First we’d need to install memcached for your Operating System; I have already done this but the guide from this Real Python goes over how you would do it for your own system. Mine being Manjaro, it didn’t include it and I had to find it on the Arch Wiki. Make sure to start the service and then we’ll start this off.\n\nfrom pymemcache.client import base\n# init a client; make sure it is already running:\nclient = base.Client(('localhost', 11211))\n\nUsing this is very simple and there really are only two functions to care about: get() and set(). If we wanted to set a value then we tell the client what the key, value pair is.\n\nclient.set('turtles', 'Turtles')\nclient.get('turtles')\n\nb'Turtles'\n\n\nAnd, that’s really all there is to using this from Python!\nI would like to point out that the results are encoded as byte type. This is not a problem for that text but is a problem as soon as you need to operate on the values.\n\nclient.set('someNumber', 42)\n\niLike = client.get('turtles')\ncount = client.get('someNumber')\n\nprint(f'I had {count} {iLike} but when I got 2 more I had {count +2} {iLike}')\n\nTypeError: can't concat int to bytes\n\n\nWe can solve this with a cast in this case at least.\n\nclient.set('someNumber', 42)\n\niLike = client.get('turtles')\ncount = client.get('someNumber')\n\nprint(f'I had {count.decode()} {iLike.decode()} but when I got 2 more I had {int(count) +2} {iLike.decode()}')\n\nI had 42 Turtles but when I got 2 more I had 44 Turtles\n\n\nSo, can we take a lambda function and put it in memcached?\n\nf = (lambda x: print(f'{x} likes turtles'))\nclient.set('iLike', f)\nclient.get('iLike')\n\nb'<function <lambda> at 0x7f9f70829000>'\n\n\nIt accepts it! That’s the good news. The bad news is that since it was converted it no longer works as a function.\n\nf(\"He\"), client.get('iLike')(\"He\")\n\nHe likes turtles\n\n\nTypeError: 'bytes' object is not callable\n\n\nYou cannot just decode it and get what we want.\n\nclient.get('iLike').decode()(\"He\")\n\nTypeError: 'str' object is not callable\n\n\nWe can work around this by serializing the object and then deserialize it on the other side. We’ll need to use dill and pickle; you may need to install the dill package since it is not part of the standard library but it is a requirement for this to work.\n\ns = dill.dumps(f)\nclient.set('cereal', s)\ndill.loads(client.get('cereal'))(\"He\")\n\nHe likes turtles\n\n\nNow we can implement the function I want as a Network Shared Library!\n\nfrom datetime import datetime\naDate = datetime.now()\n\n# My custom function:\nymd = (lambda x: \"{y}/{m}/{d}\".format(y=x.year, m=x.month, d=x.day ))\ns = dill.dumps(ymd)\n\n# Store in 'network library'\nclient.set('ymd', s)\nundo = (lambda key: dill.loads(client.get(key)))\n\nundo('ymd')(aDate)\n\n'2022/11/10'\n\n\nThere you go! If you have a spare Rasberry Pi or something then you too can have a small library of custom functions shareable over your home network to use!"
  },
  {
    "objectID": "posts/nbks/2022-09-28-cycle-jobs-part-one.html",
    "href": "posts/nbks/2022-09-28-cycle-jobs-part-one.html",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part One.",
    "section": "",
    "text": "This post series was inspired by the Korolev Job And Two Smoking Barrels where upon accepting the quest I realized how terrible the rewards were in comparison. For those that don’t play the game a little background will be necessary. The Cycle: Frontier is an Evacuation Shooter game and there are three main Corporations - or Organizations, if you prefer - who act as quest and reward givers in the game. There are two kinds of these: Campaign and Jobs. Whereas the campaign quests will push you along to different areas of the world, instead the Jobs function as a way to collect scrips and excuse to send players to the planet.\nThere are three kinds of quests: 1. Collect Stuff. 2. Deposit Stuff. 3. Kill Stuff: including players.\nFor Deposit Jobs, you carry the requested items to a Dead Drop and then deposit the items in question. For this one, it requires you deposit a gun you purchase from the shop. Now, you could find this weapon or loot it from other people but those are not garunteed at all. The Kmark - which is cash, basically - reward is $19,000 and the gun it wants you to deposit is $22,000 so there is no reason to take this job unless you already have this gun. Anyways, lets get to the fun part."
  },
  {
    "objectID": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#scraping-and-cleaning-cycle-data",
    "href": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#scraping-and-cleaning-cycle-data",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part One.",
    "section": "Scraping and Cleaning Cycle Data",
    "text": "Scraping and Cleaning Cycle Data\nSo, we’ll start with the normal imports for doing data in Python.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport requests as r\n\nThankfully, there is an official wiki for the game which is maintained by both the Developers and the Community together. We’re going to pull the data from there as that should be the most up to date data. Like most online data scraping, there is some try-fail loops to getting what you’re after from the webpage. Since there are three organizations, there are three tables with jobs we’d like to pull from the website. After some trial and error I found that using match=\"Name\" was perfect for pulling the tables out of the webpage.\n\nurl = \"https://thecyclefrontier.wiki/wiki/Jobs\"\n\nsite = pd.read_html(url, match=\"Name\",\n    converters = {\n        \"Name\": str,\n        \"Description\": str, \n        \"Unlocked\": int, \n        \"Tasks\": str,\n        \"Rewards\": str})\n\nYou may notice the addition of converters argument above which is a really useful feature I didn’t know previously; basically, if you know the column names coming in then you can tell Pandas what data type you want so you don’t have to convert later. So, what does the data look like?\n\nsite[0].head(8)\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Unlock Level\n      Difficulty\n      Tasks\n      Rewards\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      4.0\n      Easy\n      Collect: 2 Hydraulic Piston 10 Hardened Metals\n      3800  K-Marks  1  Korolev Scrip  15  Korolev R...\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      7.0\n      Medium\n      Collect: 4 Derelict Explosives\n      11000  K-Marks  8  Korolev Scrip  52  Korolev ...\n    \n    \n      5\n      11000\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6\n      8\n      Korolev Scrip\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7\n      52\n      Korolev Reputation\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nThat is not really what we were hoping would come in. Checking the actual site, this is caused by a table which exists inside one of the row cells. But - like in the previous post - this can still be used after some work. So, lets get to work!"
  },
  {
    "objectID": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#fixing-the-job-rewards",
    "href": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#fixing-the-job-rewards",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part One.",
    "section": "Fixing the Job Rewards",
    "text": "Fixing the Job Rewards\nWe dont need most of the columns for what we’re going to be accomplishing so we’re going to pull them out. I’m going to do a copy as pandas sometimes doesn’t play so nicely with updates. What I’ve found is that since Pandas uses pointers underneath, sometimes when doing updates to slices I don’t always get what I expect. So, we’ll splice and copy to only get the data we care about in its own independent dataframe.\n\nrewardsSubset = site[0][[\"Name\", \"Description\", \"Difficulty\"]].copy()\nrewardsSubset\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      Difficulty\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      Easy\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      Medium\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      183\n      470\n      Korolev Reputation\n      NaN\n    \n    \n      184\n      No Expiry Date\n      There you are, finally! There's been an accide...\n      Medium\n    \n    \n      185\n      6300\n      K-Marks\n      NaN\n    \n    \n      186\n      9\n      Korolev Scrip\n      NaN\n    \n    \n      187\n      62\n      Korolev Reputation\n      NaN\n    \n  \n\n188 rows × 3 columns\n\n\n\nThese column names are not useful so we’re going to correct those so they make sense with the project. We’re going to call the final column Job which will make more sense as the work gets done.\n\nrewardsSubset.columns = [\"Units\", \"Rewards\", \"Job\"]\nrewardsSubset.head()\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      Easy\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      Medium\n    \n  \n\n\n\n\nSo, looking at above we can see that we have extra data in the Job Column which is no longer appropriate. We’re going to simply fill that column with a null value: np.NaN\n\nrewardsSubset.Job = np.NaN\nrewardsSubset.head(12)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      NaN\n    \n    \n      1\n      3800\n      K-Marks\n      NaN\n    \n    \n      2\n      1\n      Korolev Scrip\n      NaN\n    \n    \n      3\n      15\n      Korolev Reputation\n      NaN\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      NaN\n    \n    \n      5\n      11000\n      K-Marks\n      NaN\n    \n    \n      6\n      8\n      Korolev Scrip\n      NaN\n    \n    \n      7\n      52\n      Korolev Reputation\n      NaN\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n      NaN\n    \n    \n      9\n      6900\n      K-Marks\n      NaN\n    \n    \n      10\n      9\n      Korolev Scrip\n      NaN\n    \n    \n      11\n      62\n      Korolev Reputation\n      NaN\n    \n  \n\n\n\n\nSo, now for the hard part: getting the Job Title into the Job Column. Looking at the data above, we can see that the Job Title is always stored in a multiple of four. We can confirm this by simply dividing the total number of columns by 4 just to be sure.\n\n# This should be divisible by 4 since they rewards for jobs are always in this format now.\nlen(rewardsSubset) / 4\n\n47.0\n\n\nAnd, we have a perfect divide! Good! This is since each Job always hands out Kmarks, a matching Corp Scrip and Corp Reputation. So, what we need to do now is pull the Job Title from the Units column and insert it into the next three columns under Job. To do this, we’re going to build a range of values which are multiples of 4 starting at 0 and up to the total amount of jobs. We don’t want to hard code this since the count of jobs should be expected to change over time.\n\ntopIndex = len(rewardsSubset) / 4 - 3\nindex = range( 0, 44, 4)\n\nNext we’ll want a numpy array of the offsets. We don’t want to use a list because then it will add the values to a python list instead of creating a set of indexes. In effect, we’re trying to take advantage of Broadcasting in numpy. We’ll do an illustration of this quick.\n\nlistMistake = [1,2,3]\nbroadcastCorrect = np.array(listMistake)\n\n[index[1]] + listMistake, index[1] + broadcastCorrect\n\n([4, 1, 2, 3], array([5, 6, 7]))\n\n\nAbove you can see [4, 1, 2, 3] is definitely not what we’re after. So, after setting up the proper offset lets make sure we’re getting what we want. I often sanity check my initial design since experience as taught me you can still trip even after the initial testing works. So, lets do that now.\n\noffset = np.array([1, 2, 3])\n\n\n# this is how we'll iterate; proof it works.\nfor i in index[:3]:\n    aJob = rewardsSubset.iloc[i, 0]\n    print(f'{aJob} is at index {i}')\n\nNew Mining Tools is at index 0\nExplosive Excavation is at index 4\nMining Bot is at index 8\n\n\nAnd, there we go! We’re getting exactly what we wanted and expected. This is also a good initial test for the loop which we’re going to tuck into a function at the end of all this. So, now to test the logic of swapping the values from the Unit Column to the Job Column.\n\n# Do the thing:\naJob = rewardsSubset.iloc[index[0], 0]\nindexes = index[0] + offset\nrewardsSubset.iloc[ indexes, 2 ] = aJob\nrewardsSubset.head(9)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      NaN\n    \n    \n      1\n      3800\n      K-Marks\n      New Mining Tools\n    \n    \n      2\n      1\n      Korolev Scrip\n      New Mining Tools\n    \n    \n      3\n      15\n      Korolev Reputation\n      New Mining Tools\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      NaN\n    \n    \n      5\n      11000\n      K-Marks\n      NaN\n    \n    \n      6\n      8\n      Korolev Scrip\n      NaN\n    \n    \n      7\n      52\n      Korolev Reputation\n      NaN\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n      NaN\n    \n  \n\n\n\n\n\nfor i in index:\n    aJob = rewardsSubset.iloc[i, 0]\n    indexes = i + offset\n    rewardsSubset.iloc[ indexes, 2 ] = aJob\n\nrewardsSubset.head(12)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      0\n      New Mining Tools\n      We are producing new Mining Tools for new Pros...\n      NaN\n    \n    \n      1\n      3800\n      K-Marks\n      New Mining Tools\n    \n    \n      2\n      1\n      Korolev Scrip\n      New Mining Tools\n    \n    \n      3\n      15\n      Korolev Reputation\n      New Mining Tools\n    \n    \n      4\n      Explosive Excavation\n      One of our mines collapsed with valuable equip...\n      NaN\n    \n    \n      5\n      11000\n      K-Marks\n      Explosive Excavation\n    \n    \n      6\n      8\n      Korolev Scrip\n      Explosive Excavation\n    \n    \n      7\n      52\n      Korolev Reputation\n      Explosive Excavation\n    \n    \n      8\n      Mining Bot\n      Our engineers have designed an autonomous mini...\n      NaN\n    \n    \n      9\n      6900\n      K-Marks\n      Mining Bot\n    \n    \n      10\n      9\n      Korolev Scrip\n      Mining Bot\n    \n    \n      11\n      62\n      Korolev Reputation\n      Mining Bot\n    \n  \n\n\n\n\nPerfect! Now all we have to do is cut the Units Columns where the Job Title still remains. Luckily, the np.NaN has remained so we can collect the indexes for Job where that values exists. And, then simply get rid of them.\n\n# Kill the NA's\ncutNA = rewardsSubset.Job.isna()\nrewardsSubset[ ~cutNA ].head(15)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      1\n      3800\n      K-Marks\n      New Mining Tools\n    \n    \n      2\n      1\n      Korolev Scrip\n      New Mining Tools\n    \n    \n      3\n      15\n      Korolev Reputation\n      New Mining Tools\n    \n    \n      5\n      11000\n      K-Marks\n      Explosive Excavation\n    \n    \n      6\n      8\n      Korolev Scrip\n      Explosive Excavation\n    \n    \n      7\n      52\n      Korolev Reputation\n      Explosive Excavation\n    \n    \n      9\n      6900\n      K-Marks\n      Mining Bot\n    \n    \n      10\n      9\n      Korolev Scrip\n      Mining Bot\n    \n    \n      11\n      62\n      Korolev Reputation\n      Mining Bot\n    \n    \n      13\n      7600\n      K-Marks\n      None of your Business\n    \n    \n      14\n      10\n      Korolev Scrip\n      None of your Business\n    \n    \n      15\n      90\n      Korolev Reputation\n      None of your Business\n    \n    \n      17\n      10000\n      K-Marks\n      Insufficient Processing Power\n    \n    \n      18\n      11\n      Korolev Scrip\n      Insufficient Processing Power\n    \n    \n      19\n      110\n      Korolev Reputation\n      Insufficient Processing Power"
  },
  {
    "objectID": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#function-to-build-job-rewards",
    "href": "posts/nbks/2022-09-28-cycle-jobs-part-one.html#function-to-build-job-rewards",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part One.",
    "section": "Function to build Job Rewards",
    "text": "Function to build Job Rewards\nNow that we have all this we can push it into a function and run it against all the different Corporation tables.\n\ndef buildJobsRewards(data):\n    # Function to take job rewards data and return a cleaned version\n\n    rewardsSubset = data[[\"Name\", \"Description\", \"Difficulty\"]].copy()\n    rewardsSubset.columns = [\"Units\", \"Rewards\", \"Job\"]\n\n    index = range( 0, len(rewardsSubset) - 4, 4)\n    offset = np.array([1, 2, 3])\n\n    rewardsSubset.Job = np.NaN\n\n    for i in index:\n        aJob = rewardsSubset.iloc[i, 0]\n        indexes = i + offset\n        rewardsSubset.iloc[ indexes, 2 ] = aJob\n        \n    cutNA = rewardsSubset.Job.isna()\n    rewardsSubset = rewardsSubset[ ~cutNA ]\n\n    rewardsSubset = rewardsSubset.assign(\n        Units = rewardsSubset['Units'].astype(int)\n    )\n\n    return rewardsSubset\n\nAnd, the final test!\n\nKorolevRewards = buildJobsRewards( site[0] )\nicaRewards = buildJobsRewards( site[1] )\nosirisRewards = buildJobsRewards( site[2] )\n\n\nKorolevRewards.head(9)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      1\n      3800\n      K-Marks\n      New Mining Tools\n    \n    \n      2\n      1\n      Korolev Scrip\n      New Mining Tools\n    \n    \n      3\n      15\n      Korolev Reputation\n      New Mining Tools\n    \n    \n      5\n      11000\n      K-Marks\n      Explosive Excavation\n    \n    \n      6\n      8\n      Korolev Scrip\n      Explosive Excavation\n    \n    \n      7\n      52\n      Korolev Reputation\n      Explosive Excavation\n    \n    \n      9\n      6900\n      K-Marks\n      Mining Bot\n    \n    \n      10\n      9\n      Korolev Scrip\n      Mining Bot\n    \n    \n      11\n      62\n      Korolev Reputation\n      Mining Bot\n    \n  \n\n\n\n\n\nicaRewards.head(9)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      1\n      4400\n      K-Marks\n      Water Filtration System\n    \n    \n      2\n      1\n      ICA Scrip\n      Water Filtration System\n    \n    \n      3\n      15\n      ICA Reputation\n      Water Filtration System\n    \n    \n      5\n      7500\n      K-Marks\n      New Beds\n    \n    \n      6\n      9\n      ICA Scrip\n      New Beds\n    \n    \n      7\n      62\n      ICA Reputation\n      New Beds\n    \n    \n      9\n      13000\n      K-Marks\n      Station Defense\n    \n    \n      10\n      12\n      ICA Scrip\n      Station Defense\n    \n    \n      11\n      130\n      ICA Reputation\n      Station Defense\n    \n  \n\n\n\n\n\nosirisRewards.head(9)\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n    \n  \n  \n    \n      1\n      2200\n      K-Marks\n      Lab equipment\n    \n    \n      2\n      1\n      Osiris Scrip\n      Lab equipment\n    \n    \n      3\n      13\n      Osiris Reputation\n      Lab equipment\n    \n    \n      5\n      8100\n      K-Marks\n      Surveillance Center\n    \n    \n      6\n      8\n      Osiris Scrip\n      Surveillance Center\n    \n    \n      7\n      43\n      Osiris Reputation\n      Surveillance Center\n    \n    \n      9\n      8100\n      K-Marks\n      Gun Manufacturing\n    \n    \n      10\n      8\n      Osiris Scrip\n      Gun Manufacturing\n    \n    \n      11\n      52\n      Osiris Reputation\n      Gun Manufacturing"
  },
  {
    "objectID": "posts/nbks/2022-09-08-reviewing-fastai-2022-coursework-thebeginning.html",
    "href": "posts/nbks/2022-09-08-reviewing-fastai-2022-coursework-thebeginning.html",
    "title": "The Lastest Lectures For Fastai are Online.",
    "section": "",
    "text": "If you’re not familiar with the list of frameworks and libraries that exist for Deep Learning - or even if you are - then you might not be aware of Fastai. This is both a Deep Learning Framework built on top of Pytorch as well as an online curriculum to quickly become productive using Deep Learning for your own interests. Jeremy is a very solid teacher and I recommend taking the course if you’re interested in learning how this all works. And, I don’t mean that simply because I like the lectures but also because I’ve taken them before and I’ve also built my own simple projects out of the lectures done by him - and Rachel since she’s also assisting with building the course. One really big plus for this course is that you’ll start by actually building a model and using it: in this version it will be Birds vs Forests instead of Cats vs Dogs like used to be. As Jeremy points out in the first lecture, most people don’t learn in an academic way. By that we mean they start with theory and then learn how to interact with the system. Instead, we learn a few basic ideas and then toss ourselves in to apply and learn as we go.\n\n\nLately, I’ve been playing a game called The Cycle: Frontier which is a kind of Extraction Shooter game. What this means is that players are dropped into a semi-persistent server to collect items, kill creatures and even kill other players. Really, it is up to the player to decide how they interact with and play the game. But being a shooter means that there are guns - and since there are guns there are categories of guns. Since fantasy is inspired by the real world in some sense, the question I would like to ask is Knowing Models are Derived from the Real World Weapons, can we build a classifier based on real life weapons that can correctly predict Fantasy Weapons?\nThis post, like the lectures, is not going to be about how this all works but instead is going to be a simple application and retrospective. That being said, let’s start! We’ll start with the important imports for getting this working.\n\nimport os\nfrom pathlib import Path\nfrom time import sleep\n\n\nfrom duckduckgo_search import ddg_images # Will write more about this later.\nfrom fastdownload import download_url    # Will need to explore this more.\n\nfrom fastcore.all import *\nfrom fastai.vision.all import *     # This is for the CNN learner.\nfrom fastai.vision.widgets import * # This is required for the cleaner later.\n\nThe first three imports you should hopefully be familiar with. Everything else you should simply grant for now.\n\n# This is a function from the notebook:\ndef search_images(term, max_images=200):\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nurls = search_images('assult rifle photos', max_images=1)\nurls[0]\n\n'http://allhdwallpapers.com/wp-content/uploads/2016/07/Assault-Rifle-5.jpg'\n\n\nSo, we’re going to download an example image using the helper functions so far and make sure its working:\n\ndest = Path('..', '__data', 'example-ar-gun.png')\ndownload_url(urls[0], dest, show_progress=False)\n\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\nNow let’s check for a DMR and make sure that is sane:\n\ndmrUrls = search_images('dmr photos', max_images=1)\ndest = Path('..', '__data', 'example-dmr-gun.png')\ndownload_url(dmrUrls[0], dest, show_progress=False)\n\nim = Image.open(dest).to_thumb(256,256)\nim\n\n\n\n\nSo, now we need data from the Internet:\n\nsearches = 'assault rifle','dmr'\npath = Path('..', '__data', 'ar_or_dmr')\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server and blocking responses\n    resize_images(path/o, max_size=400, dest=path/o)\n\n/usr/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\nThe below just checks to make sure that the downloaded images are in fact images. And, then we’re iterating though the results to remove what are failed images:\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n4\n\n\nWe can even check the source code using ?? and see for ourselves that is what it is doing:\n\n??verify_images\n\nSignature: verify_images(fns)\nDocstring: Find images in `fns` that can't be opened\nSource:   \ndef verify_image(fn):\n    \"Confirm that `fn` can be opened\"\n    try:\n        im = Image.open(fn)\n        im.draft(im.mode, (32,32))\n        im.load()\n        return True\n    except: return False\nFile:      ~/.local/lib/python3.10/site-packages/fastai/vision/utils.py\nType:      function\n\n\nDataBlocks are are really good idea which is a lazy wrapper that defines how and what to do with the downloaded images in preparation for use in a model. There is more information at the docs page here where you can check out other ways to use this. For now, we’re following Jeremy’s example and just mostly running the code as is.\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=12)\n\n\n\n\nLooking over some of htese images, we can already tell that we’re getting images that are not what we’re after. The second image in the top row is some kind radio which is not what we want. Regardless, we’re again going to simply train the model even with that image in there since we’re following Jeremy’s advice and just going to train the model and we’ll deal with the strange Radio later.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(7)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.135293\n      1.895362\n      0.333333\n      00:01\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.783494\n      1.115906\n      0.250000\n      00:01\n    \n    \n      1\n      0.578771\n      0.935797\n      0.222222\n      00:01\n    \n    \n      2\n      0.455172\n      0.774320\n      0.263889\n      00:01\n    \n    \n      3\n      0.365537\n      0.762151\n      0.263889\n      00:01\n    \n    \n      4\n      0.311685\n      0.737330\n      0.277778\n      00:01\n    \n    \n      5\n      0.263628\n      0.734078\n      0.277778\n      00:01\n    \n    \n      6\n      0.224883\n      0.718644\n      0.291667\n      00:01\n    \n  \n\n\n\nSo, this is what training will look like. The highlights are that training was very fast but - compared to many other models trained, including Jeremy’s - this one is doing pretty terrible for a Deep Learning model. The validation loss is worse than random chance which means there are very serious problems with either out Deep Learning Architecture or the data. And, the architecture is definitely not the problem so it’s the data.\n\nis_ar,_,probs = learn.predict(PILImage.create(Path('__data', 'example-dmr-gun.png')))\nprint(f\"This is a(n): {is_ar}.\")\nprint(f\"Probability it's an Assult Rifle: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a(n): assault rifle.\nProbability it's an Assult Rifle: 0.9858\n\n\n\nim = Image.open(Path('__data', 'example-dmr-gun.png')).to_thumb(512,512)\nim\n\n\n\n\nBig Oof. That’s not an Assault Rifle.\nTime to do some data cleaning. Thankfully, the Fastai library comes with a very useful function which takes the images from the data, checks what their loss is and then presents them to us so that we can remove or re-label them. Make sure you don’t make the same mistake as I did and correct import the widgets: from fastai.vision.widgets import *\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t forget once you’ve actually ran and collected what changes you want made, you’ll need actually run the below code to actually make those corrections:\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\nWhile trying to train this, I found that I was having issues unless I re-built the Data Loader so that’s what we’re doing here:\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=12)\n\n\n\n\nGot some weird stuff again. Looks like some bikes as well as some other equipment that I don’t know. That would imply that using dmr is a bad search term for what we’re after. We’ll do one more training attempt to see how much harm those are doing and then we’ll consider correcting the search terms.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(7)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.281920\n      1.533394\n      0.413333\n      00:02\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.869596\n      0.949424\n      0.306667\n      00:02\n    \n    \n      1\n      0.667905\n      0.852354\n      0.266667\n      00:02\n    \n    \n      2\n      0.529514\n      0.767100\n      0.213333\n      00:02\n    \n    \n      3\n      0.418889\n      0.783195\n      0.213333\n      00:01\n    \n    \n      4\n      0.338373\n      0.754332\n      0.226667\n      00:02\n    \n    \n      5\n      0.277043\n      0.730826\n      0.226667\n      00:02\n    \n    \n      6\n      0.237752\n      0.718148\n      0.226667\n      00:02\n    \n  \n\n\n\nLooks like our validation loss is still struggling so it’s time to update our search term from dmr to designated marksmen rifle. Speedrun time!\n\nsearches = 'assault rifle','designated marksmen rifle'\npath = Path('..', '__data', 'ar_or_dmr_v2')\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    resize_images(path/o, max_size=600, dest=path/o)\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\n\n(#7) [None,None,None,None,None,None,None]\n\n\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=12)\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(7)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.279173\n      0.758920\n      0.247312\n      00:02\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.725494\n      0.552330\n      0.215054\n      00:02\n    \n    \n      1\n      0.572957\n      0.452633\n      0.161290\n      00:02\n    \n    \n      2\n      0.429993\n      0.460270\n      0.172043\n      00:02\n    \n    \n      3\n      0.329015\n      0.517723\n      0.215054\n      00:02\n    \n    \n      4\n      0.260919\n      0.542659\n      0.215054\n      00:02\n    \n    \n      5\n      0.212036\n      0.543762\n      0.204301\n      00:02\n    \n    \n      6\n      0.176182\n      0.541302\n      0.215054\n      00:02\n    \n  \n\n\n\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n\n\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=12)\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(7)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.303078\n      1.055721\n      0.304348\n      00:02\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.759311\n      0.487650\n      0.250000\n      00:02\n    \n    \n      1\n      0.568219\n      0.461627\n      0.184783\n      00:02\n    \n    \n      2\n      0.444043\n      0.539364\n      0.163043\n      00:02\n    \n    \n      3\n      0.340317\n      0.588352\n      0.141304\n      00:02\n    \n    \n      4\n      0.269084\n      0.642941\n      0.152174\n      00:02\n    \n    \n      5\n      0.217371\n      0.619099\n      0.152174\n      00:02\n    \n    \n      6\n      0.183307\n      0.610297\n      0.163043\n      00:02\n    \n  \n\n\n\n\nis_ar,_,probs = learn.predict(PILImage.create(Path('..', '__data', 'example-dmr-gun.png')))\nprint(f\"This is a(n): {is_ar}.\")\nprint(f\"Probability it's an Assult Rifle: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a(n): designated marksmen rifle.\nProbability it's an Assult Rifle: 0.0029\n\n\nThat is much better than what we were getting. Now for the real test: Can the model tell the difference between an Assult Rifle and a DMR from the game’s wiki.\n\nfor image in [PILImage.create(Path('..', '__data', 'cycle-dmr-gun.png')), PILImage.create(Path('..', '__data', 'cycle-ar-gun.png'))]:\n    plt.figure()\n    plt.imshow(image)\n\n\n\n\n\n\n\n\nis_ar,_,probs = learn.predict(PILImage.create(Path('..', '__data', 'cycle-dmr-gun.png')))\nprint(f\"This is a(n): {is_ar}.\")\nprint(f\"Probability it's an Assult Rifle: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a(n): assault rifle.\nProbability it's an Assult Rifle: 0.9968\n\n\n\nis_ar,_,probs = learn.predict(PILImage.create(Path('..', '__data', 'cycle-ar-gun.png')))\nprint(f\"This is a(n): {is_ar}.\")\nprint(f\"Probability it's an Assult Rifle: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a(n): assault rifle.\nProbability it's an Assult Rifle: 0.9805\n\n\nLooking like either I don’t have enough data or I don’t know enough about the domain because it is struggling to figure it out.\n\n\n\nDMR also stands for Digital Mobile Radio which is why we were getting all those weird results earlier in the post."
  },
  {
    "objectID": "posts/nbks/2022-11-09-fp-discounted-cash-flows.html",
    "href": "posts/nbks/2022-11-09-fp-discounted-cash-flows.html",
    "title": "Financial Planner Review - Discounting Cash Flows",
    "section": "",
    "text": "Among the list of formulas I learned in the Financial Analyst coures, this is valuation is the one I instintively distrusted. Discounted Cash Flows is a way to evaluate the future returns on an investement based on: 1. The initial Investment. 2. The Expected Cashflow per each year.\nThe problem I have here is that the Expected Cashflows are purely guesses about what we belive the numbers to be in the future. While the example in the class picked - as far as I can tell - random numbers to fill them in, I would hope that the numbers provided here would be based on research of other similar projects. At least, I would hope in the real world this is was happens."
  },
  {
    "objectID": "posts/nbks/2022-11-09-fp-discounted-cash-flows.html#discounting-cash-flows",
    "href": "posts/nbks/2022-11-09-fp-discounted-cash-flows.html#discounting-cash-flows",
    "title": "Financial Planner Review - Discounting Cash Flows",
    "section": "Discounting Cash Flows",
    "text": "Discounting Cash Flows\nThe formula for this is: > \\(DCF= (CF_1)/(1 + r)^1 + (CF_2)/(1 + r)^2 + .. + (CF_n)/(1 + r)^n\\)\n… where: \n\nCF stands for Cash Flow\nr is the interest rate\nn is the period of discount.\n\nThis all follows from the simple idea that money now is more valuble than money in the future. If given the choice between $100 now and $100 in a year then you’d obviously take the $100 now. This also applies to money we earn in the future: $60 now is better than the $60 we’d earn in the future. So, example time.\nWe’re going to use a similar example to the class: Imagine that we’re considering inveseting money into a venture. We’re going to do something a bit more modern and say we’re investing in a growing Online Streamer with the expectation that we’ll get some of the money in turn. A deal is worked out and you’ll be getting some kind of slice of the money they make in exchange. They’re going to ask for $500 in investment. For the interest rate, we’ll use the Inflation Rate since this is not a loan. Right now, it’s 8.2% so we’ll use that as our Interest Rate; Sometimes this also called the Discount Rate and Inflation certainly applies.\nWe’ll use random numbers over the span of 6 years and then calculate if this was a good idea.\n\nfrom random import randint\n\nyear = range(0,6)\ninterestRate = .082\n\npredictions = [ randint(0, 300) for _ in range(5)]\ncashFlow = [-500] + predictions\ncashFlow\n\n[-500, 195, 127, 44, 42, 196]\n\n\nWe’ll need a function to calculate the Present Value for each term in the formula. For this, we’ll just write up a quick lambda function in python.\n\nPV = (lambda f,i,n: f/(1+i)**n)\nPV(30, interestRate, 1)\n\n27.726432532347502\n\n\nWe’ll usually see this in a table so we’ll add all this to a Data frame.\n\ndata = pd.DataFrame(\n    {\"Year\":year,\n    \"Cash\":cashFlow,\n    'Pv':repeat(0,len(year))\n})\ndata\n\n\n\n\n\n  \n    \n      \n      Year\n      Cash\n      Pv\n    \n  \n  \n    \n      0\n      0\n      -500\n      0\n    \n    \n      1\n      1\n      195\n      0\n    \n    \n      2\n      2\n      127\n      0\n    \n    \n      3\n      3\n      44\n      0\n    \n    \n      4\n      4\n      42\n      0\n    \n    \n      5\n      5\n      196\n      0\n    \n  \n\n\n\n\n… and now we can iterate through the rows and fill in the Present Value per year.\n\n\nfor _,r in data.iterrows():\n    data.loc[r.Year, 'PV'] = round(PV(r.Cash, interestRate, r.Year), 2 )\n\nLastly, we’ll take the sum to see if it was worth it.\n\ndata.PV.sum()\n\n-13.749999999999972\n\n\nLooks like we lost about $14 which is not that surprising since making money in Streaming can be quite challenging."
  },
  {
    "objectID": "posts/nbks/2022-10-19-ferrets-or-dragons.html",
    "href": "posts/nbks/2022-10-19-ferrets-or-dragons.html",
    "title": "Something Cute Or Something Dangerous",
    "section": "",
    "text": "Today we’re going to do something a tad simpier since the last attempt to build an image Classification model was too ambitious. Today we’re going to build an image classifier model to tell the difference between Ferrets and Dragons. These are categories I’m far more familiar with and much more comfortable telling apart. After all, one is a floofer and the other is a scaley boi.\nWe’re going to be using the same code and the same process as before. As a guideline, if you’re writing the same code more than twice then you should turn it into a function; if we’re writing the same notebook code more than twice then it’s time to build a template. By Template I mean a notebook that contains the boilerplate code which will be constaintly used over and over again. There may be a better name for this concept but looking over other words they don’t fit. And, the template concept is already something used in Web Design - such as [Jinja](https://en.wikipedia.org/wiki/Jinja_(template_engine). Although,"
  },
  {
    "objectID": "posts/nbks/2022-10-19-ferrets-or-dragons.html#the-actual-work",
    "href": "posts/nbks/2022-10-19-ferrets-or-dragons.html#the-actual-work",
    "title": "Something Cute Or Something Dangerous",
    "section": "The Actual Work",
    "text": "The Actual Work\nWe’ll start with our normal imports for working on these problems. Note that we didn’t need to import pandas and such since fastai is doing this for us.\n\nimport os\nfrom pathlib import Path\nfrom time import sleep\n\n\nfrom duckduckgo_search import ddg_images\nfrom fastdownload import download_url\n\nfrom fastcore.all import *\nfrom fastai.vision.all import *     # This is for the CNN learner.\n\nAs before, we’re going to reuse the image searching and download function used by Jeremy since it’s so useful.\n\n# This is a function from the notebook:\ndef search_images(term, max_images=200):\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\ndest = Path('..', '__data', 'example-ferret.png')\ndownload_url(urls[0], dest, show_progress=False)\n\nim = Image.open(dest)\nim.to_thumb(420)\n\n\n\n\nWhat a cute litte floofer! Now we’ll collect our data for training - and remove the failed images.\n\nsearches = 'ferret','dragon'\npath = Path('..', '__data', 'ferret_or_dragon')\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server and blocking responses\n    resize_images(path/o, max_size=400, dest=path/o)\n\n/usr/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n\n\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n5\n\n\nI still need to write up a post about these Data Block objects since they’re a good idea and might be able to generalize them beyond just Machine Learning.\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=71),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path)\n\ndls.show_batch(max_n=12)\n\n\n\n\nAnd, now the training!\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(7)\n\n/home/ranuse/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/home/ranuse/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.740871\n      0.012448\n      0.000000\n      00:04\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.084491\n      0.001599\n      0.000000\n      00:02\n    \n    \n      1\n      0.049812\n      0.000780\n      0.000000\n      00:02\n    \n    \n      2\n      0.032287\n      0.000196\n      0.000000\n      00:02\n    \n    \n      3\n      0.023381\n      0.000115\n      0.000000\n      00:02\n    \n    \n      4\n      0.018074\n      0.000076\n      0.000000\n      00:02\n    \n    \n      5\n      0.014552\n      0.000082\n      0.000000\n      00:02\n    \n    \n      6\n      0.012025\n      0.000100\n      0.000000\n      00:02\n    \n  \n\n\n\nThe model doing this well isn’t surprising since the categories we’ve picked are intentionally distinct. After all, the point of this post is to show how simple this can be.\nSo, now we check to see if it is a Floofer.\n\nis_ferret,_,probs = learn.predict(PILImage.create(Path('..', '__data', 'example-ferret.png')))\nprint(f\"This is a: {is_ferret}.\")\nprint(f\"Probability it's a Floofer: {probs[1]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: ferret.\nProbability it's a Floofer: 1.0000\n\n\nYou may have caught there is a single difference in the code here. I changed the probs[0] to probs[1] to pull the probability. I’m not sure how or why it’s returning like this but the probability tensor is reversed even though the category is correct. If I do the Dragon category as a example:\n\nlearn.predict(PILImage.create(Path('..', '__data', 'example-dragon.png')))\n\n\n\n\n\n\n\n\n('dragon', TensorBase(0), TensorBase([1.0000e+00, 1.3599e-08]))\n\n\n… then we see that the category is correct and the probability tensor for Dragon is now in index 0 instead of ferret. We can check the this under learn.dls.vocab and the Dragon category is in fact in index 0:\n\nlearn.dls.vocab\n\n['dragon', 'ferret']\n\n\nI would not of expected to need to check this for such a simple model but from now on I’ll need to keep in mind the order. So, let’s wright some code quick so we don’t have to think about this again. We’ll attach these together using a dictionary.\n\n_,_,probs = learn.predict(PILImage.create(Path('..', '__data', 'example-dragon.png')))\ndict(zip(learn.dls.vocab, probs))\n\n\n\n\n\n\n\n\n{'dragon': TensorBase(1.), 'ferret': TensorBase(1.3599e-08)}\n\n\nUsing this now, we can simply ask for the probability based on the category we care about. Moving this a step forward, we can turn this into a function now for future use:\n\ndef paired_categories(learner, impath):\n    _,_,probs = learner.predict(PILImage.create(impath))\n    return dict(zip(learn.dls.vocab, probs))\n\n\npCats = paired_categories(learn, Path('..', '__data', 'example-dragon.png'))\npCats\n\n\n\n\n\n\n\n\n{'dragon': TensorBase(1.), 'ferret': TensorBase(1.3599e-08)}\n\n\n\npCats['dragon']\n\nTensorBase(1.)"
  },
  {
    "objectID": "posts/nbks/2022-09-27-cycle-weapons-best-general.html",
    "href": "posts/nbks/2022-09-27-cycle-weapons-best-general.html",
    "title": "Some Data Exploration of The Cycle - Frontier Weapons",
    "section": "",
    "text": "Recently, I’ve been playing as much The Cycle: Frontier as I can reasonably fit into my days along with getting work and projects done. If you’re not familiar, it’s a First Person Shooter game which focuses around dropping you to a planet in a semi-persistent world with loot and other players. The Station has some Corporations which hand out jobs as a pretty thin attempt to get you down to the planet. While down there, other players - who are not on your team - can decide how they want to deal with you: talk to you, lie to you, kill you, help you. I’ve heard these games be called both Looter Shooters as well as Evac Shooters and I’m admittidly not the biggest fan of these names.\nAs you build reputation with the different Corps on the Station you can unlock the ability to purchase weapons that each specializes in. Some of them are pretty fun and others are kind of terrible. Today we’re going to do part of the process which was inspired by this article by Robert Ritz. In it, he goes over how to setup an automated Data Pipeline using Kaggle and Deepnote together. This part is going to be simply getting the data downloaded, cleaned and some observations about the guns in this game. I still need to do some more investigation about Deepnote - namely the price, utility and such before actually commiting to that part; I should be able to simply cut that part out and do the download/upload to Kaggle from one of my own servers but if it works then I’m going to use it.\n\n\nTo start with, if you’re following along, scraping data from the Cycle’s Wiki page is annoying. There are tables inserted inside the tables which caused quite a problem while trying to simply pull the data from the website. So, if you’re going to us this as the basis for your own tools then beware that you’ll be certain to need to do some custom work. We’ll start with the normal imports for a project like this.\n\nimport pandas as pd             # for the data.\nimport numpy as np              # for a NaN type\nimport matplotlib.pyplot as plt # For plotting, and some customization of plots.\nimport seaborn as sns           # For pretty plots.\n\n# Fix the size of the graphs\nsns.set(rc={\"figure.figsize\":(11, 8)})\n\nThe website we’re going to be using for the data is their official wiki page - which can be found here. We’ll be pulling from the weapons page which luckily contains a table of all the guns without having to join them. Pandas allows you to read html off a website and will attempt to pull any tables it finds on the webpage. Sadly, due to the nested tables and the way the tables are tagged this simply doesn’t work here. But, you can ask pd.read_html() to look for an attribute and then pull the data from the page using that; it will still need to end up as a table though otherwise pandas will reject it. After doing quite a bit of exploration, I found that you can pull the total table with the attribute zebra as that is the only table which uses it.\n\n\nurl = \"https://thecyclefrontier.wiki/wiki/Weapons\"\nsite = pd.read_html(url, attrs={\"class\":\"zebra\"})[0]\n\n\nsite.head()\n\n\n\n\n\n  \n    \n      \n      Image\n      Name\n      Type\n      Ammo\n      Faction\n      Buy Price\n      Sell Value\n      Rarity\n      Weight\n      Crit Multi\n      Damage\n      Pen\n      Mag Size\n      Refire Rate\n      RPM\n      Reload Time\n      Move Speed\n      Proj. Speed\n    \n  \n  \n    \n      0\n      NaN\n      Advocate\n      AR\n      Medium\n      ICA\n      76000  K-Marks\n      22781  K-Marks\n      Epic\n      35.0\n      1.7\n      11.0\n      26.0\n      24.0\n      0.105\n      571.43\n      3.2\n      0.9\n      29000\n    \n    \n      1\n      76000.0\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      22781.0\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      AR-55 Autorifle\n      AR\n      Medium\n      Station\n      1700  K-Marks\n      524  K-Marks\n      Common\n      35.0\n      1.7\n      12.0\n      10.0\n      22.0\n      0.110\n      545.45\n      2.7\n      0.9\n      28000\n    \n    \n      4\n      1700.0\n      K-Marks\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nYou can see above that this works but the formatting is still a little messed up. Looking at the tables though, this will be an easy fix since rows which we don’t need contain lots NaN. If you’ve never seen this before it just means Not A Number and is a special value used by numpy for this. So, let’s clean the data for use. You can check a Series for these values using .isna() and then we’ll pass the opposite indexes to pull those out:\n\ndata = site[~site.Type.isna()]\ndata.head()\n\n\n\n\n\n  \n    \n      \n      Image\n      Name\n      Type\n      Ammo\n      Faction\n      Buy Price\n      Sell Value\n      Rarity\n      Weight\n      Crit Multi\n      Damage\n      Pen\n      Mag Size\n      Refire Rate\n      RPM\n      Reload Time\n      Move Speed\n      Proj. Speed\n    \n  \n  \n    \n      0\n      NaN\n      Advocate\n      AR\n      Medium\n      ICA\n      76000  K-Marks\n      22781  K-Marks\n      Epic\n      35.0\n      1.7\n      11.0\n      26.0\n      24.0\n      0.105\n      571.430\n      3.20\n      0.9\n      29000\n    \n    \n      3\n      NaN\n      AR-55 Autorifle\n      AR\n      Medium\n      Station\n      1700  K-Marks\n      524  K-Marks\n      Common\n      35.0\n      1.7\n      12.0\n      10.0\n      22.0\n      0.110\n      545.450\n      2.70\n      0.9\n      28000\n    \n    \n      6\n      NaN\n      Asp Flechette Gun\n      SMG\n      Light\n      Osiris\n      54000  K-Marks\n      16131  K-Marks\n      Epic\n      30.0\n      1.5\n      9.0\n      26.0\n      20.0\n      0.095\n      631.580\n      2.50\n      1.0\n      24000\n    \n    \n      9\n      NaN\n      B9 Trenchgun\n      Shotgun\n      Shotgun\n      Station\n      1200  K-Marks\n      371  K-Marks\n      Common\n      25.0\n      1.2\n      10.0\n      10.0\n      5.0\n      0.950\n      63.158\n      2.40\n      1.0\n      26000\n    \n    \n      12\n      NaN\n      Basilisk\n      DMR\n      Heavy\n      Osiris\n      275000  K-Marks\n      82448  K-Marks\n      Exotic\n      50.0\n      1.6\n      34.0\n      28.0\n      8.0\n      0.500\n      120.000\n      3.85\n      0.8\n      45000\n    \n  \n\n\n\n\nLooking much better now. Let’s quickly check the data’s types and make sure they make sense still.\n\ndata.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 26 entries, 0 to 75\nData columns (total 18 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Image        0 non-null      float64\n 1   Name         26 non-null     object \n 2   Type         26 non-null     object \n 3   Ammo         26 non-null     object \n 4   Faction      26 non-null     object \n 5   Buy Price    26 non-null     object \n 6   Sell Value   26 non-null     object \n 7   Rarity       26 non-null     object \n 8   Weight       26 non-null     float64\n 9   Crit Multi   26 non-null     float64\n 10  Damage       26 non-null     float64\n 11  Pen          26 non-null     float64\n 12  Mag Size     26 non-null     float64\n 13  Refire Rate  26 non-null     float64\n 14  RPM          26 non-null     float64\n 15  Reload Time  26 non-null     float64\n 16  Move Speed   26 non-null     float64\n 17  Proj. Speed  26 non-null     object \ndtypes: float64(10), object(8)\nmemory usage: 3.9+ KB\n\n\nSomething is wrong with the Proj. Speed at this point since it shouldn’t be an object but instead should be a number. Checking the values we find that there are string values in here.\n\n# There is a hitscan in there; how should we deal with that?\ndata['Proj. Speed'].unique()\n\narray(['29000', '28000', '24000', '26000', '45000', '50000', 'Hitscan',\n       '30000', '60000', '40000', '70000', '4000', '35000', '34000'],\n      dtype=object)\n\n\nThe value of Hitscan is preventing the conversion to numbers. I didn’t realize any of the guns in this game were hitscan at all. Which weapons are these?\n\ndata[ data['Proj. Speed'] == 'Hitscan' ][['Name', 'Proj. Speed']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Proj. Speed\n    \n  \n  \n    \n      21\n      Gorgon\n      Hitscan\n    \n    \n      75\n      Zeus Beam\n      Hitscan\n    \n  \n\n\n\n\nOk, so we’ll need to replace this with something that wont hurt our analysis so we’re also going to change these to np.NaN.\n\n# Fix hitscan info:\nindx = data['Proj. Speed'] == 'Hitscan'\ndata.loc[indx, 'Proj. Speed'] = np.NaN\n\nThere are some other columns - Sell Value, Buy Price as examples - which have come in as object so they’re being treated as strings. We need these to be numbers if we end up using them. And, after fixing the Hitscan problem we’ll need to convert that column to numbers.\n\n\ndata = data.assign(\n    Sell = data['Sell Value'].str.replace(' K-Marks', '').astype('float'),\n    Buy = data['Buy Price'].str.replace(' K-Marks', '').astype('float'),\n    DPS = data['Refire Rate'] * data['Damage'],\n    Faction = data['Faction'].astype('category'),\n    Velocity = data['Proj. Speed'].astype('float')\n)\ndata = data.assign(\n    perWeight = data['Sell'] / data['Weight']\n)\n\n# # This removes the legendary weapons\n# data = data.query('Faction != \"Printing\"')\n\ndata = data.drop(labels = ['Sell Value', 'Buy Price', 'Image', 'Proj. Speed'],axis = 1)\n\nNow we’ve got data to work with!\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Type\n      Ammo\n      Faction\n      Rarity\n      Weight\n      Crit Multi\n      Damage\n      Pen\n      Mag Size\n      Refire Rate\n      RPM\n      Reload Time\n      Move Speed\n      Sell\n      Buy\n      DPS\n      Velocity\n      perWeight\n    \n  \n  \n    \n      0\n      Advocate\n      AR\n      Medium\n      ICA\n      Epic\n      35.0\n      1.7\n      11.0\n      26.0\n      24.0\n      0.105\n      571.430\n      3.20\n      0.9\n      22781.0\n      76000.0\n      1.155\n      29000.0\n      650.885714\n    \n    \n      3\n      AR-55 Autorifle\n      AR\n      Medium\n      Station\n      Common\n      35.0\n      1.7\n      12.0\n      10.0\n      22.0\n      0.110\n      545.450\n      2.70\n      0.9\n      524.0\n      1700.0\n      1.320\n      28000.0\n      14.971429\n    \n    \n      6\n      Asp Flechette Gun\n      SMG\n      Light\n      Osiris\n      Epic\n      30.0\n      1.5\n      9.0\n      26.0\n      20.0\n      0.095\n      631.580\n      2.50\n      1.0\n      16131.0\n      54000.0\n      0.855\n      24000.0\n      537.700000\n    \n    \n      9\n      B9 Trenchgun\n      Shotgun\n      Shotgun\n      Station\n      Common\n      25.0\n      1.2\n      10.0\n      10.0\n      5.0\n      0.950\n      63.158\n      2.40\n      1.0\n      371.0\n      1200.0\n      9.500\n      26000.0\n      14.840000\n    \n    \n      12\n      Basilisk\n      DMR\n      Heavy\n      Osiris\n      Exotic\n      50.0\n      1.6\n      34.0\n      28.0\n      8.0\n      0.500\n      120.000\n      3.85\n      0.8\n      82448.0\n      275000.0\n      17.000\n      45000.0\n      1648.960000\n    \n  \n\n\n\n\n\n\n\nA word here about some extra cleaning which I’ve elected to do. Looking at the data, there are two more problems that should be brought up here. The first is that I’m taking Snipers out of the analysis. The reason for this is that there really are only two of them and everyone understands why they’re as powerful as they are.\n\nsns.scatterplot(x = data.Damage, y = data['Pen'], hue = data.Type)\nplt.title(\"The Reason Snipers Rule\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'The Reason Snipers Rule')\n\n\n\n\n\nThe second adjustment is that I’m pulling the ICA Garuntee out of the analysis because it’s the only one of its kind.\n\ndata.query('Type == \"LMG\"')[['Name', 'Type']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Type\n    \n  \n  \n    \n      27\n      ICA Guarantee\n      LMG\n    \n  \n\n\n\n\nAnd, the same for the Komrad for the same reason.\n\ndata.query('Type == \"Launcher\"')[['Name', 'Type']]\n\n\n\n\n\n  \n    \n      \n      Name\n      Type\n    \n  \n  \n    \n      45\n      KOMRAD\n      Launcher\n    \n  \n\n\n\n\nSo, let’s start with what we all care about the most: Damage.\n\ndata[['Type', 'Damage']].groupby('Type').mean().sort_values('Damage', ascending=False).T\n\n\n\n\n\n  \n    \n      Type\n      DMR\n      Pistol\n      AR\n      SMG\n      Shotgun\n    \n  \n  \n    \n      Damage\n      32.0\n      17.666667\n      11.0\n      9.25\n      9.25\n    \n  \n\n\n\n\nSo, DMR’s do about twice as much damage as the next category down. From this, we’d expect DMRs to be used quite a bit; what weapons are in this category?\n\ndata.query('Type == \"DMR\"')\n\n\n\n\n\n  \n    \n      \n      Name\n      Type\n      Ammo\n      Faction\n      Rarity\n      Weight\n      Crit Multi\n      Damage\n      Pen\n      Mag Size\n      Refire Rate\n      RPM\n      Reload Time\n      Move Speed\n      Sell\n      Buy\n      DPS\n      Velocity\n      perWeight\n    \n  \n  \n    \n      12\n      Basilisk\n      DMR\n      Heavy\n      Osiris\n      Exotic\n      50.0\n      1.6\n      34.0\n      28.0\n      8.0\n      0.5\n      120.0\n      3.85\n      0.8\n      82448.0\n      275000.0\n      17.0\n      45000.0\n      1648.96\n    \n    \n      36\n      KBR Longshot\n      DMR\n      Heavy\n      Korolev\n      Epic\n      50.0\n      1.5\n      35.0\n      26.0\n      12.0\n      0.6\n      100.0\n      3.55\n      0.9\n      29776.0\n      99000.0\n      21.0\n      40000.0\n      595.52\n    \n    \n      51\n      Lacerator\n      DMR\n      Heavy\n      ICA\n      Rare\n      50.0\n      1.5\n      27.0\n      23.0\n      16.0\n      0.4\n      150.0\n      2.55\n      0.9\n      12203.0\n      41000.0\n      10.8\n      35000.0\n      244.06\n    \n  \n\n\n\n\nAll these weapons get used in my experience - and from watching others play the game. Although, the lowest tier gun in here is Rare so that probably helps a lot. Note that the Rarity of a gun informs the Pen for the Gun and therefore adds more damage when being fired. So, the higher the tier of Rarity therefore the more damage the gun can do per hit - and they have higher damage counts as well.\nConsidering this, these guns have a limit placed on their Rate of Fire.\n\ndata[['Type', 'Refire Rate']].groupby('Type').mean().sort_values('Refire Rate', ascending=False).T\n\n\n\n\n\n  \n    \n      Type\n      Shotgun\n      DMR\n      Pistol\n      AR\n      SMG\n    \n  \n  \n    \n      Refire Rate\n      0.77625\n      0.5\n      0.288333\n      0.1475\n      0.08125\n    \n  \n\n\n\n\nThis is a match in the order of the columns - ignoring the Shotgun Category. Shotguns have a low damage (per pellet), and a high re-fire rate. That’s obviously because the damage per pellet masks how lethal Shotguns are: See any Shattergun Montage basically.\nSo, it looks like either this was coincidence or they’re intentionally using this to offset damage.\n\nsns.scatterplot(x = data.Damage, y = data['Refire Rate'], hue = data.Type)\nplt.title(\"Damage With Refire Rate\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Damage With Refire Rate')\n\n\n\n\n\nIf we check the relationship, the Damage also quite high against the constraint on the Refire Rate. These guns appear really strong in comparison to everything else - setting aside Snipers of course. But is it really? Let’s Normalize the Damage column and see if it really is that far out. If you’re not familiar with Normalization then this is a common process in Machine Learning where the values of a column are scaled based on the minimum and maximum values.\n\ndef normalize(column): return ( column - column.min()) / (column.max() - column.min())\nround(normalize(data['Refire Rate']).mean(), 2)\n\n# Lets save this and re-plot:\ndata['Norm Refire'] = round(normalize(data['Refire Rate']), 2)\ndata['Norm Damage'] = round(normalize(data['Damage']), 2)\n\n\n# Re-plot:\nsns.scatterplot(x = data['Norm Damage'], y = data['Norm Refire'], hue = data.Type)\nplt.title(\"Normalized Damage With Normalized Refire Rate\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Normalized Damage With Normalized Refire Rate')\n\n\n\n\n\nNormalizing doesn’t look to have changed anyting aside from the scale so that still looks really solid. And, If we check the Critical Multiplier - which is a stand in for HeadShots since I believe the Groin also counts - the reward is about average for accuracy.\n\nsns.scatterplot(x = data.Damage, y = data['Crit Multi'], hue = data.Type)\nplt.title(\"Damage With Critical Multiplier\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Damage With Critical Multiplier')"
  },
  {
    "objectID": "posts/nbks/2022-10-05-cycle-loot-values-download.html",
    "href": "posts/nbks/2022-10-05-cycle-loot-values-download.html",
    "title": "Downloading and Exploring Loot in Cycle Frontier",
    "section": "",
    "text": "First, we’ll pull our normal libraries for working on projects like this.\nOur data source is still going to be the Official Wiki and we’ll be pulling from the Loot Page - which has already started being updated for Season Two. From my previous post, the class zebra is still the best attribute to target to pull the data into a usable Data Frame.\nAgain, from previous experience, it is best to create a copy of columns that we actually need.\nLooking at our data, we already have a problem: the Printing column has columns NaN values insted of no. The blank values got converted to NaN when it was imported. While I’m sure there is a way to correct this on import, this isn’t how I’m going to do it. What I’m going to do is find the indexes which contain Yes and then update the other columns. You might be thinking - correctly - that we could simply find the indexes of the other values and then update those. Unfortunately, this is harder and I’ll show you what I mean.\nIf we check the values, we’ll see there is this nan value which doens’t look like the normal np.NaN value.\nAnd, if we try to do a logical comparison with the literal value from the data it fails.\nHere we so the problem with objects and probably the pointers underneath. We could fix this but it’s much easier to reverse what we’re detecting so I’m doing that instead.\nI’m not going to spend much time explaining this part; please see the previous posts about how this works.\nLooking at the rows, we can see the data we’re after - the Name of the Loot - is in the Name column. Luckily, like the previous post about data extraction, these are in multiples of 5 insted of 4 like before. So, we can simply borrow the same code from before and update the range.\nNow we’ll build the loop for the iteration like before.\nNow we need a column to dump the correct values into which is going to be called Loot. We’ll fill these with np.NaN so we can repeat the same trick as the previous post.\nSo, pull the name of of the loot per subset and then update it like before.\nIf we play through the old trick of dropping the Loot columns now we’ll lose valuable data. So, we’ll need a new trick to keep that data. Thankfully, this is a problem I’d already solved at a previous time: we’ll simply fill the values. As a word of caution, filling values can be dangerous to an analysis so if you’re doing this then make sure it will not have a negative impact. In this instance, I’m duplicating the data so that it’s not lost at all. Even then, we’ll need to be cautious in the future about how the data is used.\nThere are a few techniques for filling in missing values and one such technique is called Fill Forward. What this does is take the values in a column, take that value and then simply inserts it down the rows where it finds NAs. This is exactly what we’re after - but only for those specific columns.\nThen, we’ll take these values and simply insert them into the real data frame where we want them.\nAnd, now we’ll pull the same trick and delete the rows with np.NaN.\nAlmost there! The column name Image is not what that should be called so we’ll update that to Unit like we had in the prevous post.\nJust one more problem I’d like to correct before we do some fun questions at the end. Pandas has also copied the idea of Categoreies from R. And, it allows us to set the order of the values as well. We’re going to use this in a minute but it’s also a nice to have.\nChanging the type is just as easy as you’d think:\nYou can see that now there is a list of the possible Categories below the values: Categories (6, object): ['Common', 'Epic', 'Exotic', 'Legendary', 'Rare', 'Uncommon']. However, the order matters and so we cannot leave this how it is; Uncommon is clealy not larger in type then Epic. The proper way to do this would be to declare the order - which is what is next. We’ll use pd.Categorical() to convert them and set the order of the Categories.\nThere we go! See that the order has now been fixed: Categories (6, object): ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']. Here I would like to note that I did take liberty to set Legendary above Exotic since the wiki as them mixed and the actual games sorting puts Legendary above Exotic and so I did the same."
  },
  {
    "objectID": "posts/nbks/2022-10-05-cycle-loot-values-download.html#what-are-the-best-two-items-to-sell-per-rarity",
    "href": "posts/nbks/2022-10-05-cycle-loot-values-download.html#what-are-the-best-two-items-to-sell-per-rarity",
    "title": "Downloading and Exploring Loot in Cycle Frontier",
    "section": "What Are The Best Two Items To Sell, Per Rarity?",
    "text": "What Are The Best Two Items To Sell, Per Rarity?\nThis is a pretty simple ask. We just have to remember that we’re working with Tidy Data and we’ll want to filter the Name column for K-Marks before doing anything else. Next, we’ll do a groupby() for the Rarity and then pull out the Units column since that has the values we’re after. Luckily, pandas already has a function to get the largest values .largest() which also works as an aggregation function for groupby().\n\ntmp = lootData.query(\"Name == 'K-Marks'\").groupby(\"Rarity\")['Unit'].nlargest(2)\ntmp\n\nRarity        \nCommon     336       900.0\n           111       760.0\nUncommon   356      3417.0\n           181      1709.0\nRare       371     11533.0\n           366      5126.0\nEpic       121     20183.0\n           396     17300.0\nExotic     401    129746.0\n           106     77848.0\nLegendary  441    518985.0\n           431    116772.0\nName: Unit, dtype: float64\n\n\nSo, we have our values but we don’t have any way to identify them since we cannot include the Name column since then it becomes a Data Frame and .nlargest() doesn’t work. If we look closely, the old indexes have been carried over and are included in our results so we’ll need to give ourselves access to them. We’re going to do this with .reset_index() since that will push those indexes into the Data Frame.\n\ntmp.reset_index()\n\n\n\n\n\n  \n    \n      \n      Rarity\n      level_1\n      Unit\n    \n  \n  \n    \n      0\n      Common\n      336\n      900.0\n    \n    \n      1\n      Common\n      111\n      760.0\n    \n    \n      2\n      Uncommon\n      356\n      3417.0\n    \n    \n      3\n      Uncommon\n      181\n      1709.0\n    \n    \n      4\n      Rare\n      371\n      11533.0\n    \n    \n      5\n      Rare\n      366\n      5126.0\n    \n    \n      6\n      Epic\n      121\n      20183.0\n    \n    \n      7\n      Epic\n      396\n      17300.0\n    \n    \n      8\n      Exotic\n      401\n      129746.0\n    \n    \n      9\n      Exotic\n      106\n      77848.0\n    \n    \n      10\n      Legendary\n      441\n      518985.0\n    \n    \n      11\n      Legendary\n      431\n      116772.0\n    \n  \n\n\n\n\n\n# Keep those index numbers\ntmp = tmp.reset_index()\n\nThe index column is called level_1 which we’re going to simply insert into the original data.\n\n# Best items to sell per rarity:\nlootData.loc[ tmp['level_1'].tolist() ].sort_values(\"Rarity\", ascending=False)[['Unit', 'Name', 'Rarity', 'Loot']]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Loot\n    \n  \n  \n    \n      441\n      518985.0\n      K-Marks\n      Legendary\n      Alpha Crusher Head\n    \n    \n      431\n      116772.0\n      K-Marks\n      Legendary\n      Savage Marauder Head\n    \n    \n      401\n      129746.0\n      K-Marks\n      Exotic\n      Alpha Crusher Heart\n    \n    \n      106\n      77848.0\n      K-Marks\n      Exotic\n      Progenitor Slag\n    \n    \n      121\n      20183.0\n      K-Marks\n      Epic\n      NiC Oil Cannister\n    \n    \n      396\n      17300.0\n      K-Marks\n      Epic\n      Crusher Flesh\n    \n    \n      371\n      11533.0\n      K-Marks\n      Rare\n      Crusher Hide\n    \n    \n      366\n      5126.0\n      K-Marks\n      Rare\n      Mature Rattler Eyes\n    \n    \n      356\n      3417.0\n      K-Marks\n      Uncommon\n      Hardened Bone Plates\n    \n    \n      181\n      1709.0\n      K-Marks\n      Uncommon\n      Derelict Explosives\n    \n    \n      336\n      900.0\n      K-Marks\n      Common\n      Toxic Glands\n    \n    \n      111\n      760.0\n      K-Marks\n      Common\n      Progenitor Composite\n    \n  \n\n\n\n\nThere we go! Well, except that this doesn’t account for weight and what we really want is to carry the most value per weight since we’re limited in the game by the backpack size. Let’s do the same but for the per weight value instead.\n\ntmp = lootData.query(\"Name == 'K-Marks / Weight'\").groupby(\"Rarity\")['Unit'].nlargest(2).reset_index()\nlootData.loc[ tmp['level_1'].tolist() ].sort_values(\"Rarity\", ascending=False)[['Unit', 'Name', 'Rarity', 'Loot']]\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Loot\n    \n  \n  \n    \n      443\n      17300.0\n      K-Marks / Weight\n      Legendary\n      Alpha Crusher Head\n    \n    \n      433\n      5839.0\n      K-Marks / Weight\n      Legendary\n      Savage Marauder Head\n    \n    \n      108\n      6487.0\n      K-Marks / Weight\n      Exotic\n      Progenitor Slag\n    \n    \n      403\n      6487.0\n      K-Marks / Weight\n      Exotic\n      Alpha Crusher Heart\n    \n    \n      123\n      4037.0\n      K-Marks / Weight\n      Epic\n      NiC Oil Cannister\n    \n    \n      118\n      3844.0\n      K-Marks / Weight\n      Epic\n      Letium Clot\n    \n    \n      373\n      1153.0\n      K-Marks / Weight\n      Rare\n      Crusher Hide\n    \n    \n      368\n      1025.0\n      K-Marks / Weight\n      Rare\n      Mature Rattler Eyes\n    \n    \n      458\n      570.0\n      K-Marks / Weight\n      Uncommon\n      Print Resin\n    \n    \n      358\n      380.0\n      K-Marks / Weight\n      Uncommon\n      Hardened Bone Plates\n    \n    \n      148\n      1000.0\n      K-Marks / Weight\n      Common\n      Old Currency\n    \n    \n      283\n      506.0\n      K-Marks / Weight\n      Common\n      Nutritional Bar\n    \n  \n\n\n\n\nAnd, now we see there are some important changes. The values in the Common Rarity are totally different!"
  },
  {
    "objectID": "posts/nbks/2022-10-05-cycle-loot-values-download.html#are-the-rarity-values-linear",
    "href": "posts/nbks/2022-10-05-cycle-loot-values-download.html#are-the-rarity-values-linear",
    "title": "Downloading and Exploring Loot in Cycle Frontier",
    "section": "Are the Rarity Values Linear?",
    "text": "Are the Rarity Values Linear?\nWe’ll aggregate over all the values in a category and then simply plot them.\n\nlootData.query(\"Name == 'K-Marks'\").groupby(\"Rarity\")['Unit'].mean().plot()\nplt.title(\"Mean K-Marks Per Rarity\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Mean K-Marks Per Rarity')\n\n\n\n\n\nWait, what? This looks terrible! Everything from Common to Epic is worth nothing compared to Exotic and Legendary. Maybe we’re dealing with some outlier problems? Let’s check the median just in case.\n\nlootData.query(\"Name == 'K-Marks'\").groupby(\"Rarity\")['Unit'].median().plot()\nplt.title(\"Median K-Marks Per Rarity\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Median K-Marks Per Rarity')\n\n\n\n\n\nThis is even worse! Legendary loot is definitely being affected by outliers but Exotic is so far and above better than everything else. Maybe we’re looking at the wrong values? Maybe we’re making the same mistake and we need to account for the per weight?\n\nlootData.query(\"Name == 'K-Marks / Weight'\").groupby(\"Rarity\")['Unit'].median().plot()\nplt.title(\"Median K-Marks/Weight Per Rarity\", size=15, fontweight='bold')\n\nText(0.5, 1.0, 'Median K-Marks/Weight Per Rarity')\n\n\n\n\n\nI mean it’s better but not really. What is in this category?\n\nlootData.query(\"Rarity == 'Exotic'\").query(\"Name == 'K-Marks'\")\n\n\n\n\n\n  \n    \n      \n      Unit\n      Name\n      Rarity\n      Personal Quarters\n      Campaigns\n      Jobs\n      Printing\n      Loot\n    \n  \n  \n    \n      91\n      8650.0\n      K-Marks\n      Exotic\n      Yes x4\n      Yes x9\n      Yes\n      No\n      Charged Tharis Iron Ingot\n    \n    \n      106\n      77848.0\n      K-Marks\n      Exotic\n      Yes x6\n      Yes x9\n      Yes\n      No\n      Progenitor Slag\n    \n    \n      401\n      129746.0\n      K-Marks\n      Exotic\n      Yes x6\n      Yes x1\n      Yes\n      No\n      Alpha Crusher Heart\n    \n  \n\n\n\n\nThis category makes no sense and I don’t know why it exists. I’m not sure they understand what this category is for either looking at what is in here. But, that will be it for now."
  },
  {
    "objectID": "posts/nbks/2022-10-14-cycle-calculate-job-values.html",
    "href": "posts/nbks/2022-10-14-cycle-calculate-job-values.html",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Three.",
    "section": "",
    "text": "This is the third post in a series about collecting, cleaning and analyzing which Jobs are worth doing in The Cycle: Frontier game. If you’ve found this post before the others I would recommend the other posts first to catch up: Post One and Post Two. We’ll pick up where we left off having separate datasets for the faction rewards as well as all the loot. Let’s get our imports!\nThere are two tasks we’ll need to adjust before we continue. The first is that our Tasks work was only for one faction so we’ll need to loop through each faction’s tasks and then append them all together into a single dataset; the code is included simply for observation.\nThe second task is that we need a single dataset for all the job rewards. We’ll definitely want to tag each so we don’t lose track of which task belongs to which Faction.\nFor this analysis, we’re really only interested in the Kmarks so we’ll need to only pull those rows; we’ll do some work with the others later but for now just the money.\nJoins are a complicated topic which I’m not going to flesh out here. In this instance, what we want is the loot table with the Kmark value connected to the jobs table with respect to the name of the loot. And, we’ll tell Pandas to join those together below making sure that as long as it exists in the left table that it gets connected to something in the right table.\nWe’ll now multiply the count of the loot times their values to get the cost per resource in the task.\nNow we’ll just group by the name of the task to and take the sume of each to get the total cost per task.\nThis is the first piece we’ll need to get the results we’re after; the other part is all the jobs we collected in the previous post - and brought here.\nNow we have the rewards and the cost per task we can finally calculate the Balance of each job!"
  },
  {
    "objectID": "posts/nbks/2022-10-14-cycle-calculate-job-values.html#conclusions---and-questions",
    "href": "posts/nbks/2022-10-14-cycle-calculate-job-values.html#conclusions---and-questions",
    "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Three.",
    "section": "Conclusions - and Questions!",
    "text": "Conclusions - and Questions!\nSo, now that we have the data let’s work on some questions!\nGiven any particular faction, which task has the highest Balance?\n\nbalanceMax = results.Balance.max()\nresults.loc[results.Balance == balanceMax]\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Faction\n      Cost\n      Balance\n    \n  \n  \n    \n      91\n      227000\n      K-Marks\n      Striking Big\n      ICA\n      0.0\n      227000.0\n    \n  \n\n\n\n\nLooks like it is the Job Striking Big. What’s this job about?: > Damn it! We ran out of Fuel for our Radiation Shields here on the Station. And there’s no replacement for Nanite Infused Crude Oil. You know what to do.\nIt’s collecting Oil Cannisters! Those are worth a lot of money so it’s not a surprise that a task which wants you to collect them would also pay out so highly.\nWhat about the job that pays the least?\n\nbalanceMin = results.Balance.min()\nresults.loc[results.Balance == balanceMin]\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Faction\n      Cost\n      Balance\n    \n  \n  \n    \n      94\n      2200\n      K-Marks\n      Lab equipment\n      Osiris\n      676.0\n      1524.0\n    \n  \n\n\n\n\nWell, it our good friends Osiris failing the station again. That looks like a low level job from a cost and reward that low. Not really much of a surprise here.\nSo, what about the mean balance - and how many tasks are there which reward more than that average value?\n\nbalanceMean = results.Balance.mean()\nf\"${round(balanceMean, 2)}\"\n\n'$20600.89'\n\n\n\nlen( results.loc[results.Balance >= balanceMean])\n\n40\n\n\n\n# How many are there per faction?\nresults.loc[\n    results.Balance >= balanceMean ]\\\n        .groupby('Faction')\\\n        .count()\\\n        .reset_index()\\\n        .rename({'Units':'Count'}, axis=1)[[\n    'Faction', 'Count'\n]]\n\n\n\n\n\n  \n    \n      \n      Faction\n      Count\n    \n  \n  \n    \n      0\n      ICA\n      14\n    \n    \n      1\n      Korolev\n      14\n    \n    \n      2\n      Osiris\n      12\n    \n  \n\n\n\n\nAnd, unsurpisingly Osiris is slightly behind but they’re mostly the same.\nOk, so what’s the cost of the Job that inspired all this work: And Two Smoking Barrels?\n\nresults.loc[results.Job.str.contains(\"Barrel\")]\n\n\n\n\n\n  \n    \n      \n      Units\n      Rewards\n      Job\n      Faction\n      Cost\n      Balance\n    \n  \n  \n    \n      39\n      19000\n      K-Marks\n      And two smoking Barrels\n      Korolev\n      0.0\n      19000.0\n    \n  \n\n\n\n\nWait a minute! There should be a cost here and it is missing! Well, that’s because there are no guns in the loot table. So, there is no cost when we connect the data together. And, after some further checking there are some missing values here. That quest we checked above with the Oil? They’re not named the same betwee the tables: NiC Oil vs NiC Oil Cannister. It’s not the only one like this either.\nLooks like we’ve got more work to do."
  },
  {
    "objectID": "posts/Mds/2020-07-09-Voices-of-the-Void.html",
    "href": "posts/Mds/2020-07-09-Voices-of-the-Void.html",
    "title": "Book Review: Voices of the Void",
    "section": "",
    "text": "The Voices of the Void by David Stewart is a pleasant short story that mixes a Science Fiction backdrop with some supernatural horror. We join the introduced protagonist - Andrew Dalatent - entering a mining colony called New Gibralter where he appears tasked with figuring out what happened to the people there. And, I say it appears because as he starts his descent you find out he’s more aware of the whys and whats of the situation than is initially let on.\nCoupled with his own supernatural ability to observe future and past versions of events around him, we get to join him as he tries to survive against humanity removed of its own free will. These sections that Stewart uses were my own personal favorite because they’re surprisingly easy to follow but also because they grow slowly out of being just a novel tactic Andrew uses to protect himself into an important aspect of the later story.\nShort read but enjoyable."
  },
  {
    "objectID": "posts/Mds/2021-12-08-Challenges-For-2022.html",
    "href": "posts/Mds/2021-12-08-Challenges-For-2022.html",
    "title": "Challenges for The Year: 2022 Edition",
    "section": "",
    "text": "I’m lowering the number back to 50 books even though 100 books is still the larger goal. The real reason is I need to find a way to fit reading back into my schedule. With so many projects, I will be shifting to audiobooks which is depressing since I read faster than I can process audiobooks. I do have a trick for this: listen to ebooks via the Kindle Store in a Web Browser. This is probably a good enough place to explain how this works.\nInstall an extension for your Web Browswer which does Text-To-Speech - such as Read Aloud. Honestly, finding a quaility Text-to-Speech reader and paying for the extension is a better idea than free; the lack of intonation from the extension I am using makes listening much harder. Concentrating is harder compared to a professionally done book from Audible. Go to Your Online Kindle Library and find a book to listen to. Open the book up and then hit the play button inside your Text-To-Speech extension. You will need to adjust the speed, voice, volume and location when you hit play per new book for tuning. But, this is a good way to fit more average fiction reading into your day.\nPutting this in the background of the first half of my work shift sounds correct. Since I am not usually interacting with people then it should be an easy way to get 2-3 hours of “reading” done each day. Let the challenge begin."
  },
  {
    "objectID": "posts/Mds/2021-12-08-Challenges-For-2022.html#have-three-streams-of-income",
    "href": "posts/Mds/2021-12-08-Challenges-For-2022.html#have-three-streams-of-income",
    "title": "Challenges for The Year: 2022 Edition",
    "section": "Have Three Streams of Income",
    "text": "Have Three Streams of Income\nWe’re moving from planning to application with this one.\n\nPrimary “9-5” Job.\nObviously, my job is the primary source of income - and I have no real interest in changing that. Working Technicaly Support is usually an awful experience since you’re dealing with End Users which have no real understanding of technology nor how their behaviors make little sense. However, where I work I am dealing with people that at least have some background in technology so having conversations with them is usually much less painful than End Users. Since my role is also on Overnights, this significantly limits the interactions with the kinds of people who constantly cause problems for themselves - and therefore me.\n\n\nStreaming\nI’m going to take a serious attempt at making money on Content Creation sites like Youtube, Twitch and TikTok. The competition here is enormous and this is not the kind of job I admit I’m naturally good at but it fits well into my life. I play games with my friends and coworkers consistenly enough that these can be put online without too much trouble. Getting people to watch or care about the content is a totally different problem. Doing the research that I have so far, we’re really going to have to lean on Personality quite a bit otherwise this simply wont work. Due to being on Linux, there really is nothing but Splitgate which I can play competitively and stream. And looking at the numbers, Splitgate is struggling both as a streamed game as well as keeping an audiance of players:  \nSo, maybe I could try and niche with Splitgate but I would be concerned with getting locked into playing a game struggling to grow. To start though, I will need: 1. To become Affiliate on Twitch. 2. Get better at tags on Youtube. 3. To start posting shorts to Tik Tok.\n\n\nWeb Design\nThis is the least fun option but I’m capable of this. I have found a set of frameworks and a niche that I can work in without getting too far dragged into the larger frameworks: Single Page Websites. As I expand my ability to use tools such as Blender, Inkscape then I can push into more artistic and aesthetic styles instead of the more normal Single Page Sites. Not much else to say aside from if this works out then expect a Portfolio coming soon.\n\n\nLogo Design\nBetween slowly expanding Blender skills and working on my drawing skills, this would be something fun that I could doodle designs and then try to sell to others. There is quite a bit about communicating concepts artistically along with a good amount of Color Theory to learn before I can do this."
  },
  {
    "objectID": "posts/Mds/2021-12-08-Challenges-For-2022.html#new-job-title",
    "href": "posts/Mds/2021-12-08-Challenges-For-2022.html#new-job-title",
    "title": "Challenges for The Year: 2022 Edition",
    "section": "New Job Title",
    "text": "New Job Title\nThis year is the real year that matters though for my job; I have a patent pending for the company and this should be year I get the nay/yay for whether it goes through. No matter how that goes, I’m pushing for an Analytics Job either inside or outside the company. I am familiar enough and competent enough to be doing this as a real job and I am well bored of doing Technical Support. It also is not a real future for me; there is no where to go that I want to be. So, if by six months in I don’t see any progress here then I’m going to start applying for part time Analytics jobs to build experience and push into a new career."
  },
  {
    "objectID": "posts/Mds/2021-12-08-Challenges-For-2022.html#blog-posts-for-the-year.",
    "href": "posts/Mds/2021-12-08-Challenges-For-2022.html#blog-posts-for-the-year.",
    "title": "Challenges for The Year: 2022 Edition",
    "section": "36 Blog Posts For the Year.",
    "text": "36 Blog Posts For the Year.\nI have been spending a good amount of time reading Medium posts and articles by others this year. And, my expectations are far too high and totally unnecessary. While in college, I learned about one of the books that significanly changed my expecations about writing: The Elements of Style by Strunk and White. The book emphasis is always on Omit Needless Words and I extended that to all of my writing since then: write only what needs to be said. While a good guide, this is now preventing me from writing at all. If someone else has said it then I am not writing about it; if someone hasn’t written about it then I am not confident in writing about it. I know enough and am learning enough to write content to clarify my thoughts for myself and others. So, three posts per month on anything I feel like: Game Design, Data Science, etc"
  },
  {
    "objectID": "posts/Mds/2021-12-08-Challenges-For-2022.html#make-the-secret-project-public",
    "href": "posts/Mds/2021-12-08-Challenges-For-2022.html#make-the-secret-project-public",
    "title": "Challenges for The Year: 2022 Edition",
    "section": "Make the Secret Project Public",
    "text": "Make the Secret Project Public\nBy the end of the year, there will be public posts about this project. Watch for a reveal in the future."
  },
  {
    "objectID": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html",
    "href": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html",
    "title": "How to Setup V Rising on Linux",
    "section": "",
    "text": "If you’re here becuase you just want the answer then here are the exact steps that always work for me. 1. Download the V Rising Dedicted Server from steam to your account. 2. Download Proton Experimental and change to the Bleeding Edge Beta branch. 3. Run this command:\nSTEAM_COMPAT_CLIENT_INSTALL_PATH=/tmp STEAM_COMPAT_DATA_PATH=/tmp <Path/to/proton/experimental/install/location/proton> run VRisingServer.exe <args-from-offical-repo>\n\nIf this doesn’t work then proceed to the below:\n\n\nDownload Lutris and install the dependecies it asks for.\nFind and install the Epic Games laucher from Lutris.\nDo the steps it asks you to run through: including those dependencies.\nReboot.\nRun the command:\n\nSTEAM_COMPAT_CLIENT_INSTALL_PATH=/tmp STEAM_COMPAT_DATA_PATH=/tmp <Path/to/proton/experimental/install/location/proton> run VRisingServer.exe <args-from-offical-repo>"
  },
  {
    "objectID": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#how-do-we-run-something-manaully-with-proton",
    "href": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#how-do-we-run-something-manaully-with-proton",
    "title": "How to Setup V Rising on Linux",
    "section": "How Do We Run Something Manaully With Proton?",
    "text": "How Do We Run Something Manaully With Proton?\nLuckily, just like the wonder people over at the Github Page, we can find that answer online! According to this old thread from Reddit: > STEAM_COMPAT_DATA_PATH=~/.proton/ ~/.steam/steam/steamapps/common/Proton 3.7/proton run whatever.exe\n> You need to create ~/.proton (it can be any directory and can be empty)\nExcellent! Now we have something to work with. Looking at the command I built, I simply put those in /tmp since I didn’t want to think about what compatdata is; I still don’t really know what this is after a quick Google but it works like this. So, simply create a little script to be ran in bash:\n#!/usr/bin/bash\nSTEAM_COMPAT_CLIENT_INSTALL_PATH=/tmp STEAM_COMPAT_DATA_PATH=/tmp <Path/to/proton/experimental/install/location/proton> run VRisingServer.exe <args-from-offical-repo>\n… and then chmod +x scriptName.sh and run it to start the server."
  },
  {
    "objectID": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#this-didnt-work",
    "href": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#this-didnt-work",
    "title": "How to Setup V Rising on Linux",
    "section": "This Didn’t Work!",
    "text": "This Didn’t Work!\nSo, in my own troubleshooting I found that I ended up with an error Failed to create batch mode window: Success. when I ran this a second time on my server. However, after doing some testing on multiple different distributions - Ubuntu 20.10, EndevourOS, Manajaro - I found that it didn’t work on any of my systems even though it had ran once. Adding to my confusion, the dedicated server ran find on my own Desktop and without issues at all. Considering one of my servers was Manjaro just like my desktop then there must be something installed on locally which is not being installed along with the Steam Proton Dependencies. The only noticable difference in dependencies was that I have Lutris Installed on my own Destktop and it’s not on any of my other systems. So, I ran through exacty what is installed via Lutris and once I rebooted my other server it worked without issues."
  },
  {
    "objectID": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#so-what-is-missing",
    "href": "posts/Mds/2022-06-01-how-to-setup-a-dedicated-v-rising-server-on-linux.html#so-what-is-missing",
    "title": "How to Setup V Rising on Linux",
    "section": "So, What Is Missing?",
    "text": "So, What Is Missing?\nThe honest answer is that I don’t know. The list of dependences that gets installed along with the process is quite a list and I simply don’t have time to isolate which ones are the correct packages. If you have the time to figure it out then please let me know so I can update this post. Otherwise, Happy Hunting!"
  },
  {
    "objectID": "posts/Mds/2021-05-18-Game-Review-The-Forest.html",
    "href": "posts/Mds/2021-05-18-Game-Review-The-Forest.html",
    "title": "Game Review: The Forest",
    "section": "",
    "text": "Game Header Image\n\n\nThe Forest is another game in the Survival Genre which tries to use Environmental Storytelling to push a plot. You can play with your friends or you can play alone - which definitely gives you a different experience. Alone - so I’m told - feels a lot lot a horror game where you’ll fear leaving the walls of whatever you’ve built. With friends, it was good fun but definitely doesn’t feel like a horror game: FREE BONES!\nWe’ll start with the stuff I didn’t like first. Like most survival games there is crafting for shelters, walls, defenses and cooking. Unlike most other survival games that I’ve played, crafting buildings in this one feels pretty pointless. There are about four buildings that actually matter: Drying Rack, Bonfire, Water Collector, Shelter. Anything that produces light barely matters and you have to constantly feed fuel to the fires to get the initial “overburn” to really see in the darkness - or really far at all.\nWalls are fun but we built on an island off the coast of the main island so we basically just started building random stuff for fun since our base never got attacked. Which is real unfortunate because this game has one of the best building systems I’ve ever played with. The buildings are cumulative so you can build what is basically a blueprint that your friends can collectively dump resources into. And, everyone can see them and since there is a visual blueprint you can actually lay out your base “Blueprint” in full for discussion. What a great idea!\nSince it’s Environmental Storytelling, you can basically skip all of it until the end while murdering everything along the way. This is what happened to us - we ended up running into the caves to wipe out the natives and just kind of picked up whatever we found - on our way to kill more of the natives. This isn’t really the game’s fault but more the way we played so beware of missing stuff until the end when you have no choice but to address the story that you don’t really understand.\nThe combat is surprisingly fun and one the better Combat systems in a Survival Game. Ranged weapons are not broken and do not nullify enemies into a shooting range and melee feels really good with trading blows with the Cannibals. This isn’t really due to the system being complicated but much more about how well done the animations and the interactions with the Natives themselves: they flee, they try to kind of spook and intimidate you, they call their friends. Enemy and animal variety was pretty good and you get introduced to newer enemies at the right times - at least for us. Learn how to block; you’re going to need it.\nI mentioned the Night above but it’s really a positive: when it’s dark it is fucking dark. Wandering around at night can get you lost really fast. Everything moving around you - leaves, trees, animals - will making you constantly on edge thinking about if you’re about to get attacked in the dark. Following enemies in the dark is just hard so you wont know how many there are nor where they ran of to. Running around in the Forest really does feel like moving around in the woods stumbling onto deer and opposing murderhobos while you try and find a stable water supply.\nA few annoyances around multiplayer we ran into where one time we died and my friend had to afk to do something else. And, I got stuck in the plane behind him for a full hour - not game time, real life time - since it was impossible to actually get around him to play the game.\nAnother is that the server doesn’t actually save the state. You must save your own character at shelters otherwise you lose progress. This makes absolutely no sense at all when we’re playing on a server being hosted by someone else. You will lose progress if you don’t personally constantly save.\nAnother was that since we were all doing our own tasks and split up often, some of us didn’t end up having all the tools necessary to get to the end of the game. As a five-man group, one of us got stuck behind because of this while we basically tried to finish the game. If you do this then the bad news is that you all have to vote at the end otherwise you can’t finish the game. We ended up just watching the endings on Youtube after doing all the work to get to the end.\nAlso, game works in Linux completely in Proton 6.3-4."
  },
  {
    "objectID": "posts/Mds/2022-09-08-Game-Review-Pawnbarion.html",
    "href": "posts/Mds/2022-09-08-Game-Review-Pawnbarion.html",
    "title": "Game Review: Pawnbarian",
    "section": "",
    "text": "A friend of mine recommended that I check this game out as he’d stumbled across it somewhere, somehow. Pawnbarian is what looks to be the first published game by j4nw and it’s a good start. There was an earlier iteration of the game on their itch.io page if you go lookikng for it to try out. Checking out the store page, the game presents itself well and is instantly recognizable since it’s a chess board with unique avatars of somekind for your piece and the enemies. The aesthetic is clean and simple which doesn’t distract while playing the game: \n\n\nThe game is simple to undertand too - even if you’ve never been good at chess. You play as an Avatar - the starter is, of course, the Pawnbarian - which comes with usually a single gimmick and a different deck of moves. The Deck of Moves is how you move around the board to kill the enemy tokens - and they’re simply cards with different chess pieces which limit how you move. For example, the pawn moves like the pawn does in chess; the rook moves like the rook and so on. Your moves for the turn are drawn from this deck and you can see both the remaining moves as well as what was discarded so you have some idea of what could happen next round as the cards get pulled. You’re not limited by time since once the all the cards are drawn then they are simply put back and you get access to all of them again. \nAs mentioned, your avatar has a gimmick and the Pawnbarians is that when he plays a card with the Cantrip - the lightning symbol - then he gets to move again. And, you can chain these the more you play them to take as many moves as possible. When the cantrip is played then it also draws another card for you to play so you’re never limited by the cards in the deck. At the end of each round, you can use the gold earned by winning the round to add more abilities to your cards. The cantrip is not the only gimmick to add to cards; you can also add diagonal attack or a horizontal attack which will not only harm the square you land on but allow you to hit multiple enemy tokens. There more as well but we’ll limit the discussion to just those as they get the point across.\nEnemies also have their own gimmicks, such as the Nimble attribute in the screenshot above, which constrains you simply stomping all over the enemy tokens. There are three dungeons with different enemies and different gimmicks to hinder you in your progress in the dungeons. I do wish there were more than the three dungeons since I stomped most of them much faster than I hoped. That isn’t to say some levels are not a challenge; the Void Grasp afflicted boss of Dungeons three was a frustrating challenge to beat but that’s why we play the game. What is not lacking is the list of different avatars you can play; there are 6 of them with their own ways to play the boards.\nIf you like Rouge-lights and Chess then you’ll end up enjoying turning this game on when you’re looking for a break from either Chess or Rouge-lights."
  },
  {
    "objectID": "posts/Mds/2022-11-16-just-a-catpost.html",
    "href": "posts/Mds/2022-11-16-just-a-catpost.html",
    "title": "Nothing To See Here",
    "section": "",
    "text": "Stable Diffusion is Hilariously Fun\nCat."
  },
  {
    "objectID": "posts/Mds/2020-09-30-Game-Review-Post-Void.html",
    "href": "posts/Mds/2020-09-30-Game-Review-Post-Void.html",
    "title": "Game Review: Post Void",
    "section": "",
    "text": "Post Void is a hypnotic scramble of early first-person shooter design that values speed above all else. Keep your head full and reach the end; Kill what you can to see it mend; Get the high score or try again.\n\nAbout a month ago I Steam was kind enough to recommend a rough looking LSD-inspired shooter. And, this little gem is called Post Void created by a little Developer group called YCJY Games. Looking at the videos, screenshots and description I was immediately sold:\n“Post Void is a hypnotic scramble of early first-person shooter design that values speed above all else. Keep your head full and reach the end; Kill what you can to see it mend; Get the high score or try again.”\n\n\n\n\n\nPost Void\n\n\nThe game delivers on all of that with the classic Arcade Shooter style that I miss everytime I spin up any modern First Person Shooter and with a price tag of $2.99, there wasn’t any risk.\nThere is a tutorial level to get you acquainted with the basic ideas of the game - which is really just: You have a slide like many modern shooters. Your health is that glass tiki skull which drains away. Then you’re dropped right in and good luck.\nThe management of your health which forces you to press on creates a fun stresser which is simple but not overwhelming to manage: kill to fill. The enemy varieties are also limited for the 11 stages you race through - which isn’t all that many. It’s not a long game and wouldn’t actually mind seeing a Post Void 2 which pulls some elements from Rouge-likes and maybe a bit of coop shuffle. Also, the aim is a bit forgiving and there were moments where I was sure I would miss but still killed an enemy.\nThe art style is a bit rough on the eyes at first but actually is the right choice to make registering enemies and threats blend into the background just enough that you can miss or run right into them if you’re not careful. I’m also partial to colorful games so I admit this visually is my kind of game.\nAll in all, I’d recommend stuffing this game into downtime between lobbies and breaks between studying to stay alert.\nAlso, the Shotgun is best. Who knows why anyone wants to use the knife: weirdos."
  },
  {
    "objectID": "posts/Mds/2020-12-30-Challenges-For-2021.html",
    "href": "posts/Mds/2020-12-30-Challenges-For-2021.html",
    "title": "Challenges for 2021",
    "section": "",
    "text": "Read 100 Books.\nThis is a challenge because I haven’t done it yet and fitting that many books into my life forces me to think about my spent time. With 52 weeks in the year, this means I will need to read 1.92 books per week. If I’m going to have a break day from everything - on Sundays - then that usually means reading about 100-134 pages of the current novel per day. Doing this along with being productive and pursuing larger goals gets it onto my list.\n\n\nCreate Three Potential Streams of Income.\nI wont be discussing what those are but if modern society and the current state of affairs has taught me anything it’s to be proactive. You don’t want to be responding to a loss after it’s happened; you want to have responded before it has happened. Always have a plan B - and maybe a C. If you need your computer to work, then you’ll want a fallback that can throw in as a substitute. With working from home, the first thing I did was check to see if I can use my phone as a substitute for if my ISP decides to do “maintenance.” It works by the way. I wont hold myself to actually generating an income yet but that’s a stretch goal for the year.\n\n\nNew Job Title\nI’m bored of being a Senior at this point and either need some larger technical challenges or just a swap in the set of responsibilities I deal with on a daily basis. There are some possibilities which I will try either to create or follow up on but I’m not staying where I am.\n\n\nA Blog Post a Month.\nI have this place where I can drop interesting project ideas or discuss problems that I’ve solved for myself to share with others. Since I don’t have a lack of problems to solve, then I should be writing about them for others to read and refer back to. Solving a problem a month is not the challenge but remembering to write about it is.\n\n\nDraw An Original Comic.\nIf you’re following me on any social media then you’ve probably seen some crude drawings. While I admit I’m doing them for my own enjoyment - and challenge - I should push towards something tangible.\n\n[A book you’ve been meaning to read.]: The Grammer of Science by Karl Pearson\n[A book about a topic that fascinates.]: Unrestricted Warfare by Qiao Liang, Wang Xiangsui\n[A book in the backlist of a favorite author.]: Legionaire by Jason Anspath, Nick Cole\n[A book recommended by someone with great taste.]: Awake in the Nightland by John C. Wright\n[Three books by the same author.]:\n\nForgotten Ruin by Jason Anspath, Nick Cole\nHit And Fade by Jason Anspath, Nick Cole\nViolence of Action by Jason Anspath, Nick Cole\n\n[A book you chose for the cover.]: JinJang by Iris Paustian\n[A book by an author who is new to you.]: The Forge of Christendom: The End of Days and the Epic Rise of the West by Tom Holland\n[A book in translation.]: The Nine Chapters on the Mathematical Art: Companion and Commentary by Shen Kangshen\n[A book outside your comfort zone.]: The Social Construction of Reality: A Treatise In the Sociology of Knowledge\n[A book published before you were born]: The Constitution of the United States of America by The Found Fathers"
  },
  {
    "objectID": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html",
    "href": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html",
    "title": "Credit Card Security",
    "section": "",
    "text": "Everyone is worried about having their credit card information stolen at some point. We all trust the banks to ensure nobody can just guess our number and submit orders in our name. But, did you know math is the secret behind your security? There are four main issues with stealing a credit card: 1. Getting the Credit Card Number 2. Getting the Security Code 3. Getting the Expiration Date 4. Getting your Personal Details"
  },
  {
    "objectID": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-credit-card-number",
    "href": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-credit-card-number",
    "title": "Credit Card Security",
    "section": "Getting the Credit Card Number",
    "text": "Getting the Credit Card Number\nThere are a total of 10000000000000000 total numbers that can exist for a credit card number. The odds, therefore, to get your number is 1:10000000000000000. But, it’s worse than that because the entire collection of numbers are not used to generate credit cards. In fact, credit cards can pass or fail passed on something called Luhn’s Algorithm This algorithm is used to create what is called a subgroup inside of the total number of cards. We don’t want criminals to just guess numbers and accidentally hit anyone. This is formula is a first line of defense to ensure fake numbers cannot be submitted.\nALGORITHM_CONSTANT = 9\n\ndef confirmLuhn(number):                                             #==== Luhn's Algo ==========================\n    count = 0                                                    # Sum all numbers excluding the last number.\n    sum   = 0                                                    # For every other number, square it.\n                                                                     # Compare the last digit of the CC number\n    for digit in number:                                         # against the last digit of the sum.\n        num = int(digit)\n        if   count == 15:                                    # If final digit,\n            return (sum *ALGORITHM_CONSTANT %10 == num)  # compare final digits.\n        elif count % 2 == 1:                                 # If off number,\n            sum   += num*num                             # square number.\n            count +=1\n        else:                                                # If not off number,\n            sum   += num                                 # just add the number to the sum\n            count += 1                                   #===========================================\n\n\noutfile = open(\"CCNumbers.txt\", 'w')                          # File opening, obviously.\n                                                              # First we generate the number space that we're testing.\nfor x in range(0, 0000000000000100):                          # The real CC number space is actually [0, 9999999999999999], but that's too much.\n    xString = \"%.16d\" % x                                 # This is probably lazy design, but I want to fill out the space of numbers and keep the 0's\n    if confirmLuhn(xString): outfile.write(xString+\"n\")  # If it's one of them, then write the result to the file.\n\noutfile.close()                                               # BILLIONS OF YEARS OF WORK AND YOU FORGOT TO CLOSE THE STORAGE FILE IN THE CODE?!?\nI wrote a basic CC number finder that uses a very small space of numbers to check against as existing numbers - and, you can find it herefor those wishing to pull it and build on it."
  },
  {
    "objectID": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-security-code",
    "href": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-security-code",
    "title": "Credit Card Security",
    "section": "Getting the Security Code",
    "text": "Getting the Security Code\nThe probability of getting the 3 digit number on the back of that card is 1:1000. That might not seem like a lot, and it’s not, but without that number you can’t use the card online. And, most banks will lock your card after a few failed tries. If I can see and memorize the 16 digit Credit Card number from seeing once - this isn’t all that hard, it just takes a bit of practice - this 1:1000 probability is what keeps them from using it online without your consent.\nLet’s assume that I did memorize your number and I’m trying to figure out what that code is. Most banks I know of will lock a hard after 5 failed attempts. That means I have a 5:1000 or 1:200 chance of guessing right before the card number becomes useless. Of course, that’s completely ignoring if I don’t have the number and am just trying to guess everything. Even if I beat the 1:10000000000000000 chance of picking the right number and it passes the Luhn Algorithm then I still need to guess with a 1:1000 chance of getting the right number within each potential number. But, it gets even worse for me."
  },
  {
    "objectID": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-expiration-date",
    "href": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-the-expiration-date",
    "title": "Credit Card Security",
    "section": "Getting the Expiration Date",
    "text": "Getting the Expiration Date\nI still have to get past your expiration date. There really is no good way to know the actual probability since banks can use any number of years before the card expires. But, I’ll assume a larger case to overestimate since banks will make the task as hard as possible. Let’s guess that the range of expiration dates is from 2000 - 2030 since some cards are in the wild. We’re looking at a 1:360 chance - which is really tiny when you compare it to other probability spaces. But, we need to think about the entire probability space of 1:10000000000000000 * 1:200 * 1:360 just to find the right card. To select the right card data of just you is 1:720,000,000,000,000,000,000. For comparison, that’s about 6 multiples large than the estimated total of every human being ever alive.\nBut, what if I memorized all of this information?"
  },
  {
    "objectID": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-your-personal-details",
    "href": "posts/Mds/2021-02-20-Luhns-Algorithm-Credit-Card.html#getting-your-personal-details",
    "title": "Credit Card Security",
    "section": "Getting your Personal Details",
    "text": "Getting your Personal Details\nEven after selecting the right credit card with the right security code and the right expiration date you still need to know the precise living address of the person you’re trying to attack to submit orders online. No wonder credit card scammers steal them with skimmers or fake online bank pages! Math wins."
  },
  {
    "objectID": "posts/Mds/2022-11-01-game-review-powerwash-simulator.html",
    "href": "posts/Mds/2022-11-01-game-review-powerwash-simulator.html",
    "title": "Game Review: Powerwash Simulator",
    "section": "",
    "text": "> Wash away your worries with the soothing sounds of high-pressure water. Fire up your power washer and blast away every speck of dirt and grime you can find, all with the simple satisfaction of power-washing to a sparkling finish.\nMy friends have been playing this game and I finally picked it up. After doing some cooperative levels with them, I started into the Career Mode of the game. This is the Story Mode and does what you’d expect from a Single Player experience: slowly increasing difficulty through different tools, different surfaces and different buildings with different shapes. There is a story here which is silly; reach the end or spoil it with a Let’s Play if you’d like but I laughed.\nThe beginning is mostly slow to work through since the early tools are acceptable for the tasks you’re given. If you invest the stars and money into betters tools and nozzles then you can get ahead of the curve - at least in my experience. Discussing which tools are best is somewhat moot since you’ll beat the levels faster - but do you really want even want to?\nThis brings me to my final thoughs: I’m not sure if this game is fun. After reaching the end of everything, I can say it was relaxing and I definitely accomplished something of value. If nothing else, the game lends itself well to listening to podcasts while you mindlessly clean bathrooms and boats. The core loop works but I found myself somewhat resistant to completing levels around the middle of the game. It was putting on the podcasts to distract me that ended up making the game playable."
  },
  {
    "objectID": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html",
    "href": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html",
    "title": "Simple Introduction to Python Debugging",
    "section": "",
    "text": "While working on a script that I’m writing for work, I was having trouble really tracking down where my logic was failing. After looking over the code, everything looked correct but the results were showing me otherwise. So, I opened up python’s debugging system and started to dig in."
  },
  {
    "objectID": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html#what-is-debugging",
    "href": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html#what-is-debugging",
    "title": "Simple Introduction to Python Debugging",
    "section": "What is Debugging?",
    "text": "What is Debugging?\n\nIn computer programming and software development, debugging is the process of finding and resolving bugs (defects or problems that prevent correct operation) within computer programs, software, or systems. cf: https://en.wikipedia.org/wiki/Debugging\n\nThere are plenty of tools to do stuff like this but Python comes with its own: pdb — The Python Debugger. To activate it, you’d simply go into your terminal and type:\nuser@station# python3 -m pdb <script-name>\nThis with start the script from the top and immediately stop before running anything:\nuser@station:/scripts# python3 -m pdb keyIntegrityCheck.py\n> /scripts/keyIntegrityCheck.py(5)<module>()\n-> from pathlib import Path\n(Pdb)\nWhile this is useful, we need to be able to explore the file. You could keep an editor open on a different monitor and move around but sometimes that’s not going to be an option. To print all the lines in the file you’d use:\n(Pdb) ll                                                                                    \n1     #!/usr/bin/env python3                                        \n2     ## Author: Collin Mitchell                                 \n3     ## Purpose: To check the integrity of keys without user input.  \n4                                                                               \n5  -> from pathlib import Path                                                   \n6     from itertools import filterfalse                                            \n7     import subprocess as sp                                                          \n8     import json                                                                     \n9     import sys    \n# ...\n370                 integrityFunc = dd.get(str(filename).split('.')[-1])\n371                 if integrityFunc:\n372                     consistent = integrityFunc( filename )\n373                     if not consistent: print(\"{} is not consistent.\".format(filename))\n374                 else:\n375                     print(\"{} is missing a checker; please report {} so it can be added.\".format(filename,filename))\n(Pdb)\nYou can see the pointer on line five telling us where the current execution point is. We can move the pointer a single line using n:\n(Pdb) n                                                                                                                                                                   [336/1995]\n> /scripts/keyIntegrityCheck.py(6)<module>()\n-> from itertools import filterfalse                   \n(Pdb) ll                                                                                    \n  1     #!/usr/bin/env python3                                        \n  2     ## Author: Collin Mitchell                                 \n  3     ## Purpose: To check the integrity of keys without user input.  \n  4                                                                               \n  5     from pathlib import Path                                                   \n  6  -> from itertools import filterfalse                                            \n  7     import subprocess as sp                                                          \n  8     import json                                                                     \n  9     import sys     \nYou can see the next 11 lines in the console buffer using l:\n(Pdb) l\n  1     #!/usr/bin/env python3\n  2     ## Author: Collin Mitchell\n  3     ## Purpose: To check the integrity of keys without user input.\n  4  \n  5     from pathlib import Path\n  6  -> from itertools import filterfalse\n  7     import subprocess as sp\n  8     import json\n  9     import sys\n 10  \n 11     # stuff to do with phpserialize:\n(Pdb)\nNot that if you do this and run it again that you wont get the same result:\n(Pdb) l\n 12     import codecs\n 13     try:\n 14         codecs.lookup_error('surrogateescape')\n 15         default_errors = 'surrogateescape'\n 16     except LookupError:\n 17         default_errors = 'strict'\n 18     try:\n 19         xrange\n 20     except NameError:\n 21         xrange = range\n 22     try:\n(Pdb)\n… but you will get the next 11 lines instead. You can tell it which lines to list centered on a line number using l 10:\n(Pdb) l 10\n  5     from pathlib import Path\n  6  -> from itertools import filterfalse\n  7     import subprocess as sp\n  8     import json\n  9     import sys\n 10  \n 11     # stuff to do with phpserialize:\n 12     import codecs\n 13     try:\n 14         codecs.lookup_error('surrogateescape')\n 15         default_errors = 'surrogateescape'\n(Pdb)\nNow that we can move around, let’s discuss how to actually stop code execution using breakpoints. These are locations you set - sometimes with conditions - to stop the code execution and explore the current state. You can set these using ‘b’:\n(Pdb) b\n(Pdb)\nSince we don’t have any breakpoints set, then it makes sense we don’t see any listed. So, now lets set one:\n(Pdb) b 9\nBreakpoint 1 at /scripts/keyIntegrityCheck.py:9\n(Pdb)\n… and continue execution until the breakpoint using c:\n(Pdb) c\n> /scripts/keyIntegrityCheck.py(9)<module>()\n-> import sys\n(Pdb)\n… and list the active breakpoints again:\n(Pdb) b\nNum Type         Disp Enb   Where\n1   breakpoint   keep yes   at /scripts/keyIntegrityCheck.py:9\n        breakpoint already hit 1 time\n(Pdb)\nYou can clear that breakpoint using the number of the breakpoint as well:\n(Pdb) b\nNum Type         Disp Enb   Where\n1   breakpoint   keep yes   at /scripts/keyIntegrityCheck.py:9\n        breakpoint already hit 1 time\n(Pdb) cl 1\nDeleted breakpoint 1 at /scripts/keyIntegrityCheck.py:9\n(Pdb) b\n(Pdb)\nConditional breakpoints I found a bit tricky to get to work correctly because you need to place a comma after the statement:\n(Pdb) b 371, integrityFunc.__name__ == 'integrityPhp'\nBreakpoint 3 at /scripts/keyIntegrityCheck.py:371\n(Pdb)\n… and then you c until it triggers:\n> /scripts/keyIntegrityCheck.py(371)<module>()\n-> if integrityFunc:\n(Pdb) integrityFunc\n<function integrityPhp at 0x7f99291477b8>\n(Pdb)"
  },
  {
    "objectID": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html#conclusion",
    "href": "posts/Mds/2020-10-21-Simple-Python-Debugging-Example.html#conclusion",
    "title": "Simple Introduction to Python Debugging",
    "section": "Conclusion",
    "text": "Conclusion\nI couldn’t find much outside of the official documentation when I needed it so hopefully you find the highlights useful. There are also tools for whichever IDE you’re using so look out for those as well."
  },
  {
    "objectID": "posts/Mds/2021-12-08-Review-Of-2021-Challenges.html",
    "href": "posts/Mds/2021-12-08-Review-Of-2021-Challenges.html",
    "title": "Review of 2021 - Challenges, Thoughts",
    "section": "",
    "text": "The honest truth is that I what I wanted to get done this year was pretty fluid and I did not get many of the commits done. Looking at what I have been working on though - and what I ended up spending my time on - I didn’t do so bad this year.\nFirstly, I finally got a macro workflow that is working for pushing my projects forward along with learning new topics. One of the problems which I have been struggling with is how to both learn enough new concepts and frameworks along with actually using them. The normal process being used it to 1. Find something of interest. 2. See what’s been made in it. 3. Work on tutorials, docs for the framework/tool/information. 4. Try to apply it to pet projects. 5. Maybe use it in a real project.\nThis process has problems that I needed solved. Since I have my hands in multiple different places - Data Science, Linux, Programming, Web Design, Secret Project I can’t discuss yet, etc - then this simply doesn’t work. With my life growing in so many dimensions, I simply cannot sit down and follow this process for a single framework and then maybe add it to what I am working on. If it takes a month or two to apply and become comfortable with the framework then it’s too slow and the other aspects of my life will end up neglected. So, what do I do now?\nI have split the months of the year up into four quarters - just like a a business calendar. Then, the first month of that quarter is a Learning Month which means I review what I need to learn, allocate out a set of classes for the month and then do power through them in that month. I take notes about what I’m learning and for some parts will actually practice with a very small pet project which is meant to implement a single thing. After that month is over, the next two months are Apply Months which means that I take everything that I learned in that month, review it, and then start adding it to my projects.\nThis allows me to schedule up to four of classes covering any material I might need coming up in the next few months. This is also better because it keeps learning and application separate. When my schedule was fluid I kept finding ways to not work on projects since I constantly fell back into learning about new frameworks and tools. Now, the clear separation is allowing me to be strict about when and what tasks are supposed to be done. It is already paying out dividends – even though the first attempt at it I was sick for most of the first month of Apply.\n\n\nOf all the challenges that I wanted to complete, this is the one. And, I did worse than last year too: 81 books last year vs 31 books this year. Looking over how my life has changed since the Pandemic started, I think the biggest reason is that much of my reading was done in between. If I was waiting in line at a store then I was reading; while walking from the parking garage at work into the office I was reading. Now that we’re remote only and there are no lines when I am out running errands, I just don’t really read books as much as I used to. I do listen to them while walking around stores but often I don’t count those in the totals; I finished all of the Dragonlance Chronicles books via Audible for example and they’re not filled out on Goodreads. This means setting up time at home to read and I’m always working on projects or doing chores or watching lecture videos. That really only leaves downtime at work to read and we’ve been relatively busy lately. And, I happen to also have projects for Datto that I work on so that’s not really in the cards either. I’ll have a separate post to what the Challenges of 2022 are going to be so strategies will be discussed in that post.\n\n\n\nFor this one, I was successful. Currently, I have two streams of income: one active (Datto) and one semi-passive (Crypto) to work with. I am making money in both of those realms at this point which was more than I had set for the goal. The goal was to identify three potential streams of income that I could have for the year and there was no shortage of ideas for this one. I’ll discuss some of my favorites in the real Challenges of 2022 post but there are plenty more than three,\n\n\n\nThis is was a failed challenge. After exploring and talking with lots of people, this is going to be the hardest one. And, you’d consider that since I already have so many useful skills that it would be the easiest but it’s simply not. Realistically, I like where I work and I don’t really want to leave any time soon. All the positions that are within reach are jobs that I simply don’t want. And, I am not willing to trade in the job I have for those other jobs since they’d change something fundamental about what I want: working overnights, well limiting interacting with users, flexible time table for my own work. I’ll discuss more details in the Challenges of 2022 post when I write it.\n\n\n\nThis is another place that I did decent but did not hold myself too. The biggest factor for this was simply not setting aside the time for it. It’s lower on the list of priorities which need to get done so I simply didn’t schedule out the time. Along side that was anything I wanted to write about I didn’t feel either qualified to write about or didn’t feel what I was willing to write would be be long or interesting enough to justify the post. This is a mindset problem which I should be able to easily solve in the coming year. Especially after looking at what the average person on Medium is writing about; they’re not even trying to write quality content most of the time but instead are simply writing very short not-even-tutorial posts about how to use a framework. There is also a surprising amount of “X Ideas fro Passive Income” posts which are mostly all the same thing over and over. I can do better than that.\n\n\n\nI dropped this one entirely – and I’m dropping it for next year too. I have a list of Daily Tasks which include things like exercise, typing practice along with drawing practice and by far the task I skip the most is drawing. I need to change this so I can actually start drawing at an ok level. Right now, I am struggling with curved lines and that’s quite basic for this level of challenge. I have a few ideas – when don’t I? - but this is a long road which I need to build basic habits out before even trying some kind of real challenge like a Comic."
  },
  {
    "objectID": "posts/Mds/2020-10-28-The-Pulp-Mindset-Review.html",
    "href": "posts/Mds/2020-10-28-The-Pulp-Mindset-Review.html",
    "title": "Book Review: The Pulp Mindset; A Newpub Survival Mindset",
    "section": "",
    "text": "This book is about something worth noticing as the access to writing and publishing has broadened. There are healthy and helpful tips for writing and about writing learned from reading the Pulps of the past - and as someone that hasn’t read much of them and slowly breaking that particular seal I cannot agree more. However, reading the book feels more like what my own essays in high school felt like with repetition spread around and re-wordings of the same ideas.\nThat isn’t to say don’t purchase or read it; I think you should just because there is some good history here. The author’s knowledge about such topics as the speech Mutation or Death was quite a surprise and the relationship between Old Pub and New Pub is a meaningful distinction that Cowan does well. Personally, I think he’s overselling it as there is enough poor writing on display via Amazon Unlimited or niches such as LitRPG where the divide between practiced writers or amateurs shows up from the first page.\nAll in all, spend the money but get it in digital."
  },
  {
    "objectID": "posts/Mds/2021-02-09-Memorandum-Of-Understanding-Blurb.html",
    "href": "posts/Mds/2021-02-09-Memorandum-Of-Understanding-Blurb.html",
    "title": "What is a Memorandum of Understanding?",
    "section": "",
    "text": "A Memorandum of Understanding primarily serves as a point of negotiation between two or more parties with respect to their intents. The document serves as a way to find common ground along with set expectations about what individual parties are both responsible for and expect from the other party. Special consideration should be paid to whether the document is Legally binding in the legal context it is being used; these are normally not legally binding. It is best thought of as similar to a Gentlemen’s Agreement: “an informal and legally non-binding agreement between two or more parties. It is typically oral, but it may be written or simply understood as part of an unspoken agreement by convention or through mutually-beneficial etiquette”[2].\n\n\n\nSample from Violence Against Women Oganization\nFormSwift"
  },
  {
    "objectID": "posts/Mds/2021-02-09-Memorandum-Of-Understanding-Blurb.html#citations",
    "href": "posts/Mds/2021-02-09-Memorandum-Of-Understanding-Blurb.html#citations",
    "title": "What is a Memorandum of Understanding?",
    "section": "Citations:",
    "text": "Citations:\n\nhttps://en.wikipedia.org/wiki/Memorandum_of_understanding\nhttps://en.wikipedia.org/wiki/Gentlemen’s_agreement\nhttps://www.investopedia.com/terms/m/mou.asp\nhttps://www.investopedia.com/terms/l/letterofintent.asp"
  },
  {
    "objectID": "posts/Mds/2020-09-18-Game-Review-Empyrion-Galactic-Survival.html",
    "href": "posts/Mds/2020-09-18-Game-Review-Empyrion-Galactic-Survival.html",
    "title": "Game Review: Empyrion - Galactic Survival",
    "section": "",
    "text": "If you’re looking for a TL:DR: game is fun but has some challenges.\nThe first time I opened the map to see all the different systems we could visit I was basically sold on the game. The first time we made our system hop, we almost got trapped in hellscape and had a blast trying to get off that planet.\nEmpyrion - Galactic Survival is - well - a Survival Game that adds the idea of different planets and systems to explore instead of simply other towns or woods or whatever that splotch over that hill is. What’s nice about it was as you explore the menus, it gives you something to plan for while trying to build up some semblance of safety where you are. As you scroll out, there are hundreds of systems to explore with their own rings of planets and moons and asteroid fields - which really is the allure of this game. I want to float around the Galaxy and do stupid things.\nOut of the gate, there are lots of geometries to play with when building bases or anything else which is a nice change from other Survival games where you’re kind of stuck with a few pre-defined structures. Sadly, the internal structure of the actual block shape wont impact where you can actually place anything since you’re still locked into a cubic construction system. This does make a good number of the more interesting shapes rather impractical to try and use on anything you build.\nThe plethora of pieces to care about makes navigating and management challenging. Finding out just how to build your first actual base was annoying until you realize that you need a core or a starter block to build off anything. Different blocks are tied to different kinds of structures which are denoted by the initials HV,CV,SV,BA which are just not explained. You’re going to be reading the descriptions of the stuff you build; you’re going to have to just about all the time because you’re going to miss important details. My mistake was not realizing that shields will require a Pentaxid to fuel them and there is no warning about this until the shields don’t work - except the last line in the text. A better idea maybe would have been that you don’t have any shields until you’ve actually fed your Vessel Pentaxid and therefore you’ll be forced to realize this. There is a good amount of finding out the hard way.\nLikewise, there are lots of small stats and management that goes into a base which make sense but just don’t feel intuitive at all. I know that I need power and to do that I need something which makes power but I don’t know anything about Solar Panel rates from looking at the device before construction; there is no real way to optimize buildings or make decisions about if I want something until I’ve made it and realized that I either don’t need it yet or it’s missing necessary components. The Solar Panels need a Capacitor else they’re really not much use once the sun is down - and the base doesn’t store anything. Explore the Control Panel as soon as you can.\nContrary to some complaints, the combat isn’t bad for a Survival game and fights in space have been pretty fun. Having my friend float around in a tuned Small Vessel hecking out resources in Zirax controlled space to avoid pulling us into a space fight with our starter Capital Vessel only to be discovered by a wandering drone led to quite the episode of us coordinating his docking while everything in the system came to murder us escaping with a queue’d up lock to a different system for just such an emergency is why this game has been fun. The complaints about being outgunned are certainly true though. The Defense Station that sits just outside the atmosphere is a space graveyard of failed attempts to kick it out of the orbit. After building a prototype to try and kill the thing, I point-blanked it with 6 pulse Lasers 49 times before it finally blew apart my small vessel and I lost. The AI for the enemies in the game is certainly questionable too. I’ve watched as three factions of different types mingle with one another as if they’re not even aware of each other until I am taking shots at them and then they’re all after me like there is some pact to murder non-NPCs in the Galaxy on sight.\nAll in all, I’m not regretting buying it and it’s probably my favorite current Survival game. I think that the criticisms are pretty fair though; the game is very ambitious and they haven’t delivered yet but the platform is there so long as they’re continuing to diligently add the pieces patch by patch I’ll keep playing it. Complicated but promising."
  },
  {
    "objectID": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html",
    "href": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html",
    "title": "How Do You Create a Normal Map?",
    "section": "",
    "text": "Looking around the Internet, it ended up being much harder to find the information about how to make a Custom Normal Map then I though it should be. After finally figuring this out, I thought I’d formalize/share it here so that it doesn’t get lost among everything else."
  },
  {
    "objectID": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html#where-were-going",
    "href": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html#where-were-going",
    "title": "How Do You Create a Normal Map?",
    "section": "Where We’re Going",
    "text": "Where We’re Going\nWe’ll go over the how to do this with a simple example: dirt. First you’ll want to start blender up and clear the scene. Drop in a new plane and make it whatever size you like; I scaled it by 2x so if you’re trying to just copy my examples then simply do that.  Since we’re just working on making a material to import into a game, we don’t really need anything else. So, we’re moving on to making the texture: \nAgain, this is just a basic example so we’ll create something like this:  Not bad, not complicated but will show us the effect we’re after; feel free to copy the settings if you’re following along. Now add an Image Texture Node to the graph but don’t attach it to anything: \nCreate a new image and call it what your target material is about. In my case, I’m just making dirt so we’re calling it BasicDirt:  Now we’re going to do something called Baking. So, what is Baking actually? What we’re doing is saving information to the texture so that it doesn’t have to be re-calculated. Eevee - the default engine in blender - doesn’t support baking so you’ll need to switch over to Cycles for this to work. Once that is done then go ahead and click Bake and it should work!  You should now see the image show up in the Image Editor in the UI. You’ll want to save this to an acutal file for use in the Game Engine - or for other uses like making a normal map for it."
  },
  {
    "objectID": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html#the-next-stop",
    "href": "posts/Mds/2022-02-02-How-To-Make-Matching-Textures-And-Normals.html#the-next-stop",
    "title": "How Do You Create a Normal Map?",
    "section": "The Next Stop",
    "text": "The Next Stop\nOk, now that you have the texture go ahead and pull this into your photo editing software. I’m going to do this with Gimp so to follow along you’re going to need it. Otherwise, you should just be able to look up “Making a Normal Map in Photoshop” and it should be a simple option. I spent quite a bit of time trying to make the Normal Map in Blender and I couldn’t get it working; if someone did figure this out then please let me know. But until then, you’ll want pull your image into Gimp first. Once that is done, you’ll select Filters > Generic > Normal Maps{{ site.baseurl }}. and it will present you with the ability to set how harsh the effect is:  … and that scale is not the default. Go ahead and play around with that number to get the effect you want. Depending on your Game Engine, save it with the proper nomenclature for when you pull it in. \nGet your Blender project back open and hop into the Shading Pane once more. Add a new Image Texture Node and insert the new Normal map you created. Once added the image into the Image Texture Node, you’ll want to switch over to non-color: \nAnd, that’s how you Make a Normal Map, and add it into blender. You can improve it some using other options like adding a Bump Node between it."
  },
  {
    "objectID": "posts/Mds/2021-02-04-Introduction-To-Streamlit.html",
    "href": "posts/Mds/2021-02-04-Introduction-To-Streamlit.html",
    "title": "Quick Introduction to Streamlit",
    "section": "",
    "text": "Streamlit is a Framework which is growing in popularity among Python Data Scientists. And the reason for this is rather obvious if you’ve every tried mixing building Web Applications with Data Exporation or Reporting for other people inside your company - or just in general for your projects.\nWe’re going to build a little Web App with the Iris dataset. And yes, please don’t groan; I am also sick of the Iris dataset but it’s clean and simple and accessible.\nSo, first we start with actually downloading and it’s just their namesake:\npython -m pip install streamlit\nNext we’ll import and scaffold off their introduction application:\nimport streamlit as st\n# To make things easier later, we're also importing numpy and pandas for\n# working with sample data.\nimport numpy as np\nimport pandas as pd\n\nst.text(\"Hello World\")\n… and save that to a file called irisExplore.py. Once that’s done, you’ll run it from the console using streamlit run irisExplore.py: \nAnd there we have our first Web Application. Calling the HTML elements that you want and expect is just that simple. If you want a paragraph then you’ll use st.title():\nst.title(\"Turtles Turtles Turtles\")\nst.subheader(\"But, which movie reference is it?\")\nst.text(\"Hello World\")\n And, there is even support for Latex too!:\nst.title(\"Turtles Turtles Turtles\")\nst.subheader(\"But, which movie reference is it?\")\nst.text(\"Hello World\")\nst.latex(r'''\n    a + ar + a r^2 + a r^3 + \\cdots + a r^{n-1} =\n    \\sum_{k=0}^{n-1} ar^k =\n    a \\left(\\frac{1-r^{n}}{1-r}\\right)\n    ''')\n\n\n\nHello World, Now With Latex!\n\n\n\n\nSo, now we’re going to take a quick look at how to add charts. First we’re going to collect the iris dataset. If you have R or something like Tensorflow installed then you can import it from there. I’m going to collect it from someone random on the internet. You should probably download this dataset and store it somewhere; you’ll never get away from it.\nimport pandas as pd\niris = pd.read_csv(\"https://datahub.io/machine-learning/iris/r/iris.csv\")\nAdding the chart is just as easy and you would expect:\nst.line_chart(iris)\n Not exactly what we’re looking but it’s kind of neat. This should serve as a reminder that when it comes to tools, it’s Garbage In, Garbage Out for what you feed it. Be careful what you’re doing because our tools wont know any better."
  },
  {
    "objectID": "posts/Mds/2021-09-07-game-review-ethereal-estate.html",
    "href": "posts/Mds/2021-09-07-game-review-ethereal-estate.html",
    "title": "Game Review: Ethereal Estate",
    "section": "",
    "text": "Game Header Image\n\n\nI had quite a bit of fun with this game. While working on a Data Science project to try and make predictions on “What Genre Is this Game Based on it’s Header Image?” I stumbled across this weird little game trying to solve an error. Looking through the sample set of games, Ethereal Estate pops out among all the other DLC headers and who knows what else these other images are supposed to inspire. The game is free and you play as a ghost - along with maybe your friend - to terrorize, ruin and throw everything within reach around: like a proper ghost should!\nAnyways, the trailer is simple but hilarious so good work Emergence Studio for getting my attention and pulling me in. Your ghost experience is a shopping list of mayhem courtesy of the designer where you cross all kinds of lines: lighting things on fire, throwing people through windows, ramming things with swords, stuff people in closets, poisoning food, breaking records, bringing your victoms back to murder them again, etc. The game is surprising smooth and simple - and fun. My friend and I found the list a bit unclear while roaming around throwing people through windows; my favorite activity in the whole game. I just hope there are more levels coming since there is only the one and we cleared it without too much trouble. Maybe a more traditional graveyard?"
  },
  {
    "objectID": "posts/Mds/2022-01-06-Game-Review-How-I-Lost-My-Eraser.html",
    "href": "posts/Mds/2022-01-06-Game-Review-How-I-Lost-My-Eraser.html",
    "title": "Game Review: Eraser",
    "section": "",
    "text": "> Throw your stick-figure-self at cannonballs and airplanes in an upward pursuit of the eraser! Play cooperatively online to share the journey! Who’s to say whether the company will help or hinder, though…\nRecently, I was looking for some simple and free games to enjoy and ran across this little gem. Eraser by Ringating feels like playing a Platformer from the 90s but with the Visuals and Game Design philosophy of now. Let me explain.\nFirst off, there is no such thing as death in this game. You simply cannot die and there is no real punishment for making a mistake. The worst that happens is that you have to start all over or fall a little ways down and have to regain ground again. You might be thinking they’re the same because if you died in platformers from before then you’d also have to start all over; They all had a checkpoint system where if you had lives and you died then you could start from the checkpoint again. Games like Spyro have the faeries which record your position after all. But, this game has no hard Start Over Because You Died like those games do. Sure, you could incidentilly fall all the way down - I certainly did more times than I’d like to admit - but there is no hard lose condition. You keep playing as long as you’re willing to and sometimes you fall and have to start part of it again. Somewhere along the way, the levels make it harder to fall to to the beginning so there isn’t even really a checkpoint system as much as the design makes it harder to return to the beginning.\nThe game itself is just smooth to play. The musical theme of simple classical fits nicely with the theme of the game of being a simple stick figure in a simple world full of simple enemies and obstacles. I’d definitely recommend picking it up and playing it for a few minutes when you don’t have a lot of time to commit to something else - and it’s free on Steam!"
  }
]
{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /apache/data science/python/spark/2021/01/18/Minimal-Spark-Cluster-Without-Hadoop\n",
    "badges: false\n",
    "categories:\n",
    "- python\n",
    "- data science\n",
    "- apache\n",
    "- spark\n",
    "comments: false\n",
    "date: '2021-01-18'\n",
    "description: Setting Up Without Hadoop Cluster\n",
    "output-file: 2021-01-18-minimal-spark-cluster-without-hadoop.html\n",
    "title: Minimal Spark Cluster\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "If you'd like to setup Apache Spark to experiment with but you don't want to use a premade ISO or setup your own then I'm going to show you how. This configuration will be a minimal one using Linux Operating Sytems; I'm going to use Ubuntu so change the install based on your package mananger.\n",
    "I'm going to assume that you've setup the hosts, their networking and have some way to configure and deploy them. There are options like Puppet or Salt but I'll be avoiding those and leave them up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "I have script that does all of this but we're going to go over it so you understand each part and then I'll attach the bash script at the end.\n",
    "To start with, we're going to need Java since Spark is dependent on it. Login to each host - Master and Slaves - and you'll want to run:\n",
    "```bash\n",
    "apt update && apt install openjdk-8-jre openjdk-8-jdk -y\n",
    "```\n",
    "Remember to adjust this for own Linux Distribution.\n",
    "\n",
    "Figure out which directory you want to install Apache Spark into; Normally, you'd use `/opt` so that's what we're going to be using. Download the archive of the files from the website:\n",
    "```bash\n",
    "wget -O /opt/spark-2.4.6-bin-hadoop2.7.tgz https://downloads.apache.org/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "```\n",
    "\n",
    "If they obsolete the download link then you can find the versions online [here](https://spark.apache.org/downloads.html). I'm using the 3.0.1 version Pre-built for Apache Hadoop 2.7 for this but feel free to experiment.\n",
    "Once the file is downloaded, you'll want to unpack the archive:\n",
    "```bash\n",
    "tar -xzvf /opt/spark-2.4.6-bin-hadoop2.7.tgz  -C /opt\n",
    "rm /opt/spark-2.4.6-bin-hadoop2.7.tgz\n",
    "```\n",
    "\n",
    "Now you'll have files ready for usage.\n",
    "Next you'll want to add the environmental variables so that linux will know where to look for the binary when you call it:\n",
    "```bash\n",
    "echo -e \"\\nexport SPARK_HOME=/opt/spark-2.4.6-bin-hadoop2.7\\nexport PATH=/opt/spark-2.4.6-bin-hadoop2.7/bin:$PATH\" | tee -a ~/.bashrc\n",
    "export SPARK_HOME=/opt/spark-2.4.6-bin-hadoop2.7\n",
    "export PATH=/opt/spark-2.4.6-bin-hadoop2.7/bin\n",
    "```\n",
    "\n",
    "Do this for all the hosts that you'll need to run Spark on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Files\n",
    "It is best to create a master copy of these next few configuratoin files to copy to each host in turn.\n",
    "This way you only need to edit each file ones and then copy them all the the appropriate hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your master server, you'll need to update two files: `spark-defaults.conf` and `spark-env.sh`. Both of these should be found inside the **conf** directory of your spark home.\n",
    "Make sure you make a backup of them before you do anything else:\n",
    "```bash\n",
    "cp spark-defaults.conf spark-defaults.conf.bkp\n",
    "cp spark-env.sh spark-env.sh.bkp\n",
    "```\n",
    "\n",
    "Next you'll want to open them and add the master info information. For the defaults file simple uncomment and modify the line to look like:\n",
    "```\n",
    "spark.master                       spark://<host or IP Addr>:7077\n",
    "```\n",
    "... and the env file you'll want to look for the line:\n",
    "```\n",
    "SPARK_MASTER_HOST='<host of IP Addr>'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You shouldn't need to change anything else in this file so long as you have full control of the systems you're using. In my case, I happen to not and changed where shuffle and worker logs are ran to avoid the operating system from filling up and crashing the host. If you need to worrry about those then update the lines `SPARK_LOCAL_DIRS`, `SPARK_WORKER_DIR` and `SPARK_PID_DIR` to point to somewhere else on the system which wont fill up the partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you'll want to collect the names or IP addresses of all the hosts in your cluster and add them to the `slaves` file in `config` directory just like were the others are.\n",
    "Make sure to test the connectivity of your hosts using `ping` or something else to confirm they can actually talk to one another!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datastore\n",
    "Now we're going to work around not having a Hadoop cluster. How this works, is that we're going to create a shared folder on all of the hosts which references the Master as the **Source of Truth**.\n",
    "First, create a folder in your spark home to hold the data:\n",
    "```bash\n",
    "mdkir $SPARK_HOME/Data\n",
    "```\n",
    "\n",
    "Go ahead and create a file in here for future usage: `touch turtles`\n",
    "\n",
    "Next you'll go ahead and install a package called `sshfs` which is used to remotely mount a folder from one host and another:\n",
    "```bash\n",
    "sudo apt install sshfs\n",
    "```\n",
    "Repeat this for all the hosts in your cluster. Once that is done, you'll connect the slaves to the master using:\n",
    "```bash\n",
    "sshfs <username>@<master-address>:/opt/spark-2.4.6-bin-hadoop2.7/Data /opt/spark-2.4.6-bin-hadoop2.7/Data\n",
    "```\n",
    "Now you should be able to see the `turtles` file we created earlier if you list the files in the Data directory\n",
    "```bash\n",
    "ls Data\n",
    "```\n",
    "If you see the file then feel free to move on! If not, then double back and troubleshoot the connection between those two computers.\n",
    "Could also be permissions or something like that as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect the Dots, Start the Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got it all connected together, go ahead and run the appropriate commands on the masters and servers to start them all up:\n",
    "```bash\n",
    "# master:\n",
    "$SPARK_HOME/sbin/start-master.sh\n",
    "\n",
    "# slaves:\n",
    "$SPARK_HOME/sbin/start-slave.sh spark://<master-Addr>:7077\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try and run it on the master:\n",
    "```bash\n",
    "username@HOST:~# $SPARK_HOME/bin/pyspark \n",
    "Python 2.7.12 (default, Apr 15 2020, 17:07:12) \n",
    "[GCC 5.4.0 20160609] on linux2\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "20/10/13 05:19:50 WARN Utils: Your hostname, HOST.localdomain resolves to a loopback address: 127.0.0.1; using <address> instead (on interface eth0)\n",
    "20/10/13 05:19:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    "20/10/13 05:19:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.6\n",
    "      /_/\n",
    "\n",
    "Using Python version 2.7.12 (default, Apr 15 2020 17:07:12)\n",
    "SparkSession available as 'spark'.\n",
    ">>> \n",
    "```\n",
    "That should give you the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can transfer data into that directory and read from it using the `spark.read.*` function that you need.\n",
    "Note that copying Big Data into that directory is not a good idea. If you're looking at TeraBytes or Petabytes worth of data then you'll definitely need a real Cluster. But, I've already made some interesting observations in this limited environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "badges: false\n",
    "categories:\n",
    "- game\n",
    "- python\n",
    "- data\n",
    "- science\n",
    "- exploration\n",
    "- cycle\n",
    "- frontier\n",
    "comments: false\n",
    "date: '2022-12-06'\n",
    "title: Which Cycle Frontier Jobs Are Worth Doing? - Part Five.\n",
    "description: Bit of Cleanup and Auto-updating\n",
    "output-file: 2022-12-06-cycle-jobs-part-five.html\n",
    "toc: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where We Left Off\n",
    "In the previuos post, we dealt with missing values and finally asking some last minute questions. In this post, we're going to follow in the footsteps of **Robert Ritz's** [post](https://www.datafantic.com/create-an-auto-updating-dataset-on-kaggle-with-deepnote/) about getting a dataset updated automatically to avoid cluttering up Kaggle with old datasets which are not very useful being out of date.\n",
    "\n",
    "Note that this post is about getting the data uploaded and not about doing any sort of Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Bit of Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# Imports \n",
    "import pandas as pd             # for the data.\n",
    "import numpy as np              # for a NaN type\n",
    "import matplotlib.pyplot as plt # For plotting, and some customization of plots.\n",
    "import seaborn as sns           # For pretty plots.\n",
    "import requests as r            # For downloading from websites\n",
    "\n",
    "# Fix the size of the graphs\n",
    "sns.set(rc={\"figure.figsize\":(11, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "urlJobs = \"https://thecyclefrontier.wiki/wiki/Jobs\"\n",
    "urlLoot = \"https://thecyclefrontier.wiki/wiki/Loot\"\n",
    "urlDataDrives = 'https://thecyclefrontier.wiki/wiki/Utilities#Data_Drives-0'\n",
    "urlGun = \"https://thecyclefrontier.wiki/wiki/Weapons\"\n",
    "urlGear = 'https://thecyclefrontier.wiki/wiki/Gear'\n",
    "urlAmmo = \"https://thecyclefrontier.wiki/wiki/Ammo\"\n",
    "urlMiner = \"https://thecyclefrontier.wiki/wiki/Heavy_Mining_Tool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# Functions:\n",
    "\n",
    "def buildJobsRewards(data):\n",
    "    # Function to take job rewards data and return a cleaned version\n",
    "\n",
    "    rewardsSubset = data[[\"Name\", \"Description\", \"Difficulty\"]].copy()\n",
    "    rewardsSubset.columns = [\"Units\", \"Rewards\", \"Job\"]\n",
    "\n",
    "    index = range( 0, len(rewardsSubset) - 4, 4)\n",
    "    offset = np.array([1, 2, 3])\n",
    "\n",
    "    rewardsSubset.Job = np.NaN\n",
    "\n",
    "    for i in index:\n",
    "        aJob = rewardsSubset.iloc[i, 0]\n",
    "        indexes = i + offset\n",
    "        rewardsSubset.iloc[ indexes, 2 ] = aJob\n",
    "        \n",
    "    cutNA = rewardsSubset.Job.isna()\n",
    "    rewardsSubset = rewardsSubset[ ~cutNA ]\n",
    "\n",
    "    rewardsSubset = rewardsSubset.assign(\n",
    "        Units = rewardsSubset['Units'].astype(int)\n",
    "    )\n",
    "\n",
    "    return rewardsSubset\n",
    "\n",
    "def breakLoot(taskString, index=0):\n",
    "    parts = taskString.split(' ', maxsplit=1)\n",
    "    if index == 0:\n",
    "        return int(parts[index])\n",
    "    elif index == 1:\n",
    "        return parts[index]\n",
    "    else:\n",
    "        # This shouldn't be called.\n",
    "        return None\n",
    "\n",
    "def extractSite(siteData, columns, adjust, step,  offset):\n",
    "    if not isinstance(columns, list):\n",
    "        print(\"Columns argument must be a list.\")\n",
    "        return None\n",
    "    siteSubset = siteData[columns].copy()\n",
    "\n",
    "    siteSubset = siteSubset.assign(\n",
    "        Loot = np.NaN\n",
    "    )\n",
    "\n",
    "    # Some extra error handling \n",
    "    if not isinstance(adjust, int):\n",
    "        print(\"adjust argument must be an int.\")\n",
    "        return None\n",
    "    if not isinstance(step, int):\n",
    "        print(\"step argument must be an int.\")\n",
    "        return None\n",
    "    if not isinstance(offset, list):\n",
    "        print(\"offset argument must be a list.\")\n",
    "        return None\n",
    "    \n",
    "    index = range( 0, len(siteSubset) - adjust, step)\n",
    "    offset = np.array(offset)\n",
    "\n",
    "    for i in index:\n",
    "        aLoot = siteSubset.iloc[i, 1]\n",
    "        indexes = i + offset\n",
    "        siteSubset.iloc[indexes, len(siteSubset.columns)-1] = aLoot\n",
    "\n",
    "    tmp = siteSubset.iloc[:, 1:len(siteSubset.columns)]\n",
    "    tmp = tmp.fillna(method=\"ffill\")\n",
    "    siteSubset.iloc[:, 1:len(siteSubset.columns)-1] = tmp\n",
    "\n",
    "    cutNA = siteSubset.Loot.isna()\n",
    "    returnData = siteSubset[ ~cutNA ]\n",
    "    returnData = returnData.rename(columns={'Image':'Unit', 'Name':'Reward'})\n",
    "\n",
    "    return returnData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# Jobs Data:\n",
    "siteJobs = pd.read_html(urlJobs, match=\"Name\",\n",
    "    converters = {\n",
    "        \"Name\": str,\n",
    "        \"Description\": str, \n",
    "        \"Unlocked\": int, \n",
    "        \"Tasks\": str,\n",
    "        \"Rewards\": str})\n",
    "\n",
    "korolevRewards = buildJobsRewards( siteJobs[0] )\n",
    "icaRewards = buildJobsRewards( siteJobs[1] )\n",
    "osirisRewards = buildJobsRewards( siteJobs[2] )\n",
    "\n",
    "# Add Jobs Together:\n",
    "allJobRewards = pd.concat([korolevRewards, icaRewards, osirisRewards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# Loot Data:\n",
    "siteLoot = pd.read_html(urlLoot, attrs={\"class\":\"zebra\"})[0]\n",
    "\n",
    "lootSubset = siteLoot[[\n",
    "    'Image', 'Name', 'Rarity',\n",
    "    'Personal Quarters', 'Campaigns',\n",
    "    'Jobs', 'Printing']].copy()\n",
    "\n",
    "filterIndex = lootSubset.Printing == \"Yes\"\n",
    "lootSubset.loc[~filterIndex, \"Printing\"] = \"No\"\n",
    "\n",
    "\n",
    "# Change range to 5 instead of 4\n",
    "index = range( 0, len(lootSubset) - 4, 5)\n",
    "offset = np.array([1, 2, 3, 4])\n",
    "\n",
    "lootSubset = lootSubset.assign(\n",
    "    Loot = np.NaN\n",
    ")\n",
    "\n",
    "for i in index:\n",
    "    # Correct Loot column\n",
    "    aLoot = lootSubset.iloc[i, 1]\n",
    "    indexes = i + offset\n",
    "    lootSubset.iloc[ indexes, 7 ] = aLoot\n",
    "\n",
    "tmp = lootSubset.iloc[:, 1:7]\n",
    "tmp = tmp.fillna(method=\"ffill\")\n",
    "lootSubset.iloc[:, 1:7] = tmp\n",
    "\n",
    "cutNA = lootSubset.Loot.isna()\n",
    "lootData = lootSubset[ ~cutNA ]\n",
    "lootData = lootData.rename(columns={'Image':'Unit'})\n",
    "lootData['Rarity'] = pd.Categorical(\n",
    "    lootData.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']\n",
    ")\n",
    "loot = lootData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "tasks = []\n",
    "\n",
    "for index in range(0,3):\n",
    "    tasksSubset = siteJobs[index][[\"Name\", \"Description\", \"Tasks\"]].copy()\n",
    "    tasksSubset = tasksSubset[ ~tasksSubset.Tasks.isna()]\n",
    "    tasksSubset = tasksSubset[ ~tasksSubset.Tasks.str.contains(\"Kill\")]\n",
    "\n",
    "    regex = r\"(\\d+\\s[\\w]+\\s[\\w]+)\"\n",
    "    tmp = tasksSubset.Tasks.str.extractall(regex)\n",
    "\n",
    "    count = tmp.reset_index()[0].apply(breakLoot).values\n",
    "    aLoot = tmp.reset_index()[0].apply(breakLoot, index=1).values\n",
    "\n",
    "    tmp = tmp.assign(\n",
    "        count = count,\n",
    "        loot = aLoot\n",
    "    )\n",
    "\n",
    "    nameDescriptSlice = tasksSubset.loc[tmp.reset_index()[\"level_0\"], ['Name', 'Description']]\n",
    "\n",
    "    tmp = tmp.assign(\n",
    "        name = nameDescriptSlice.Name.values,\n",
    "        description = nameDescriptSlice.Description.values\n",
    "    )\n",
    "\n",
    "    taskSlice = tmp.reset_index().drop([\n",
    "        'level_0',\n",
    "        'match',\n",
    "        0\n",
    "    ], axis =1 )\n",
    "\n",
    "    taskSlice = taskSlice[['name', 'count', 'loot', 'description']]\n",
    "    tasks.append(taskSlice)\n",
    "tasks = pd.concat([*tasks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# Most corrections:\n",
    "tasks.loc[ tasks.loot == \"Master Unit\", 'loot'] = 'Master Unit CPU'\n",
    "tasks.loc[ tasks.loot == \"Zero Systems\", 'loot'] = 'Zero Systems CPU'\n",
    "tasks.loc[ tasks.loot == \"Pure Focus\", 'loot'] = 'Pure Focus Crystal'\n",
    "tasks.loc[ tasks.loot == \"Heavy Mining\", 'loot'] = 'Heavy Mining Tool'\n",
    "tasks.loc[ tasks.loot == \"Magnetic Field\", 'loot'] = 'Magnetic Field Stabilizer'\n",
    "tasks.loc[ tasks.loot == \"Brittle Titan\", 'loot'] = 'Brittle Titan Ore'\n",
    "tasks.loc[ tasks.loot == \"NiC Oil\", 'loot'] = 'NiC Oil Cannister'\n",
    "tasks.loc[ tasks.loot == \"Charged Spinal\", 'loot'] = 'Charged Spinal Base'\n",
    "tasks.loc[ tasks.loot == \"Hardened Bone\", 'loot'] = 'Hardened Bone Plates'\n",
    "tasks.loc[ tasks.loot == \"Pale Ivy\", 'loot'] = 'Pale Ivy Blossom'\n",
    "tasks.loc[ tasks.loot == \"Glowy Brightcap\", 'loot'] = 'Glowy Brightcap Mushroom'\n",
    "tasks.loc[ tasks.loot == \"Blue Runner\", 'loot'] = 'Blue Runner Egg'\n",
    "tasks.loc[ tasks.loot == \"Magic\", 'loot'] = 'Magic-GROW Fertilizer'\n",
    "tasks.loc[ tasks.loot == \"Letium\", 'loot'] = 'Letium Clot'\n",
    "tasks.loc[ tasks.loot == \"Azure Tree\", 'loot'] = 'Azure Tree Bark'\n",
    "\n",
    "# Fix the Gun Naming:\n",
    "tasks.loc[tasks.loot.str.contains('Advocate at'), 'loot'] = 'Advocate'\n",
    "\n",
    "tasks = tasks.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "siteDrive = pd.read_html(urlDataDrives, attrs={\"class\":\"zebra\"})[2]\n",
    "\n",
    "\n",
    "drives = extractSite(siteDrive, ['Image', 'Name', 'Rarity', 'Weight'], 2, 3, [1, 2])\n",
    "drives['Rarity'] = pd.Categorical(\n",
    "        drives.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']\n",
    "    )\n",
    "drives['Loot'] = drives['Rarity'].astype('str') + ' Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "siteGun = pd.read_html(urlGun, attrs={\"class\":\"zebra\"})[0]\n",
    "\n",
    "gunData = siteGun[~siteGun.Type.isna()]\n",
    "indx = gunData['Proj. Speed'] == 'Hitscan'\n",
    "gunData.loc[indx, 'Proj. Speed'] = np.NaN\n",
    "\n",
    "gunData = gunData.assign(\n",
    "    Unit = gunData['Sell Value'].str.replace(' K-Marks', '').astype('float'),\n",
    "    Reward = \"K-Marks\",\n",
    "    Loot = gunData['Name']\n",
    ")\n",
    "\n",
    "# # This removes the legendary weapons\n",
    "# data = data.query('Faction != \"Printing\"')\n",
    "\n",
    "guns = gunData.drop('Image', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "siteGear = pd.read_html(urlGear)\n",
    "\n",
    "siteBackPacks = siteGear[0]\n",
    "siteHelmet = siteGear[10]\n",
    "siteShield = siteGear[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "backpacks = extractSite(\n",
    "    siteData = siteBackPacks.loc[siteBackPacks.Name.str.contains(\"Backpack|K-Marks\")],\n",
    "    columns = ['Image', 'Name', 'Rarity', 'Space', 'Sale Price'],\n",
    "    adjust = 2, \n",
    "    step = 3,\n",
    "    offset = [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "ammo = pd.read_html(urlAmmo)[0]\n",
    "ammo = ammo.rename(\n",
    "    {\"Item Name\":\"Loot\", \"Sell Value\":\"Unit\"}, axis=1\n",
    "    ).assign(\n",
    "        Reward = \"K-Marks\",\n",
    "        Rarity = pd.Categorical(\n",
    "            ammo.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary'])\n",
    "    )[['Unit', 'Reward', 'Rarity', 'Loot']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "siteMiner = pd.read_html(urlMiner)\n",
    "minerData = siteMiner[0]\n",
    "\n",
    "row = minerData[1].T.to_list() + ['Heavy Mining Tool']\n",
    "columns = minerData[0].to_list() + ['Loot']\n",
    "\n",
    "mineTool = pd.DataFrame(columns = columns)\n",
    "mineTool.loc[0] = row\n",
    "mineTool = mineTool.assign(\n",
    "    Reward = \"K-Marks\",\n",
    "    Rarity = pd.Categorical(\n",
    "        mineTool.Rarity, categories = ['Common', 'Uncommon', 'Rare', 'Epic', 'Exotic', 'Legendary']\n",
    "    ),\n",
    "    Unit = mineTool['Sell Value'].astype(int),\n",
    ")[['Unit', 'Reward', 'Rarity', 'Loot']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Integration\n",
    "Following along with the article, we're going to create a Kaggle API Key. We'll go to the account page and make a new API key for this:\n",
    "![](images/2022-12-06/create-an-api-key.png)\n",
    "\n",
    "And, once we have this we add it as an Environmental Variable per the post:\n",
    "![](images/2022-12-06/add-the-api-key.png)\n",
    "\n",
    "Since we're making our own data set, we'll need to initialize the dataset on kaggle first:\n",
    "```\n",
    "!kaggle datasets init -p cycle-frontier-data\n",
    "```\n",
    "This will not work, as it did not for me, since it expects you to make the folder first. So, we'll do that and then init:\n",
    "```\n",
    "!mkdir cycle-frontier-data\n",
    "!kaggle datasets init -p cycle-frontier-data\n",
    "```\n",
    "\n",
    "Once done, you'll want to update the dataset-metadata.json file which can be found on the side:\n",
    "![](images/2022-12-06/fill-out-the-dataset-metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Merge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we need to save the data to this folder since Kaggle expects it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataPath = Path(\"cycle-frontier-data\")\n",
    "\n",
    "allJobRewards.to_csv(dataPath/\"allJobsRewards\", index=False)\n",
    "loot.to_csv(dataPath/'loot', index=False)\n",
    "tasks.to_csv(dataPath/'tasks', index=False)\n",
    "drives.to_csv(dataPath/'drives', index=False)\n",
    "guns.to_csv(dataPath/'guns', index=False)\n",
    "backpacks.to_csv(dataPath/'backpacks', index=False)\n",
    "ammo.to_csv(dataPath/'ammo', index=False)\n",
    "mineTool.to_csv(dataPath/'minetool', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Dataset to Kaggle; Schedule It To Run\n",
    "Once done, you run the create to upload the data:\n",
    "```\n",
    "!kaggle datasets create -p cycle-frontier-data\n",
    "```\n",
    "... and the dataset will be private until you fill it out the dataset and then set it to public. Make sure that at the bottom of the Notebook you include the line to update the dataset - per the post:\n",
    "```\n",
    "!kaggle datasets version -p cycle-frontier-data -m \"Automatic Update\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, you'll want to set the Notebook to run on a schedule; I set it to run as rarely as possible. If the data is out of data you could always simply login and force it to run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
